Quick Start Guide
=================

Get started with the 5D Neural Network Interpolator in just a few minutes.

Overview
--------

This guide walks you through:

1. Starting the application
2. Uploading a training dataset
3. Training a model with custom hyperparameters
4. Making predictions

Starting the Application 
-----------------------

Using Docker (recommended)
~~~~~~~~~~~

.. code-block:: bash

   # Start all services
   ./scripts/docker-start.sh

   # Or for quick start (if already set up)
   ./scripts/docker-dev.sh

   # To stop services
   ./scripts/docker-stop.sh

Soft Manual Start (Using local-build shell script)
~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Start the backend and frontend using local-build script
   ./scripts/local-build.sh

   # To stop services
   ./scripts/local-stop.sh

Manual Start
~~~~~~~~~~~

**Terminal 1 - Backend:**

.. code-block:: bash

   cd backend
   uvicorn main:app --reload

**Terminal 2 - Frontend:**

.. code-block:: bash

   cd frontend
   npm run dev

Access the Application
~~~~~~~~~~~~~~~~~~~~~

Open your browser and navigate to:

* **Frontend**: http://localhost:3000
* **API Docs**: http://localhost:8000/docs

Build the documentation
~~~~~~~~~~~

.. code-block:: bash

   ./scripts/build-docs.sh

Step-by-Step Workflow
---------------------

Step 1: Upload Training Dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Navigate to http://localhost:3000/upload
2. Select **"Training"** dataset type
3. Click **"Click to upload"** or drag and drop your ``.pkl`` file
4. Wait for validation and preview
5. Click **"Proceed to Training →"**

**Dataset Requirements:**

* Format: Python pickle (``.pkl``)
* Structure: Dictionary with keys ``'X'`` and ``'y'``
* ``X``: NumPy array of shape ``(n_samples, 5)`` - 5D feature vectors
* ``y``: NumPy array of shape ``(n_samples,)`` - 1D target values

**Example Dataset Creation:**

.. code-block:: python

   import numpy as np
   import pickle

   # Generate sample data
   n_samples = 1000
   X = np.random.randn(n_samples, 5)
   y = np.sum(X**2, axis=1) + 0.1 * np.random.randn(n_samples)

   # Save as pickle
   data = {'X': X, 'y': y}
   with open('training_data.pkl', 'wb') as f:
       pickle.dump(data, f)

Step 2: Configure Hyperparameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On the training page, you'll see interactive sliders for:

**Neural Network Architecture:**

* **Hidden Layer 1**: 8-256 neurons (default: 64)
* **Hidden Layer 2**: 8-128 neurons (default: 32)
* **Hidden Layer 3**: 4-64 neurons (default: 16)

**Training Parameters:**

* **Learning Rate**: 0.0001-0.01 (default: 0.001)
* **Max Iterations**: 100-2000 (default: 500)
* **Early Stopping**: On/Off (default: On)

**Tips:**

* Larger networks (more neurons) = more capacity but slower training
* Higher learning rates = faster convergence but may be unstable
* Early stopping prevents overfitting and saves time

Step 3: Train the Model
~~~~~~~~~~~~~~~~~~~~~~~

1. Adjust hyperparameters using the sliders
2. Click **"Start Training"**
3. Wait for training to complete (<1 minute for typical datasets)
4. Review the results:

   * **R² Score**: Model fit quality (>0.95 is excellent)
   * **MSE/MAE/RMSE**: Error metrics
   * **Hyperparameters Used**: Confirmation of settings

**Training Results Example:**

.. code-block:: text

   Training Complete
   ─────────────────
   R² Score: 0.9876
   MSE:      0.0123
   MAE:      0.0987
   RMSE:     0.1109

   Hyperparameters Used:
   Architecture: [64, 32, 16]
   Learning Rate: 0.001
   Max Iterations: 500
   Early Stopping: Yes

Step 4: Make Predictions
~~~~~~~~~~~~~~~~~~~~~~~~

**Option A: Batch Prediction**

1. Navigate to http://localhost:3000/upload
2. Select **"Prediction"** dataset type
3. Upload a ``.pkl`` file containing only ``X`` data (shape: ``n, 5``)
4. Go to http://localhost:3000/predict
5. Select **"Batch Prediction"** mode
6. Click **"Generate Batch Predictions"**

**Example Prediction Dataset:**

.. code-block:: python

   import numpy as np
   import pickle

   # Generate prediction inputs
   X_pred = np.random.randn(100, 5)

   # Save as pickle
   with open('prediction_data.pkl', 'wb') as f:
       pickle.dump(X_pred, f)

**Option B: Single Prediction**

1. Go to http://localhost:3000/predict
2. Select **"Single Prediction"** mode
3. Enter values for all 5 features
4. Click **"Predict"**
5. View the result

**Example Single Prediction:**

.. code-block:: text

   Input Features:
   F1: 1.2345
   F2: -0.5678
   F3: 0.9876
   F4: -1.2345
   F5: 0.5432

   Prediction Result:
   3.456789

Common Workflows
---------------

Experiment with Hyperparameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: text

   1. Upload dataset
   2. Train with default settings
   3. Note R² score
   4. Upload SAME dataset again (resets training state)
   5. Adjust hyperparameters
   6. Train again
   7. Compare results

Quick Iteration Cycle
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: bash

   # Using Docker - complete reset
   ./scripts/docker-start.sh

   # Or just restart services
   ./scripts/docker-dev.sh restart

Best Practices
-------------

Dataset Preparation
~~~~~~~~~~~~~~~~~~

* **Size**: 1,000-10,000 samples recommended
* **Quality**: Remove NaN/inf values before upload
* **Normalization**: Not required (automatic standardization)
* **Validation**: Check data shape and types before upload

Model Training
~~~~~~~~~~~~~

* Start with default hyperparameters
* Adjust based on R² score:

  * R² < 0.8: Increase network size or iterations
  * R² > 0.99: May be overfitting, reduce complexity
  * Training too slow: Reduce iterations or network size

* Early stopping is recommended for most cases

Performance Tips
~~~~~~~~~~~~~~~

* Use Docker for consistent performance
* Train on datasets < 10,000 samples for <1min training
* Batch predictions are faster than many single predictions
* Keep browser tab active during training

Troubleshooting Quick Start Issues
----------------------------------

**"Upload Dataset First" button stuck:**

* Refresh the page
* Check backend is running: http://localhost:8000/health
* Re-upload the dataset

**Training fails:**

* Verify dataset format (must be dictionary with 'X' and 'y')
* Check dataset shape (X must be n×5, y must be 1D)
* Try with smaller dataset first

**Predictions fail:**

* Ensure model is trained first
* For batch: upload prediction dataset
* For single: fill all 5 feature fields

Next Steps
----------

* :doc:`usage` - Detailed feature documentation
* :doc:`datasets` - Dataset format specifications
* :doc:`api/backend` - API reference for programmatic access
* :doc:`testing/overview` - Running tests
