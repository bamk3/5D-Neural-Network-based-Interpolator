%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjornstrup]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{User Guide}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


        \usepackage{charter}
        \usepackage[defaultsans]{lato}
        \usepackage{inconsolata}
    

\title{5D Neural Network Interpolator Documentation}
\date{Nov 26, 2025}
\release{0.1.0}
\author{Makimona Kiakisolako (bamk3)}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
Welcome to the \sphinxstylestrong{5D Neural Network Interpolator} documentation. This application provides a complete solution for 5D function interpolation using neural networks, developed as coursework for the DIS course at the University of Cambridge.

\sphinxhref{https://www.python.org/downloads/release/python-312/}{\sphinxincludegraphics{{/Users/kiakisolako/Downloads/interpolator/docs/build/latex/.doctrees/images/c1b348d24fec4fb545baee639d65028698ba779f/python-3.12-blue}.svg}}
%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxnolinkurl{https://www.python.org/downloads/release/python-312/}
%
\end{footnote}
\sphinxhref{https://nextjs.org/}{\sphinxincludegraphics{{/Users/kiakisolako/Downloads/interpolator/docs/build/latex/.doctrees/images/666b40cf026e5bc3dec852aa12c91e0dedcb9901/Next.js-16-black}.svg}}
%
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxnolinkurl{https://nextjs.org/}
%
\end{footnote}

\chapter{Overview}
\label{\detokenize{index:overview}}
\sphinxAtStartPar
The 5D Interpolator is a full\sphinxhyphen{}stack web application that enables:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fast Neural Network Training}: CPU\sphinxhyphen{}optimized training in under 1 minute on datasets up to 10,000 samples

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Configurable Architecture}: Fully customizable hyperparameters including layer sizes, learning rate, and iterations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Interactive Interface}: Modern React\sphinxhyphen{}based UI with real\sphinxhyphen{}time feedback

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Batch \& Single Predictions}: Support for both bulk dataset predictions and individual feature inputs

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RESTful API}: Complete FastAPI backend with automatic documentation

\end{itemize}


\chapter{Key Features}
\label{\detokenize{index:key-features}}

\section{Application Features}
\label{\detokenize{index:application-features}}\begin{itemize}
\item {} 
\sphinxAtStartPar
5D neural network interpolation with configurable architecture

\item {} 
\sphinxAtStartPar
Dataset upload (.pkl format) with automatic validation

\item {} 
\sphinxAtStartPar
Model training with customizable hyperparameters via sliders

\item {} 
\sphinxAtStartPar
Single and batch predictions

\item {} 
\sphinxAtStartPar
RESTful API with OpenAPI/Swagger documentation

\item {} 
\sphinxAtStartPar
Modern, responsive UI with dark mode support

\end{itemize}


\section{Development Features}
\label{\detokenize{index:development-features}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Docker containerization with hot reload

\item {} 
\sphinxAtStartPar
Comprehensive test suite (52 tests, 74\% coverage)

\item {} 
\sphinxAtStartPar
Multi\sphinxhyphen{}stage Docker builds for development and production

\item {} 
\sphinxAtStartPar
Environment\sphinxhyphen{}based configuration

\item {} 
\sphinxAtStartPar
Helper scripts for common operations

\end{itemize}


\chapter{Quick Start}
\label{\detokenize{index:quick-start}}
\sphinxAtStartPar
\sphinxstylestrong{Using Docker (Recommended)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Complete setup from scratch}
./scripts/docker\PYGZhy{}start.sh

\PYG{c+c1}{\PYGZsh{} Access the application}
\PYG{c+c1}{\PYGZsh{} Frontend: http://localhost:3000}
\PYG{c+c1}{\PYGZsh{} Backend API: http://localhost:8000}
\PYG{c+c1}{\PYGZsh{} API Documentation: http://localhost:8000/docs}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Manual Setup}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backend}
\PYG{n+nb}{cd}\PYG{+w}{ }backend
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}r\PYG{+w}{ }requirements.txt
uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload

\PYG{c+c1}{\PYGZsh{} Frontend (separate terminal)}
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }install
npm\PYG{+w}{ }run\PYG{+w}{ }dev
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Download Documentation}

\sphinxAtStartPar
This documentation is available in multiple formats:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate PDF and HTML archives}
./scripts/build\PYGZhy{}docs\PYGZhy{}pdf.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
Available formats:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PDF} (\textasciitilde{}419 KB) \sphinxhyphen{} For offline reading and printing

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{HTML Archive} (\textasciitilde{}8.2 MB) \sphinxhyphen{} Complete offline browsable documentation

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Online HTML} \sphinxhyphen{} This current format

\end{itemize}

\sphinxAtStartPar
See {\hyperref[\detokenize{installation::doc}]{\sphinxcrossref{\DUrole{doc}{Installation Guide}}}} for details on building and downloading documentation.


\chapter{Documentation Contents}
\label{\detokenize{index:documentation-contents}}
\sphinxstepscope


\section{Installation Guide}
\label{\detokenize{installation:installation-guide}}\label{\detokenize{installation::doc}}
\sphinxAtStartPar
This guide covers all methods for installing and running the 5D Neural Network Interpolator.


\subsection{Prerequisites}
\label{\detokenize{installation:prerequisites}}

\subsubsection{System Requirements}
\label{\detokenize{installation:system-requirements}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Operating System}: macOS, Linux, or Windows (with WSL2)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RAM}: Minimum 4GB (8GB recommended)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Disk Space}: Minimum 2GB free space

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Internet Connection}: Required for initial setup

\end{itemize}


\subsubsection{Software Requirements}
\label{\detokenize{installation:software-requirements}}
\sphinxAtStartPar
\sphinxstylestrong{For Docker Installation (Recommended):}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Docker 20.10+ or Docker Desktop

\item {} 
\sphinxAtStartPar
Docker Compose v2.0+

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{For Manual Installation:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Python 3.12+

\item {} 
\sphinxAtStartPar
Node.js 20+

\item {} 
\sphinxAtStartPar
npm 10.8+

\item {} 
\sphinxAtStartPar
pip 23+

\end{itemize}


\subsection{Installation Methods}
\label{\detokenize{installation:installation-methods}}

\subsubsection{Method 1: Docker Installation (Recommended)}
\label{\detokenize{installation:method-1-docker-installation-recommended}}
\sphinxAtStartPar
This is the fastest and most reliable method.

\sphinxAtStartPar
\sphinxstylestrong{Step 1: Verify Docker is Running}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check Docker is installed and running}
docker\PYG{+w}{ }\PYGZhy{}\PYGZhy{}version
docker\PYG{+w}{ }compose\PYG{+w}{ }version

\PYG{c+c1}{\PYGZsh{} On Linux, ensure Docker service is running}
sudo\PYG{+w}{ }systemctl\PYG{+w}{ }status\PYG{+w}{ }docker
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 2: Clone the Repository}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
git\PYG{+w}{ }clone\PYG{+w}{ }\PYGZlt{}repository\PYGZhy{}url\PYGZgt{}
\PYG{n+nb}{cd}\PYG{+w}{ }interpolator
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 3: Run Setup Script}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Complete setup (clean + rebuild + start)}
./scripts/docker\PYGZhy{}start.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
This script will:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Check if Docker is running

\item {} 
\sphinxAtStartPar
Clean up any existing containers

\item {} 
\sphinxAtStartPar
Create environment configuration

\item {} 
\sphinxAtStartPar
Build Docker images (\textasciitilde{}3\sphinxhyphen{}5 minutes)

\item {} 
\sphinxAtStartPar
Start all services

\item {} 
\sphinxAtStartPar
Display access URLs

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Step 4: Verify Installation}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check service status}
docker\PYG{+w}{ }compose\PYG{+w}{ }ps

\PYG{c+c1}{\PYGZsh{} Test backend health}
curl\PYG{+w}{ }http://localhost:8000/health

\PYG{c+c1}{\PYGZsh{} Test frontend (should return HTML)}
curl\PYG{+w}{ }http://localhost:3000
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Access URLs:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Frontend: \sphinxurl{http://localhost:3000}

\item {} 
\sphinxAtStartPar
Backend API: \sphinxurl{http://localhost:8000}

\item {} 
\sphinxAtStartPar
API Documentation: \sphinxurl{http://localhost:8000/docs}

\end{itemize}


\subsubsection{Method 2: Manual Installation}
\label{\detokenize{installation:method-2-manual-installation}}
\sphinxAtStartPar
\sphinxstylestrong{Step 1: Install Backend Dependencies}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend

\PYG{c+c1}{\PYGZsh{} Create virtual environment (recommended)}
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} On Windows: venv\PYGZbs{}Scripts\PYGZbs{}activate}

\PYG{c+c1}{\PYGZsh{} Install dependencies}
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}r\PYG{+w}{ }requirements.txt
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 2: Install Frontend Dependencies}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }install
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 3: Start Backend Server}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} If using virtual environment}
uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload\PYG{+w}{ }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8000}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 4: Start Frontend Server} (in new terminal)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }run\PYG{+w}{ }dev
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Access URLs:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Frontend: \sphinxurl{http://localhost:3000}

\item {} 
\sphinxAtStartPar
Backend API: \sphinxurl{http://localhost:8000}

\item {} 
\sphinxAtStartPar
API Documentation: \sphinxurl{http://localhost:8000/docs}

\end{itemize}


\subsection{Environment Configuration}
\label{\detokenize{installation:environment-configuration}}

\subsubsection{Environment Variables}
\label{\detokenize{installation:environment-variables}}
\sphinxAtStartPar
The application uses environment variables for configuration. Three preset files are provided:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.env.development}} \sphinxhyphen{} For local development

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.env.production}} \sphinxhyphen{} For production deployment

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{.env.example}} \sphinxhyphen{} Template with all available variables

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Key Variables:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backend}
\PYG{n+nv}{BACKEND\PYGZus{}PORT}\PYG{o}{=}\PYG{l+m}{8000}
\PYG{n+nv}{CORS\PYGZus{}ORIGINS}\PYG{o}{=}http://localhost:3000

\PYG{c+c1}{\PYGZsh{} Frontend}
\PYG{n+nv}{FRONTEND\PYGZus{}PORT}\PYG{o}{=}\PYG{l+m}{3000}
\PYG{n+nv}{NEXT\PYGZus{}PUBLIC\PYGZus{}API\PYGZus{}URL}\PYG{o}{=}http://localhost:8000

\PYG{c+c1}{\PYGZsh{} Docker}
\PYG{n+nv}{BUILD\PYGZus{}TARGET}\PYG{o}{=}development\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} or \PYGZsq{}production\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Setup:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Copy development configuration}
cp\PYG{+w}{ }.env.development\PYG{+w}{ }.env

\PYG{c+c1}{\PYGZsh{} Or for production}
cp\PYG{+w}{ }.env.production\PYG{+w}{ }.env
\end{sphinxVerbatim}


\subsection{Troubleshooting}
\label{\detokenize{installation:troubleshooting}}

\subsubsection{Docker Issues}
\label{\detokenize{installation:docker-issues}}
\sphinxAtStartPar
\sphinxstylestrong{“Docker is not running” error:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} macOS}
open\PYG{+w}{ }\PYGZhy{}a\PYG{+w}{ }Docker

\PYG{c+c1}{\PYGZsh{} Linux}
sudo\PYG{+w}{ }systemctl\PYG{+w}{ }start\PYG{+w}{ }docker

\PYG{c+c1}{\PYGZsh{} Check status}
docker\PYG{+w}{ }info
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{“Port already in use” error:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Find process using port 3000}
lsof\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }:3000

\PYG{c+c1}{\PYGZsh{} Kill the process}
\PYG{n+nb}{kill}\PYG{+w}{ }\PYGZhy{}9\PYG{+w}{ }\PYGZlt{}PID\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{“docker\sphinxhyphen{}compose: command not found”:}

\sphinxAtStartPar
You have Docker Compose v2 (plugin version). Use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }compose\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} (with space, not hyphen)}
\end{sphinxVerbatim}


\subsubsection{Permission Issues (Linux)}
\label{\detokenize{installation:permission-issues-linux}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Add user to docker group}
sudo\PYG{+w}{ }usermod\PYG{+w}{ }\PYGZhy{}aG\PYG{+w}{ }docker\PYG{+w}{ }\PYG{n+nv}{\PYGZdl{}USER}

\PYG{c+c1}{\PYGZsh{} Apply changes}
newgrp\PYG{+w}{ }docker

\PYG{c+c1}{\PYGZsh{} Verify}
docker\PYG{+w}{ }ps
\end{sphinxVerbatim}


\subsubsection{Python/Node Issues}
\label{\detokenize{installation:python-node-issues}}
\sphinxAtStartPar
\sphinxstylestrong{Wrong Python version:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check version}
python3\PYG{+w}{ }\PYGZhy{}\PYGZhy{}version

\PYG{c+c1}{\PYGZsh{} Install Python 3.12 via package manager}
\PYG{c+c1}{\PYGZsh{} macOS:}
brew\PYG{+w}{ }install\PYG{+w}{ }python@3.12

\PYG{c+c1}{\PYGZsh{} Ubuntu/Debian:}
sudo\PYG{+w}{ }apt\PYG{+w}{ }install\PYG{+w}{ }python3.12
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{npm install fails:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Clear npm cache}
npm\PYG{+w}{ }cache\PYG{+w}{ }clean\PYG{+w}{ }\PYGZhy{}\PYGZhy{}force

\PYG{c+c1}{\PYGZsh{} Delete node\PYGZus{}modules and retry}
rm\PYG{+w}{ }\PYGZhy{}rf\PYG{+w}{ }node\PYGZus{}modules\PYG{+w}{ }package\PYGZhy{}lock.json
npm\PYG{+w}{ }install
\end{sphinxVerbatim}


\subsection{Verifying Installation}
\label{\detokenize{installation:verifying-installation}}
\sphinxAtStartPar
Run the complete test suite to verify everything works:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Using Docker}
./scripts/docker\PYGZhy{}dev.sh\PYG{+w}{ }test\PYGZhy{}backend

\PYG{c+c1}{\PYGZsh{} Manual installation}
\PYG{n+nb}{cd}\PYG{+w}{ }backend
pytest
\end{sphinxVerbatim}

\sphinxAtStartPar
Expected output: \sphinxcode{\sphinxupquote{52 passed}} with \sphinxcode{\sphinxupquote{74\% coverage}}


\subsection{Building Documentation}
\label{\detokenize{installation:building-documentation}}
\sphinxAtStartPar
This documentation can be built locally for offline access:


\subsubsection{Quick Build}
\label{\detokenize{installation:quick-build}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
./scripts/build\PYGZhy{}docs.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
This automated script will:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Check Python installation (3.12+ required)

\item {} 
\sphinxAtStartPar
Create virtual environment for Sphinx

\item {} 
\sphinxAtStartPar
Install Sphinx and dependencies

\item {} 
\sphinxAtStartPar
Build HTML documentation

\item {} 
\sphinxAtStartPar
Open in your default browser

\end{enumerate}

\sphinxAtStartPar
The documentation will be available at:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docs/build/html/index.html
\end{sphinxVerbatim}


\subsubsection{Manual Build}
\label{\detokenize{installation:manual-build}}
\sphinxAtStartPar
For manual control over the build process:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }docs

\PYG{c+c1}{\PYGZsh{} Create virtual environment (first time only)}
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate

\PYG{c+c1}{\PYGZsh{} Install dependencies (first time only)}
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}r\PYG{+w}{ }requirements.txt

\PYG{c+c1}{\PYGZsh{} Build documentation}
sphinx\PYGZhy{}build\PYG{+w}{ }\PYGZhy{}b\PYG{+w}{ }html\PYG{+w}{ }\PYG{n+nb}{source}\PYG{+w}{ }build/html

\PYG{c+c1}{\PYGZsh{} Open in browser}
open\PYG{+w}{ }build/html/index.html\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} macOS}
xdg\PYGZhy{}open\PYG{+w}{ }build/html/index.html\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Linux}
\end{sphinxVerbatim}


\subsubsection{Rebuilding Documentation}
\label{\detokenize{installation:rebuilding-documentation}}
\sphinxAtStartPar
To rebuild after making changes:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }docs
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate

\PYG{c+c1}{\PYGZsh{} Clean previous build}
rm\PYG{+w}{ }\PYGZhy{}rf\PYG{+w}{ }build/html

\PYG{c+c1}{\PYGZsh{} Rebuild}
sphinx\PYGZhy{}build\PYG{+w}{ }\PYGZhy{}b\PYG{+w}{ }html\PYG{+w}{ }\PYG{n+nb}{source}\PYG{+w}{ }build/html
\end{sphinxVerbatim}


\subsubsection{Documentation Requirements}
\label{\detokenize{installation:documentation-requirements}}
\sphinxAtStartPar
The documentation build requires:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Python 3.12+

\item {} 
\sphinxAtStartPar
Sphinx 8.2.3+

\item {} 
\sphinxAtStartPar
sphinx\sphinxhyphen{}rtd\sphinxhyphen{}theme

\item {} 
\sphinxAtStartPar
sphinxcontrib packages

\end{itemize}

\sphinxAtStartPar
All dependencies are listed in \sphinxcode{\sphinxupquote{docs/requirements.txt}}


\subsubsection{Downloading Documentation}
\label{\detokenize{installation:downloading-documentation}}
\sphinxAtStartPar
\sphinxstylestrong{Generate Downloadable Documentation:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
./scripts/build\PYGZhy{}docs\PYGZhy{}pdf.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
This generates multiple downloadable formats:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PDF}: \sphinxcode{\sphinxupquote{docs/build/downloads/5D\sphinxhyphen{}Interpolator\sphinxhyphen{}Documentation.pdf}} (\textasciitilde{}419 KB)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{HTML Archive (tar.gz)}: \sphinxcode{\sphinxupquote{docs/build/downloads/5D\sphinxhyphen{}Interpolator\sphinxhyphen{}Documentation\sphinxhyphen{}HTML.tar.gz}} (\textasciitilde{}8.2 MB)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{HTML Archive (zip)}: \sphinxcode{\sphinxupquote{docs/build/downloads/5D\sphinxhyphen{}Interpolator\sphinxhyphen{}Documentation\sphinxhyphen{}HTML.zip}} (\textasciitilde{}8.2 MB)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{PDF Generation Requirements:}

\sphinxAtStartPar
For LaTeX\sphinxhyphen{}based PDF (recommended):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{macOS}: Install MacTeX

\begin{sphinxVerbatim}[commandchars=\\\{\}]
brew\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cask\PYG{+w}{ }mactex
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ubuntu/Debian}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sudo\PYG{+w}{ }apt\PYGZhy{}get\PYG{+w}{ }install\PYG{+w}{ }texlive\PYGZhy{}latex\PYGZhy{}extra\PYG{+w}{ }texlive\PYGZhy{}fonts\PYGZhy{}recommended
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fallback}: If LaTeX not available, script automatically uses rst2pdf

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Using Downloaded Documentation:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{PDF}: Open directly in any PDF reader

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{HTML Archives}: Extract and open \sphinxcode{\sphinxupquote{index.html}} in a web browser

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Extract tar.gz}
tar\PYG{+w}{ }\PYGZhy{}xzf\PYG{+w}{ }5D\PYGZhy{}Interpolator\PYGZhy{}Documentation\PYGZhy{}HTML.tar.gz
open\PYG{+w}{ }html/index.html

\PYG{c+c1}{\PYGZsh{} Or extract zip}
unzip\PYG{+w}{ }5D\PYGZhy{}Interpolator\PYGZhy{}Documentation\PYGZhy{}HTML.zip
open\PYG{+w}{ }index.html
\end{sphinxVerbatim}

\end{itemize}


\subsection{Next Steps}
\label{\detokenize{installation:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{quickstart::doc}]{\sphinxcrossref{\DUrole{doc}{Quick Start Guide}}}} \sphinxhyphen{} Get started with your first model

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{usage::doc}]{\sphinxcrossref{\DUrole{doc}{Usage Guide}}}} \sphinxhyphen{} Learn about features and workflows

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{datasets::doc}]{\sphinxcrossref{\DUrole{doc}{Dataset Specifications}}}} \sphinxhyphen{} Understand dataset requirements

\end{itemize}

\sphinxstepscope


\section{Quick Start Guide}
\label{\detokenize{quickstart:quick-start-guide}}\label{\detokenize{quickstart::doc}}
\sphinxAtStartPar
Get started with the 5D Neural Network Interpolator in just a few minutes.


\subsection{Overview}
\label{\detokenize{quickstart:overview}}
\sphinxAtStartPar
This guide walks you through:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Starting the application

\item {} 
\sphinxAtStartPar
Uploading a training dataset

\item {} 
\sphinxAtStartPar
Training a model with custom hyperparameters

\item {} 
\sphinxAtStartPar
Making predictions

\end{enumerate}


\subsection{Starting the Application}
\label{\detokenize{quickstart:starting-the-application}}

\subsubsection{Using Docker (recommended)}
\label{\detokenize{quickstart:using-docker-recommended}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Start all services}
./scripts/docker\PYGZhy{}start.sh

\PYG{c+c1}{\PYGZsh{} Or for quick start (if already set up)}
./scripts/docker\PYGZhy{}dev.sh

\PYG{c+c1}{\PYGZsh{} To stop services}
./scripts/docker\PYGZhy{}stop.sh
\end{sphinxVerbatim}


\subsubsection{Soft Manual Start (Using local\sphinxhyphen{}build shell script)}
\label{\detokenize{quickstart:soft-manual-start-using-local-build-shell-script}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Start the backend and frontend using local\PYGZhy{}build script}
./scripts/local\PYGZhy{}build.sh

\PYG{c+c1}{\PYGZsh{} To stop services}
./scripts/local\PYGZhy{}stop.sh
\end{sphinxVerbatim}


\subsubsection{Manual Start}
\label{\detokenize{quickstart:manual-start}}
\sphinxAtStartPar
\sphinxstylestrong{Terminal 1 \sphinxhyphen{} Backend:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend
uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Terminal 2 \sphinxhyphen{} Frontend:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }run\PYG{+w}{ }dev
\end{sphinxVerbatim}


\subsubsection{Access the Application}
\label{\detokenize{quickstart:access-the-application}}
\sphinxAtStartPar
Open your browser and navigate to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Frontend}: \sphinxurl{http://localhost:3000}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{API Docs}: \sphinxurl{http://localhost:8000/docs}

\end{itemize}


\subsubsection{Build the documentation}
\label{\detokenize{quickstart:build-the-documentation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
./scripts/build\PYGZhy{}docs.sh
\end{sphinxVerbatim}


\subsection{Step\sphinxhyphen{}by\sphinxhyphen{}Step Workflow}
\label{\detokenize{quickstart:step-by-step-workflow}}

\subsubsection{Step 1: Upload Training Dataset}
\label{\detokenize{quickstart:step-1-upload-training-dataset}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Navigate to \sphinxurl{http://localhost:3000/upload}

\item {} 
\sphinxAtStartPar
Select \sphinxstylestrong{“Training”} dataset type

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Click to upload”} or drag and drop your \sphinxcode{\sphinxupquote{.pkl}} file

\item {} 
\sphinxAtStartPar
Wait for validation and preview

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Proceed to Training \(\rightarrow\)”}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Dataset Requirements:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Format: Python pickle (\sphinxcode{\sphinxupquote{.pkl}})

\item {} 
\sphinxAtStartPar
Structure: Dictionary with keys \sphinxcode{\sphinxupquote{\textquotesingle{}X\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}y\textquotesingle{}}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{X}}: NumPy array of shape \sphinxcode{\sphinxupquote{(n\_samples, 5)}} \sphinxhyphen{} 5D feature vectors

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{y}}: NumPy array of shape \sphinxcode{\sphinxupquote{(n\_samples,)}} \sphinxhyphen{} 1D target values

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example Dataset Creation:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Generate sample data}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save as pickle}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{training\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Step 2: Configure Hyperparameters}
\label{\detokenize{quickstart:step-2-configure-hyperparameters}}
\sphinxAtStartPar
On the training page, you’ll see interactive sliders for:

\sphinxAtStartPar
\sphinxstylestrong{Neural Network Architecture:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer 1}: 8\sphinxhyphen{}256 neurons (default: 64)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer 2}: 8\sphinxhyphen{}128 neurons (default: 32)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer 3}: 4\sphinxhyphen{}64 neurons (default: 16)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Training Parameters:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Learning Rate}: 0.0001\sphinxhyphen{}0.01 (default: 0.001)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Max Iterations}: 100\sphinxhyphen{}2000 (default: 500)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Early Stopping}: On/Off (default: On)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Tips:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Larger networks (more neurons) = more capacity but slower training

\item {} 
\sphinxAtStartPar
Higher learning rates = faster convergence but may be unstable

\item {} 
\sphinxAtStartPar
Early stopping prevents overfitting and saves time

\end{itemize}


\subsubsection{Step 3: Train the Model}
\label{\detokenize{quickstart:step-3-train-the-model}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Adjust hyperparameters using the sliders

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Start Training”}

\item {} 
\sphinxAtStartPar
Wait for training to complete (\textless{}1 minute for typical datasets)

\item {} 
\sphinxAtStartPar
Review the results:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{R\(\sp{\text{2}}\) Score}: Model fit quality (\textgreater{}0.95 is excellent)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MSE/MAE/RMSE}: Error metrics

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hyperparameters Used}: Confirmation of settings

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Training Results Example:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Training Complete
─────────────────
R\(\sp{\text{2}}\) Score: 0.9876
MSE:      0.0123
MAE:      0.0987
RMSE:     0.1109

Hyperparameters Used:
Architecture: [64, 32, 16]
Learning Rate: 0.001
Max Iterations: 500
Early Stopping: Yes
\end{sphinxVerbatim}


\subsubsection{Step 4: Make Predictions}
\label{\detokenize{quickstart:step-4-make-predictions}}
\sphinxAtStartPar
\sphinxstylestrong{Option A: Batch Prediction}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Navigate to \sphinxurl{http://localhost:3000/upload}

\item {} 
\sphinxAtStartPar
Select \sphinxstylestrong{“Prediction”} dataset type

\item {} 
\sphinxAtStartPar
Upload a \sphinxcode{\sphinxupquote{.pkl}} file containing only \sphinxcode{\sphinxupquote{X}} data (shape: \sphinxcode{\sphinxupquote{n, 5}})

\item {} 
\sphinxAtStartPar
Go to \sphinxurl{http://localhost:3000/predict}

\item {} 
\sphinxAtStartPar
Select \sphinxstylestrong{“Batch Prediction”} mode

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Generate Batch Predictions”}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Example Prediction Dataset:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Generate prediction inputs}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save as pickle}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Option B: Single Prediction}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Go to \sphinxurl{http://localhost:3000/predict}

\item {} 
\sphinxAtStartPar
Select \sphinxstylestrong{“Single Prediction”} mode

\item {} 
\sphinxAtStartPar
Enter values for all 5 features

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Predict”}

\item {} 
\sphinxAtStartPar
View the result

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Example Single Prediction:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Input Features:
F1: 1.2345
F2: \PYGZhy{}0.5678
F3: 0.9876
F4: \PYGZhy{}1.2345
F5: 0.5432

Prediction Result:
3.456789
\end{sphinxVerbatim}


\subsection{Common Workflows}
\label{\detokenize{quickstart:common-workflows}}

\subsubsection{Experiment with Hyperparameters}
\label{\detokenize{quickstart:experiment-with-hyperparameters}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1. Upload dataset
2. Train with default settings
3. Note R\(\sp{\text{2}}\) score
4. Upload SAME dataset again (resets training state)
5. Adjust hyperparameters
6. Train again
7. Compare results
\end{sphinxVerbatim}


\subsubsection{Quick Iteration Cycle}
\label{\detokenize{quickstart:quick-iteration-cycle}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Using Docker \PYGZhy{} complete reset}
./scripts/docker\PYGZhy{}start.sh

\PYG{c+c1}{\PYGZsh{} Or just restart services}
./scripts/docker\PYGZhy{}dev.sh\PYG{+w}{ }restart
\end{sphinxVerbatim}


\subsection{Best Practices}
\label{\detokenize{quickstart:best-practices}}

\subsubsection{Dataset Preparation}
\label{\detokenize{quickstart:dataset-preparation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Size}: 1,000\sphinxhyphen{}10,000 samples recommended

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Quality}: Remove NaN/inf values before upload

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normalization}: Not required (automatic standardization)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Validation}: Check data shape and types before upload

\end{itemize}


\subsubsection{Model Training}
\label{\detokenize{quickstart:model-training}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Start with default hyperparameters

\item {} 
\sphinxAtStartPar
Adjust based on R\(\sp{\text{2}}\) score:
\begin{itemize}
\item {} 
\sphinxAtStartPar
R\(\sp{\text{2}}\) \textless{} 0.8: Increase network size or iterations

\item {} 
\sphinxAtStartPar
R\(\sp{\text{2}}\) \textgreater{} 0.99: May be overfitting, reduce complexity

\item {} 
\sphinxAtStartPar
Training too slow: Reduce iterations or network size

\end{itemize}

\item {} 
\sphinxAtStartPar
Early stopping is recommended for most cases

\end{itemize}


\subsubsection{Performance Tips}
\label{\detokenize{quickstart:performance-tips}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Use Docker for consistent performance

\item {} 
\sphinxAtStartPar
Train on datasets \textless{} 10,000 samples for \textless{}1min training

\item {} 
\sphinxAtStartPar
Batch predictions are faster than many single predictions

\item {} 
\sphinxAtStartPar
Keep browser tab active during training

\end{itemize}


\subsection{Troubleshooting Quick Start Issues}
\label{\detokenize{quickstart:troubleshooting-quick-start-issues}}
\sphinxAtStartPar
\sphinxstylestrong{“Upload Dataset First” button stuck:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Refresh the page

\item {} 
\sphinxAtStartPar
Check backend is running: \sphinxurl{http://localhost:8000/health}

\item {} 
\sphinxAtStartPar
Re\sphinxhyphen{}upload the dataset

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Training fails:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Verify dataset format (must be dictionary with ‘X’ and ‘y’)

\item {} 
\sphinxAtStartPar
Check dataset shape (X must be n×5, y must be 1D)

\item {} 
\sphinxAtStartPar
Try with smaller dataset first

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Predictions fail:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ensure model is trained first

\item {} 
\sphinxAtStartPar
For batch: upload prediction dataset

\item {} 
\sphinxAtStartPar
For single: fill all 5 feature fields

\end{itemize}


\subsection{Next Steps}
\label{\detokenize{quickstart:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{usage::doc}]{\sphinxcrossref{\DUrole{doc}{Usage Guide}}}} \sphinxhyphen{} Detailed feature documentation

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{datasets::doc}]{\sphinxcrossref{\DUrole{doc}{Dataset Specifications}}}} \sphinxhyphen{} Dataset format specifications

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} API reference for programmatic access

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{testing/overview::doc}]{\sphinxcrossref{\DUrole{doc}{Testing Overview}}}} \sphinxhyphen{} Running tests

\end{itemize}

\sphinxstepscope


\section{Usage Guide}
\label{\detokenize{usage:usage-guide}}\label{\detokenize{usage::doc}}
\sphinxAtStartPar
Comprehensive guide to using the 5D Neural Network Interpolator.


\subsection{Application Workflow}
\label{\detokenize{usage:application-workflow}}
\sphinxAtStartPar
The typical workflow consists of three main steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Upload Training Dataset} \(\rightarrow\) 2. \sphinxstylestrong{Train Model} \(\rightarrow\) 3. \sphinxstylestrong{Make Predictions}

\end{enumerate}


\subsection{Step 1: Upload Training Dataset}
\label{\detokenize{usage:step-1-upload-training-dataset}}
\sphinxAtStartPar
Navigate to the Upload page and select a training dataset.


\subsubsection{Dataset Requirements}
\label{\detokenize{usage:dataset-requirements}}
\sphinxAtStartPar
The training dataset must be a Python pickle file (\sphinxcode{\sphinxupquote{.pkl}}) containing:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Shape: (n\PYGZus{}samples, 5)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}   \PYG{c+c1}{\PYGZsh{} Shape: (n\PYGZus{}samples,)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{X}}: 5\sphinxhyphen{}dimensional feature vectors (independent variables)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{y}}: 1\sphinxhyphen{}dimensional target values (dependent variable)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{n\_samples}}: Number of training examples

\end{itemize}


\subsubsection{Example Dataset Creation}
\label{\detokenize{usage:example-dataset-creation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Generate 1000 samples}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} Create 5D features}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create target (example: sum of squares)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save as pickle}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{training\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Upload Process}
\label{\detokenize{usage:upload-process}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Training”} dataset type

\item {} 
\sphinxAtStartPar
Click upload area or drag file

\item {} 
\sphinxAtStartPar
Wait for validation

\item {} 
\sphinxAtStartPar
Review data preview showing:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Total samples

\item {} 
\sphinxAtStartPar
Data shape

\item {} 
\sphinxAtStartPar
First 5 rows

\end{itemize}

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Proceed to Training \(\rightarrow\)”}

\end{enumerate}


\subsection{Step 2: Train Model}
\label{\detokenize{usage:step-2-train-model}}
\sphinxAtStartPar
Configure hyperparameters and train the neural network.


\subsubsection{Hyperparameter Configuration}
\label{\detokenize{usage:hyperparameter-configuration}}
\sphinxAtStartPar
\sphinxstylestrong{Neural Network Architecture}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer 1} (8\sphinxhyphen{}256 neurons)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Controls first layer capacity

\item {} 
\sphinxAtStartPar
Default: 64 neurons

\item {} 
\sphinxAtStartPar
Larger = more complex patterns

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer 2} (8\sphinxhyphen{}128 neurons)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Controls second layer capacity

\item {} 
\sphinxAtStartPar
Default: 32 neurons

\item {} 
\sphinxAtStartPar
Typically smaller than layer 1

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer 3} (4\sphinxhyphen{}64 neurons)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Controls third layer capacity

\item {} 
\sphinxAtStartPar
Default: 16 neurons

\item {} 
\sphinxAtStartPar
Smallest layer before output

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Training Parameters}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Learning Rate} (0.0001\sphinxhyphen{}0.01)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Speed of gradient descent

\item {} 
\sphinxAtStartPar
Default: 0.001

\item {} 
\sphinxAtStartPar
Higher = faster but less stable

\item {} 
\sphinxAtStartPar
Lower = slower but more precise

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Max Iterations} (100\sphinxhyphen{}2000)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Maximum training epochs

\item {} 
\sphinxAtStartPar
Default: 500

\item {} 
\sphinxAtStartPar
Higher = more training time

\item {} 
\sphinxAtStartPar
May stop early if enabled

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Early Stopping} (On/Off)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Stops when validation loss plateaus

\item {} 
\sphinxAtStartPar
Default: On (recommended)

\item {} 
\sphinxAtStartPar
Prevents overfitting

\item {} 
\sphinxAtStartPar
Saves computation time

\end{itemize}

\end{itemize}


\subsubsection{Recommended Configurations}
\label{\detokenize{usage:recommended-configurations}}
\sphinxAtStartPar
\sphinxstylestrong{Default (Balanced)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Architecture: [64, 32, 16]
Learning Rate: 0.001
Max Iterations: 500
Early Stopping: On

Use for: Most datasets
Expected time: 15\PYGZhy{}30 seconds
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Fast Training}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Architecture: [32, 16, 8]
Learning Rate: 0.01
Max Iterations: 200
Early Stopping: On

Use for: Quick experiments
Expected time: 5\PYGZhy{}10 seconds
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{High Accuracy}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Architecture: [128, 64, 32]
Learning Rate: 0.001
Max Iterations: 1000
Early Stopping: On

Use for: Best possible fit
Expected time: 30\PYGZhy{}60 seconds
\end{sphinxVerbatim}


\subsubsection{Training Process}
\label{\detokenize{usage:training-process}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Adjust sliders to desired values

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Start Training”}

\item {} 
\sphinxAtStartPar
Wait for training (typically \textless{}1 minute)

\item {} 
\sphinxAtStartPar
Review results

\end{enumerate}


\subsubsection{Understanding Training Results}
\label{\detokenize{usage:understanding-training-results}}
\sphinxAtStartPar
After training completes, you’ll see:

\sphinxAtStartPar
\sphinxstylestrong{Performance Metrics}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{R\(\sp{\text{2}}\) Score}: Model fit quality (0\sphinxhyphen{}1)
\begin{itemize}
\item {} 
\sphinxAtStartPar
\textgreater{}0.95: Excellent

\item {} 
\sphinxAtStartPar
0.90\sphinxhyphen{}0.95: Very good

\item {} 
\sphinxAtStartPar
0.80\sphinxhyphen{}0.90: Good

\item {} 
\sphinxAtStartPar
\textless{}0.80: May need tuning

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MSE (Mean Squared Error)}: Average squared error
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lower is better

\item {} 
\sphinxAtStartPar
Scale depends on target values

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{MAE (Mean Absolute Error)}: Average absolute error
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lower is better

\item {} 
\sphinxAtStartPar
Same units as target variable

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{RMSE (Root Mean Squared Error)}: Square root of MSE
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lower is better

\item {} 
\sphinxAtStartPar
Same units as target variable

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Hyperparameters Used}

\sphinxAtStartPar
Displays the configuration used for training.


\subsection{Step 3: Make Predictions}
\label{\detokenize{usage:step-3-make-predictions}}
\sphinxAtStartPar
Two prediction modes are available.


\subsubsection{Batch Prediction}
\label{\detokenize{usage:batch-prediction}}
\sphinxAtStartPar
For predicting multiple samples at once.

\sphinxAtStartPar
\sphinxstylestrong{Dataset Requirements:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Pickle file containing only X data}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}  \PYG{c+c1}{\PYGZsh{} Shape: (n\PYGZus{}samples, 5)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Example:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Create prediction inputs}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save as pickle}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Steps:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Upload prediction dataset (Upload page)

\item {} 
\sphinxAtStartPar
Go to Predict page

\item {} 
\sphinxAtStartPar
Select \sphinxstylestrong{“Batch Prediction”}

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Generate Batch Predictions”}

\item {} 
\sphinxAtStartPar
View results

\end{enumerate}


\subsubsection{Single Prediction}
\label{\detokenize{usage:single-prediction}}
\sphinxAtStartPar
For predicting one sample at a time.

\sphinxAtStartPar
\sphinxstylestrong{Steps:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Go to Predict page

\item {} 
\sphinxAtStartPar
Select \sphinxstylestrong{“Single Prediction”}

\item {} 
\sphinxAtStartPar
Enter values for all 5 features

\item {} 
\sphinxAtStartPar
Click \sphinxstylestrong{“Predict”}

\item {} 
\sphinxAtStartPar
View result

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Example Input:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Feature 1: 1.2345
Feature 2: \PYGZhy{}0.5678
Feature 3: 0.9876
Feature 4: \PYGZhy{}1.2345
Feature 5: 0.5432

Result: 3.456789
\end{sphinxVerbatim}


\subsection{Advanced Usage}
\label{\detokenize{usage:advanced-usage}}

\subsubsection{Experimenting with Hyperparameters}
\label{\detokenize{usage:experimenting-with-hyperparameters}}
\sphinxAtStartPar
To compare different configurations:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Train with configuration A

\item {} 
\sphinxAtStartPar
Note R\(\sp{\text{2}}\) score

\item {} 
\sphinxAtStartPar
Go to Upload page

\item {} 
\sphinxAtStartPar
Re\sphinxhyphen{}upload SAME dataset (resets state)

\item {} 
\sphinxAtStartPar
Return to Train page

\item {} 
\sphinxAtStartPar
Train with configuration B

\item {} 
\sphinxAtStartPar
Compare results

\end{enumerate}


\subsubsection{API Usage}
\label{\detokenize{usage:api-usage}}
\sphinxAtStartPar
For programmatic access, use the REST API:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{requests}

\PYG{n}{BASE\PYGZus{}URL} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://localhost:8000}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Upload dataset}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{r} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/upload\PYGZhy{}fit\PYGZhy{}dataset/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{files}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{f}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train with custom hyperparameters}
\PYG{n}{r} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
    \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/start\PYGZhy{}training/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{json}\PYG{o}{=}\PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hyperparameters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hidden\PYGZus{}layer\PYGZus{}1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{128}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Make prediction}
\PYG{n}{r} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
    \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/predict\PYGZhy{}single/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{json}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{features}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
See {\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} for complete API reference.


\subsection{Tips and Best Practices}
\label{\detokenize{usage:tips-and-best-practices}}

\subsubsection{Dataset Preparation}
\label{\detokenize{usage:dataset-preparation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Remove NaN/inf values before upload

\item {} 
\sphinxAtStartPar
Ensure consistent data types

\item {} 
\sphinxAtStartPar
Check for outliers

\item {} 
\sphinxAtStartPar
Recommended size: 1,000\sphinxhyphen{}10,000 samples

\end{itemize}


\subsubsection{Model Training}
\label{\detokenize{usage:model-training}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Start with defaults

\item {} 
\sphinxAtStartPar
Increase complexity if R\(\sp{\text{2}}\) \textless{} 0.9

\item {} 
\sphinxAtStartPar
Reduce complexity if overfitting

\item {} 
\sphinxAtStartPar
Use early stopping for efficiency

\item {} 
\sphinxAtStartPar
Monitor training time

\end{itemize}


\subsubsection{Prediction}
\label{\detokenize{usage:prediction}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Ensure prediction data matches training scale

\item {} 
\sphinxAtStartPar
Use batch mode for efficiency

\item {} 
\sphinxAtStartPar
Single mode good for testing

\item {} 
\sphinxAtStartPar
Validate results against known values

\end{itemize}


\subsection{Troubleshooting}
\label{\detokenize{usage:troubleshooting}}

\subsubsection{Training Issues}
\label{\detokenize{usage:training-issues}}
\sphinxAtStartPar
\sphinxstylestrong{R\(\sp{\text{2}}\) score too low (\textless{}0.8):}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Increase network size

\item {} 
\sphinxAtStartPar
Increase iterations

\item {} 
\sphinxAtStartPar
Try different learning rate

\item {} 
\sphinxAtStartPar
Check data quality

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Training too slow (\textgreater{}60 seconds):}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduce network size

\item {} 
\sphinxAtStartPar
Reduce iterations

\item {} 
\sphinxAtStartPar
Enable early stopping

\item {} 
\sphinxAtStartPar
Use smaller dataset

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Model fails to converge:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduce learning rate

\item {} 
\sphinxAtStartPar
Increase iterations

\item {} 
\sphinxAtStartPar
Check for data issues

\end{itemize}


\subsubsection{Prediction Issues}
\label{\detokenize{usage:prediction-issues}}
\sphinxAtStartPar
\sphinxstylestrong{Predictions seem wrong:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Verify model is trained

\item {} 
\sphinxAtStartPar
Check prediction data format

\item {} 
\sphinxAtStartPar
Ensure feature scales match training

\item {} 
\sphinxAtStartPar
Review R\(\sp{\text{2}}\) score

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Batch prediction fails:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Verify data shape (n, 5)

\item {} 
\sphinxAtStartPar
Check file format (.pkl)

\item {} 
\sphinxAtStartPar
Ensure model is trained

\end{itemize}


\subsection{Keyboard Shortcuts}
\label{\detokenize{usage:keyboard-shortcuts}}
\sphinxAtStartPar
While using the application:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Refresh page}: Reset state

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Browser back}: Navigate between pages

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ctrl/Cmd + Click link}: Open in new tab

\end{itemize}


\subsection{Next Steps}
\label{\detokenize{usage:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} API reference

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{testing/overview::doc}]{\sphinxcrossref{\DUrole{doc}{Testing Overview}}}} \sphinxhyphen{} Run tests

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{datasets::doc}]{\sphinxcrossref{\DUrole{doc}{Dataset Specifications}}}} \sphinxhyphen{} Dataset specifications

\end{itemize}

\sphinxstepscope


\section{Dataset Specifications}
\label{\detokenize{datasets:dataset-specifications}}\label{\detokenize{datasets::doc}}
\sphinxAtStartPar
Complete specification for dataset formats used by the 5D Interpolator.


\subsection{Training Dataset Format}
\label{\detokenize{datasets:training-dataset-format}}

\subsubsection{Structure}
\label{\detokenize{datasets:structure}}
\sphinxAtStartPar
Training datasets must be Python pickle files containing a dictionary:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Feature matrix}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}   \PYG{c+c1}{\PYGZsh{} Target vector}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Requirements}
\label{\detokenize{datasets:requirements}}
\sphinxAtStartPar
\sphinxstylestrong{File Format:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Extension: \sphinxcode{\sphinxupquote{.pkl}}

\item {} 
\sphinxAtStartPar
Type: Python pickle file

\item {} 
\sphinxAtStartPar
Encoding: Binary

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{X (Features):}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Type: \sphinxcode{\sphinxupquote{numpy.ndarray}}

\item {} 
\sphinxAtStartPar
Shape: \sphinxcode{\sphinxupquote{(n\_samples, 5)}}

\item {} 
\sphinxAtStartPar
Dtype: \sphinxcode{\sphinxupquote{float32}} or \sphinxcode{\sphinxupquote{float64}}

\item {} 
\sphinxAtStartPar
Values: Any real numbers (will be standardized)

\item {} 
\sphinxAtStartPar
Constraints:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Must have exactly 5 features

\item {} 
\sphinxAtStartPar
No NaN or inf values

\item {} 
\sphinxAtStartPar
At least 100 samples recommended

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{y (Targets):}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Type: \sphinxcode{\sphinxupquote{numpy.ndarray}}

\item {} 
\sphinxAtStartPar
Shape: \sphinxcode{\sphinxupquote{(n\_samples,)}} \sphinxhyphen{} 1D array

\item {} 
\sphinxAtStartPar
Dtype: \sphinxcode{\sphinxupquote{float32}} or \sphinxcode{\sphinxupquote{float64}}

\item {} 
\sphinxAtStartPar
Values: Any real numbers

\item {} 
\sphinxAtStartPar
Constraints:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Must match number of samples in X

\item {} 
\sphinxAtStartPar
No NaN or inf values

\end{itemize}

\end{itemize}


\subsubsection{Example Creation}
\label{\detokenize{datasets:example-creation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Generate features (1000 samples, 5 features)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate targets}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create dataset dictionary}
\PYG{n}{dataset} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Save as pickle}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{training\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Validation}
\label{\detokenize{datasets:validation}}
\sphinxAtStartPar
The system automatically validates:

\sphinxAtStartPar
\(\checkmark\) File is readable pickle
\(\checkmark\) Contains ‘X’ and ‘y’ keys
\(\checkmark\) X has shape (n, 5)
\(\checkmark\) y has shape (n,)
\(\checkmark\) X and y have same number of samples
\(\checkmark\) No NaN or inf values


\subsection{Prediction Dataset Format}
\label{\detokenize{datasets:prediction-dataset-format}}

\subsubsection{Structure}
\label{\detokenize{datasets:id1}}
\sphinxAtStartPar
Prediction datasets must be Python pickle files containing a NumPy array:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{numpy}\PYG{o}{.}\PYG{n}{ndarray}  \PYG{c+c1}{\PYGZsh{} Shape: (n\PYGZus{}samples, 5)}
\end{sphinxVerbatim}


\subsubsection{Requirements}
\label{\detokenize{datasets:id2}}
\sphinxAtStartPar
\sphinxstylestrong{File Format:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Extension: \sphinxcode{\sphinxupquote{.pkl}}

\item {} 
\sphinxAtStartPar
Type: Python pickle file

\item {} 
\sphinxAtStartPar
Encoding: Binary

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Data:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Type: \sphinxcode{\sphinxupquote{numpy.ndarray}}

\item {} 
\sphinxAtStartPar
Shape: \sphinxcode{\sphinxupquote{(n\_samples, 5)}}

\item {} 
\sphinxAtStartPar
Dtype: \sphinxcode{\sphinxupquote{float32}} or \sphinxcode{\sphinxupquote{float64}}

\item {} 
\sphinxAtStartPar
Values: Any real numbers

\item {} 
\sphinxAtStartPar
Constraints:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Must have exactly 5 features

\item {} 
\sphinxAtStartPar
No NaN or inf values

\item {} 
\sphinxAtStartPar
Any number of samples

\end{itemize}

\end{itemize}


\subsubsection{Example Creation}
\label{\detokenize{datasets:id3}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Generate prediction inputs (100 samples, 5 features)}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save as pickle}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Data Preprocessing}
\label{\detokenize{datasets:data-preprocessing}}

\subsubsection{Automatic Standardization}
\label{\detokenize{datasets:automatic-standardization}}
\sphinxAtStartPar
The system automatically standardizes all features using:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{StandardScaler}

\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{StandardScaler}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X\PYGZus{}scaled} \PYG{o}{=} \PYG{n}{scaler}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This means:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Each feature is centered (mean = 0)

\item {} 
\sphinxAtStartPar
Each feature is scaled (std = 1)

\item {} 
\sphinxAtStartPar
Same transformation applied to predictions

\item {} 
\sphinxAtStartPar
No manual normalization needed

\end{itemize}


\subsubsection{Data Splitting}
\label{\detokenize{datasets:data-splitting}}
\sphinxAtStartPar
Training data is automatically split:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{60\%} Training set

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{20\%} Validation set

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{20\%} Test set

\end{itemize}

\sphinxAtStartPar
Split is random with fixed seed (42) for reproducibility.


\subsection{Best Practices}
\label{\detokenize{datasets:best-practices}}

\subsubsection{Dataset Size}
\label{\detokenize{datasets:dataset-size}}
\sphinxAtStartPar
\sphinxstylestrong{Recommended Sizes:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Minimum: 100 samples

\item {} 
\sphinxAtStartPar
Optimal: 1,000\sphinxhyphen{}10,000 samples

\item {} 
\sphinxAtStartPar
Maximum: No hard limit (training time increases)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Training Time by Size:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
100 samples: \textasciitilde{}5 seconds

\item {} 
\sphinxAtStartPar
1,000 samples: \textasciitilde{}15 seconds

\item {} 
\sphinxAtStartPar
10,000 samples: \textasciitilde{}45 seconds

\item {} 
\sphinxAtStartPar
100,000 samples: \textasciitilde{}5 minutes

\end{itemize}


\subsubsection{Data Quality}
\label{\detokenize{datasets:data-quality}}
\sphinxAtStartPar
\sphinxstylestrong{Check for:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Missing values (NaN)

\item {} 
\sphinxAtStartPar
Infinite values (inf)

\item {} 
\sphinxAtStartPar
Outliers (\textgreater{}3 std from mean)

\item {} 
\sphinxAtStartPar
Data type consistency

\item {} 
\sphinxAtStartPar
Correct dimensions

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example Validation:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{validate\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Validate dataset before saving\PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} Check shapes}
    \PYG{k}{assert} \PYG{n}{X}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X must be 2D}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{assert} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X must have 5 features}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{assert} \PYG{n}{y}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y must be 1D}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{assert} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{==} \PYG{n}{y}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X and y must have same samples}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} Check for invalid values}
    \PYG{k}{assert} \PYG{o+ow}{not} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X contains NaN}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{assert} \PYG{o+ow}{not} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isinf}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X contains inf}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{assert} \PYG{o+ow}{not} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y contains NaN}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{assert} \PYG{o+ow}{not} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isinf}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y contains inf}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\(\checkmark\) Dataset valid: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ samples, 5 features}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Use it}
\PYG{n}{validate\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Feature Engineering}
\label{\detokenize{datasets:feature-engineering}}
\sphinxAtStartPar
Consider:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Feature scaling (optional, auto\sphinxhyphen{}standardized)

\item {} 
\sphinxAtStartPar
Polynomial features for non\sphinxhyphen{}linear relationships

\item {} 
\sphinxAtStartPar
Interaction terms

\item {} 
\sphinxAtStartPar
Domain\sphinxhyphen{}specific transformations

\end{itemize}


\subsection{Common Use Cases}
\label{\detokenize{datasets:common-use-cases}}

\subsubsection{Regression Problems}
\label{\detokenize{datasets:regression-problems}}
\sphinxAtStartPar
\sphinxstylestrong{Example: Function Approximation}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Approximate function: f(x1,...,x5) = x1\PYGZca{}2 + x2*x3 \PYGZhy{} x4 + sin(x5)}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Scientific Data}
\label{\detokenize{datasets:scientific-data}}
\sphinxAtStartPar
\sphinxstylestrong{Example: Experimental Data}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Features: temperature, pressure, concentration, time, catalyst}
\PYG{c+c1}{\PYGZsh{} Target: reaction yield}

\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
        \PYG{p}{[}\PYG{l+m+mi}{300}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mi}{60}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Sample 1}
        \PYG{p}{[}\PYG{l+m+mi}{350}\PYG{p}{,} \PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mi}{90}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Sample 2}
        \PYG{c+c1}{\PYGZsh{} ... more samples}
    \PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.75}\PYG{p}{,} \PYG{l+m+mf}{0.82}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Yields}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Troubleshooting}
\label{\detokenize{datasets:troubleshooting}}

\subsubsection{Common Errors}
\label{\detokenize{datasets:common-errors}}
\sphinxAtStartPar
\sphinxstylestrong{“Invalid format: X must have shape (n, 5)”}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Wrong: X is (n, 3)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ✗}

\PYG{c+c1}{\PYGZsh{} Correct: X is (n, 5)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} \(\checkmark\)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{“Invalid format: Dictionary must contain ‘X’ and ‘y’ keys”}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Wrong: Missing \PYGZsq{}y\PYGZsq{} key}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{features}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} ✗}

\PYG{c+c1}{\PYGZsh{} Correct: Both keys present}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} \(\checkmark\)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{“Invalid format: X and y must have same number of samples”}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Wrong: Mismatched sizes}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{90}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ✗}

\PYG{c+c1}{\PYGZsh{} Correct: Same size}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} \(\checkmark\)}
\end{sphinxVerbatim}


\subsection{Dataset Templates}
\label{\detokenize{datasets:dataset-templates}}

\subsubsection{Simple Template}
\label{\detokenize{datasets:simple-template}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Simple dataset template}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Parameters}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} Generate data}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Simple sum}

\PYG{c+c1}{\PYGZsh{} Save}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simple\PYGZus{}dataset.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Complex Template}
\label{\detokenize{datasets:complex-template}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Complex dataset template with validation}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{create\PYGZus{}dataset}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{noise\PYGZus{}level}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Create validated dataset\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{n}{seed}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Generate features}
    \PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Complex target function}
    \PYG{n}{y} \PYG{o}{=} \PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+}
         \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{\PYGZhy{}}
         \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)} \PYG{o}{+}
         \PYG{n}{np}\PYG{o}{.}\PYG{n}{log1p}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Add noise}
    \PYG{n}{y} \PYG{o}{+}\PYG{o}{=} \PYG{n}{noise\PYGZus{}level} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Validate}
    \PYG{k}{assert} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape} \PYG{o}{==} \PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{k}{assert} \PYG{n}{y}\PYG{o}{.}\PYG{n}{shape} \PYG{o}{==} \PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,}\PYG{p}{)}
    \PYG{k}{assert} \PYG{o+ow}{not} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{assert} \PYG{o+ow}{not} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Create and save}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{create\PYGZus{}dataset}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{complex\PYGZus{}dataset.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Next Steps}
\label{\detokenize{datasets:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{quickstart::doc}]{\sphinxcrossref{\DUrole{doc}{Quick Start Guide}}}} \sphinxhyphen{} Upload and use datasets

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{usage::doc}]{\sphinxcrossref{\DUrole{doc}{Usage Guide}}}} \sphinxhyphen{} Detailed usage guide

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} API for dataset upload

\end{itemize}

\sphinxstepscope


\section{Backend API Reference}
\label{\detokenize{api/backend:backend-api-reference}}\label{\detokenize{api/backend::doc}}
\sphinxAtStartPar
This document provides a complete reference for the FastAPI backend REST API.


\subsection{Base URL}
\label{\detokenize{api/backend:base-url}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Development}: \sphinxcode{\sphinxupquote{http://localhost:8000}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Production}: Configure via \sphinxcode{\sphinxupquote{BACKEND\_URL}} environment variable

\end{itemize}


\subsection{Interactive Documentation}
\label{\detokenize{api/backend:interactive-documentation}}
\sphinxAtStartPar
FastAPI provides automatic interactive documentation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Swagger UI}: \sphinxurl{http://localhost:8000/docs}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{ReDoc}: \sphinxurl{http://localhost:8000/redoc}

\end{itemize}


\subsection{Health \& Status Endpoints}
\label{\detokenize{api/backend:health-status-endpoints}}

\subsubsection{GET /}
\label{\detokenize{api/backend:get}}
\sphinxAtStartPar
Welcome message and service identification.

\sphinxAtStartPar
\sphinxstylestrong{Response:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Hello from the 5D Interpolator Backend by bamk3!\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{GET /health}
\label{\detokenize{api/backend:get-health}}
\sphinxAtStartPar
Health check endpoint for monitoring and Docker containers.

\sphinxAtStartPar
\sphinxstylestrong{Response:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}status\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}healthy\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}service\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}5D Interpolator Backend by bamk3\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{GET /status}
\label{\detokenize{api/backend:get-status}}
\sphinxAtStartPar
Get the current status of uploaded data and trained models.

\sphinxAtStartPar
\sphinxstylestrong{Response:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}training\PYGZus{}data\PYGZus{}uploaded\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}model\PYGZus{}trained\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}prediction\PYGZus{}data\PYGZus{}uploaded\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{false}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Fields:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{training\_data\_uploaded}} (boolean): Whether training dataset is loaded

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{model\_trained}} (boolean): Whether a model has been trained

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{prediction\_data\_uploaded}} (boolean): Whether prediction dataset is loaded

\end{itemize}


\subsection{Dataset Upload Endpoints}
\label{\detokenize{api/backend:dataset-upload-endpoints}}

\subsubsection{POST /upload\sphinxhyphen{}fit\sphinxhyphen{}dataset/}
\label{\detokenize{api/backend:post-upload-fit-dataset}}
\sphinxAtStartPar
Upload a training dataset for model fitting.

\sphinxAtStartPar
\sphinxstylestrong{Request:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxcode{\sphinxupquote{POST}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxcode{\sphinxupquote{multipart/form\sphinxhyphen{}data}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}: File upload with key \sphinxcode{\sphinxupquote{file}}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{File Requirements:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Format}: Python pickle (\sphinxcode{\sphinxupquote{.pkl}})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Structure}: Dictionary with keys:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{X}}: NumPy array of shape \sphinxcode{\sphinxupquote{(n, 5)}} \sphinxhyphen{} feature matrix

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{y}}: NumPy array of shape \sphinxcode{\sphinxupquote{(n,)}} \sphinxhyphen{} target vector

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Validation}: Automatic shape and format checking

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example using curl:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }http://localhost:8000/upload\PYGZhy{}fit\PYGZhy{}dataset/\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}F\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}file=@training\PYGZus{}data.pkl\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Success Response (200 OK):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Training dataset uploaded and validated successfully\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}filename\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}training\PYGZus{}data.pkl\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}content\PYGZus{}type\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}application/octet\PYGZhy{}stream\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}filepath\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}uploaded\PYGZus{}datasets/training\PYGZus{}data.pkl\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}processing\PYGZus{}result\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}./uploaded\PYGZus{}datasets/training\PYGZus{}data.pkl\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}preview\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}X\PYGZus{}preview\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[[}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}0.5}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.9}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.5}\PYG{p}{],}\PYG{+w}{ }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}\PYG{p}{],}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}y\PYGZus{}preview\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mf}{3.45}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{2.11}\PYG{p}{,}\PYG{+w}{ }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}\PYG{p}{],}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}total\PYGZus{}samples\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1000}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}X\PYGZus{}shape\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{5}\PYG{p}{],}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}y\PYGZus{}shape\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mi}{1000}\PYG{p}{]}
\PYG{+w}{  }\PYG{p}{\PYGZcb{},}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}valid\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Error Responses:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{400 Bad Request}}: Invalid file format or structure

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{500 Internal Server Error}}: Server error during processing

\end{itemize}


\subsubsection{POST /upload\sphinxhyphen{}predict\sphinxhyphen{}dataset/}
\label{\detokenize{api/backend:post-upload-predict-dataset}}
\sphinxAtStartPar
Upload a prediction dataset.

\sphinxAtStartPar
\sphinxstylestrong{Request:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxcode{\sphinxupquote{POST}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxcode{\sphinxupquote{multipart/form\sphinxhyphen{}data}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}: File upload with key \sphinxcode{\sphinxupquote{file}}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{File Requirements:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Format}: Python pickle (\sphinxcode{\sphinxupquote{.pkl}})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Structure}: NumPy array of shape \sphinxcode{\sphinxupquote{(n, 5)}}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example using curl:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }http://localhost:8000/upload\PYGZhy{}predict\PYGZhy{}dataset/\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}F\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}file=@prediction\PYGZus{}data.pkl\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Success Response (200 OK):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Prediction dataset uploaded and validated successfully\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}filename\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}prediction\PYGZus{}data.pkl\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}content\PYGZus{}type\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}application/octet\PYGZhy{}stream\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}filepath\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}uploaded\PYGZus{}datasets/prediction\PYGZus{}data.pkl\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}predict\PYGZus{}input\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}./uploaded\PYGZus{}datasets/prediction\PYGZus{}data.pkl\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}preview\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}X\PYGZus{}preview\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[[}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}0.5}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.9}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.5}\PYG{p}{],}\PYG{+w}{ }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}\PYG{p}{],}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}total\PYGZus{}samples\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{100}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}X\PYGZus{}shape\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{5}\PYG{p}{]}
\PYG{+w}{  }\PYG{p}{\PYGZcb{},}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}valid\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Model Training Endpoints}
\label{\detokenize{api/backend:model-training-endpoints}}

\subsubsection{GET /hyperparameters/defaults}
\label{\detokenize{api/backend:get-hyperparameters-defaults}}
\sphinxAtStartPar
Get default hyperparameter values.

\sphinxAtStartPar
\sphinxstylestrong{Response:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}1\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{64}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}2\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{32}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}3\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{16}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}learning\PYGZus{}rate\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.001}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}iterations\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{500}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}early\PYGZus{}stopping\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{POST /start\sphinxhyphen{}training/}
\label{\detokenize{api/backend:post-start-training}}
\sphinxAtStartPar
Train a neural network model with optional custom hyperparameters.

\sphinxAtStartPar
\sphinxstylestrong{Request:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxcode{\sphinxupquote{POST}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxcode{\sphinxupquote{application/json}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body} (optional):

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hyperparameters\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}1\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{64}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}2\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{32}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}3\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{16}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}learning\PYGZus{}rate\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.001}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}iterations\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{500}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}early\PYGZus{}stopping\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Hyperparameter Constraints:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{hidden\_layer\_1}}: 8\sphinxhyphen{}256 (int)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{hidden\_layer\_2}}: 8\sphinxhyphen{}128 (int)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{hidden\_layer\_3}}: 4\sphinxhyphen{}64 (int)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{learning\_rate}}: 0.0001\sphinxhyphen{}0.01 (float)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{max\_iterations}}: 100\sphinxhyphen{}2000 (int)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{early\_stopping}}: true/false (boolean)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example using curl:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} With default hyperparameters}
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }http://localhost:8000/start\PYGZhy{}training/\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Content\PYGZhy{}Type: application/json\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}d\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}\PYGZcb{}\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} With custom hyperparameters}
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }http://localhost:8000/start\PYGZhy{}training/\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Content\PYGZhy{}Type: application/json\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}d\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}}
\PYG{l+s+s1}{    \PYGZdq{}hyperparameters\PYGZdq{}: \PYGZob{}}
\PYG{l+s+s1}{      \PYGZdq{}hidden\PYGZus{}layer\PYGZus{}1\PYGZdq{}: 128,}
\PYG{l+s+s1}{      \PYGZdq{}hidden\PYGZus{}layer\PYGZus{}2\PYGZdq{}: 64,}
\PYG{l+s+s1}{      \PYGZdq{}hidden\PYGZus{}layer\PYGZus{}3\PYGZdq{}: 32,}
\PYG{l+s+s1}{      \PYGZdq{}learning\PYGZus{}rate\PYGZdq{}: 0.01,}
\PYG{l+s+s1}{      \PYGZdq{}max\PYGZus{}iterations\PYGZdq{}: 1000,}
\PYG{l+s+s1}{      \PYGZdq{}early\PYGZus{}stopping\PYGZdq{}: true}
\PYG{l+s+s1}{    \PYGZcb{}}
\PYG{l+s+s1}{  \PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Success Response (200 OK):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Training job initiated and completed successfully.\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}function\PYGZus{}result\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}mse\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.0123}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}mae\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.0987}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}rmse\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.1109}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}r2\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.9876}
\PYG{+w}{  }\PYG{p}{\PYGZcb{},}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hyperparameters\PYGZus{}used\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layers\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mi}{64}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{32}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mi}{16}\PYG{p}{],}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}learning\PYGZus{}rate\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.001}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}iterations\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{500}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}early\PYGZus{}stopping\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Error Response (400 Bad Request):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}detail\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}No training data uploaded. Please upload a dataset first.\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Prediction Endpoints}
\label{\detokenize{api/backend:prediction-endpoints}}

\subsubsection{POST /start\sphinxhyphen{}predict/}
\label{\detokenize{api/backend:post-start-predict}}
\sphinxAtStartPar
Generate batch predictions using uploaded dataset.

\sphinxAtStartPar
\sphinxstylestrong{Request:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxcode{\sphinxupquote{POST}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxcode{\sphinxupquote{application/json}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}: \sphinxcode{\sphinxupquote{\{\}}} (empty JSON object)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Model must be trained

\item {} 
\sphinxAtStartPar
Prediction dataset must be uploaded

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example using curl:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }http://localhost:8000/start\PYGZhy{}predict/\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Content\PYGZhy{}Type: application/json\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}d\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}\PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Success Response (200 OK):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Batch prediction completed successfully.\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}function\PYGZus{}result\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}[3.456 2.789 1.234 ...]\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}prediction\PYGZus{}type\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}batch\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{POST /predict\sphinxhyphen{}single/}
\label{\detokenize{api/backend:post-predict-single}}
\sphinxAtStartPar
Generate a single prediction from 5 input features.

\sphinxAtStartPar
\sphinxstylestrong{Request:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Method}: \sphinxcode{\sphinxupquote{POST}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Content\sphinxhyphen{}Type}: \sphinxcode{\sphinxupquote{application/json}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Body}:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}features\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}0.5}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.9}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.5}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Model must be trained

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example using curl:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
curl\PYG{+w}{ }\PYGZhy{}X\PYG{+w}{ }POST\PYG{+w}{ }http://localhost:8000/predict\PYGZhy{}single/\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}H\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Content\PYGZhy{}Type: application/json\PYGZdq{}}\PYG{+w}{ }\PYG{l+s+se}{\PYGZbs{}}
\PYG{+w}{  }\PYGZhy{}d\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}\PYGZdq{}features\PYGZdq{}: [1.2, \PYGZhy{}0.5, 0.9, \PYGZhy{}1.2, 0.5]\PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Success Response (200 OK):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}message\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Single prediction completed successfully.\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}input\PYGZus{}features\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}0.5}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.9}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{\PYGZhy{}1.2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0.5}\PYG{p}{],}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}prediction\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{3.456789}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}prediction\PYGZus{}type\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}single\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Error Response (400 Bad Request):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}detail\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Expected 5 features, got 3\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Python Client Examples}
\label{\detokenize{api/backend:python-client-examples}}

\subsubsection{Using requests library}
\label{\detokenize{api/backend:using-requests-library}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{requests}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}

\PYG{n}{BASE\PYGZus{}URL} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://localhost:8000}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Upload training dataset}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{training\PYGZus{}data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{response} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
        \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/upload\PYGZhy{}fit\PYGZhy{}dataset/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{files}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{f}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train model with custom hyperparameters}
\PYG{n}{response} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
    \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/start\PYGZhy{}training/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{json}\PYG{o}{=}\PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hyperparameters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hidden\PYGZus{}layer\PYGZus{}1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{128}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{learning\PYGZus{}rate}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}iterations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1000}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Single prediction}
\PYG{n}{response} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
    \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{BASE\PYGZus{}URL}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/predict\PYGZhy{}single/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{json}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{features}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Error Handling}
\label{\detokenize{api/backend:error-handling}}
\sphinxAtStartPar
All endpoints use standard HTTP status codes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{200 OK}}: Successful request

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{400 Bad Request}}: Invalid input or missing prerequisites

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{422 Unprocessable Entity}}: Validation error

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{500 Internal Server Error}}: Server\sphinxhyphen{}side error

\end{itemize}

\sphinxAtStartPar
Error responses include a \sphinxcode{\sphinxupquote{detail}} field with description:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}detail\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}Error description here\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Rate Limiting}
\label{\detokenize{api/backend:rate-limiting}}
\sphinxAtStartPar
Currently no rate limiting is implemented. For production deployment, consider adding rate limiting middleware.


\subsection{Authentication}
\label{\detokenize{api/backend:authentication}}
\sphinxAtStartPar
Currently no authentication is required. For production deployment with sensitive data, implement authentication middleware.

\sphinxstepscope


\section{Frontend Components}
\label{\detokenize{api/frontend:frontend-components}}\label{\detokenize{api/frontend::doc}}
\sphinxAtStartPar
This section documents the React components in the Next.js frontend application.


\subsection{Technology Stack}
\label{\detokenize{api/frontend:technology-stack}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Framework}: Next.js 16.0.3 with App Router

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{React}: 19.2.0

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Language}: TypeScript 5

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Styling}: Tailwind CSS v4

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fonts}: Geist Sans and Geist Mono

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Build Tool}: Turbopack

\end{itemize}


\subsection{Project Structure}
\label{\detokenize{api/frontend:project-structure}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
frontend/
├── src/
│   └── app/
│       ├── layout.tsx        \PYGZsh{} Root layout
│       ├── page.tsx          \PYGZsh{} Home page
│       ├── globals.css       \PYGZsh{} Global styles
│       ├── upload/
│       │   └── page.tsx      \PYGZsh{} Upload page
│       ├── train/
│       │   └── page.tsx      \PYGZsh{} Training page
│       └── predict/
│           └── page.tsx      \PYGZsh{} Prediction page
├── public/
├── package.json
└── next.config.ts
\end{sphinxVerbatim}


\subsection{Root Layout}
\label{\detokenize{api/frontend:root-layout}}
\sphinxAtStartPar
Location: \sphinxcode{\sphinxupquote{src/app/layout.tsx}}

\sphinxAtStartPar
The root layout component that wraps all pages.

\sphinxAtStartPar
\sphinxstylestrong{Features:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Loads Geist Sans and Geist Mono fonts

\item {} 
\sphinxAtStartPar
Sets up HTML metadata

\item {} 
\sphinxAtStartPar
Provides consistent layout structure

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Code Structure:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nx}{GeistSans}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}\PYG{+w}{ }\PYG{k+kr}{from}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}geist/font/sans\PYGZdq{}}\PYG{p}{;}
\PYG{k}{import}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nx}{GeistMono}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}\PYG{+w}{ }\PYG{k+kr}{from}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}geist/font/mono\PYGZdq{}}\PYG{p}{;}
\PYG{k}{import}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}./globals.css\PYGZdq{}}\PYG{p}{;}

\PYG{k}{export}\PYG{+w}{ }\PYG{k}{default}\PYG{+w}{ }\PYG{k+kd}{function}\PYG{+w}{ }\PYG{n+nx}{RootLayout}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{children}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}\PYG{o}{:}\PYG{+w}{ }\PYG{n+nx}{Readonly}\PYG{o}{\PYGZlt{}}\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{children}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{React.ReactNode}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZgt{}}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k}{return}\PYG{+w}{ }\PYG{p}{(}
\PYG{+w}{    }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{html}\PYG{+w}{ }\PYG{n+nx}{lang}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}en\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{      }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{body}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+sb}{`}\PYG{l+s+si}{\PYGZdl{}\PYGZob{}}\PYG{n+nx}{GeistSans}\PYG{p}{.}\PYG{n+nx}{variable}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+sb}{ }\PYG{l+s+si}{\PYGZdl{}\PYGZob{}}\PYG{n+nx}{GeistMono}\PYG{p}{.}\PYG{n+nx}{variable}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+sb}{`}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{        }\PYG{p}{\PYGZob{}}\PYG{n+nx}{children}\PYG{p}{\PYGZcb{}}
\PYG{+w}{      }\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{b}\PYG{err}{o}\PYG{err}{d}\PYG{err}{y}\PYG{err}{\PYGZgt{}}
\PYG{+w}{    }\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{h}\PYG{err}{t}\PYG{err}{m}\PYG{err}{l}\PYG{err}{\PYGZgt{}}
\PYG{+w}{  }\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Home Page}
\label{\detokenize{api/frontend:home-page}}
\sphinxAtStartPar
Location: \sphinxcode{\sphinxupquote{src/app/page.tsx}}

\sphinxAtStartPar
The landing page of the application.

\sphinxAtStartPar
\sphinxstylestrong{Features:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Welcome message

\item {} 
\sphinxAtStartPar
Navigation links to Upload, Train, and Predict pages

\item {} 
\sphinxAtStartPar
Responsive design

\item {} 
\sphinxAtStartPar
Dark mode support

\end{itemize}


\subsection{Upload Page}
\label{\detokenize{api/frontend:upload-page}}
\sphinxAtStartPar
Location: \sphinxcode{\sphinxupquote{src/app/upload/page.tsx}}

\sphinxAtStartPar
Component for uploading training and prediction datasets.


\subsubsection{State Management}
\label{\detokenize{api/frontend:state-management}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{datasetType}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setDatasetType}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}training\PYGZsq{}}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}prediction\PYGZsq{}}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}training\PYGZsq{}}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{file}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setFile}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{File}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{uploading}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setUploading}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{uploadResult}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setUploadResult}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{any}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{error}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setError}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{string}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Key States:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{datasetType}}: Type of dataset being uploaded

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{file}}: Selected file object

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{uploading}}: Upload in progress flag

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{uploadResult}}: Server response with dataset info

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{error}}: Error message if upload fails

\end{itemize}


\subsubsection{Upload Process}
\label{\detokenize{api/frontend:upload-process}}
\sphinxAtStartPar
\sphinxstylestrong{Training Dataset:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{handleUpload}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{async}\PYG{+w}{ }\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{formData}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{o+ow}{new}\PYG{+w}{ }\PYG{n+nx}{FormData}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{formData}\PYG{p}{.}\PYG{n+nx}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}file\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{file}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{fetch}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}http://localhost:8000/upload\PYGZhy{}fit\PYGZhy{}dataset/\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nx}{method}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}POST\PYGZsq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{body}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{formData}\PYG{p}{,}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{data}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{json}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setUploadResult}\PYG{p}{(}\PYG{n+nx}{data}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Prediction Dataset:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{fetch}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}http://localhost:8000/upload\PYGZhy{}predict\PYGZhy{}dataset/\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{method}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}POST\PYGZsq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nx}{body}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{formData}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Upload Result Display:}

\sphinxAtStartPar
Shows preview of uploaded data:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Total samples

\item {} 
\sphinxAtStartPar
Data shape

\item {} 
\sphinxAtStartPar
First 5 rows of data

\item {} 
\sphinxAtStartPar
Proceed to next step button

\end{itemize}


\subsection{Train Page}
\label{\detokenize{api/frontend:train-page}}
\sphinxAtStartPar
Location: \sphinxcode{\sphinxupquote{src/app/train/page.tsx}}

\sphinxAtStartPar
Component for training the neural network model with configurable hyperparameters.


\subsubsection{State Management}
\label{\detokenize{api/frontend:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{trainingDataUploaded}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setTrainingDataUploaded}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{modelTrained}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setModelTrained}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{training}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setTraining}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{trainResult}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setTrainResult}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{any}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{hyperparameters}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setHyperparameters}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{hidden\PYGZus{}layer\PYGZus{}1}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{64}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nx}{hidden\PYGZus{}layer\PYGZus{}2}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{32}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nx}{hidden\PYGZus{}layer\PYGZus{}3}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{16}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nx}{learning\PYGZus{}rate}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{0.001}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nx}{max\PYGZus{}iterations}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{500}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nx}{early\PYGZus{}stopping}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{true}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Hyperparameter Controls}
\label{\detokenize{api/frontend:hyperparameter-controls}}
\sphinxAtStartPar
\sphinxstylestrong{Hidden Layer Sizes:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// Layer 1: 8\PYGZhy{}256 neurons}
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{input}
\PYG{+w}{  }\PYG{k+kr}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}range\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{min}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}8\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{max}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}256\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{step}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}8\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{value}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{hyperparameters}\PYG{p}{.}\PYG{n+nx}{hidden\PYGZus{}layer\PYGZus{}1}\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n+nx}{onChange}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{n+nx}{setHyperparameters}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{p}{...}\PYG{n+nx}{hyperparameters}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{hidden\PYGZus{}layer\PYGZus{}1}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{parseInt}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{.}\PYG{n+nx}{target}\PYG{p}{.}\PYG{n+nx}{value}\PYG{p}{)}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\PYG{err}{/}\PYG{err}{\PYGZgt{}}

\PYG{c+c1}{// Layer 2: 8\PYGZhy{}128 neurons}
\PYG{c+c1}{// Layer 3: 4\PYGZhy{}64 neurons}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Learning Rate:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{input}
\PYG{+w}{  }\PYG{k+kr}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}range\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{min}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}0.0001\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{max}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}0.01\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{step}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}0.0001\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{value}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{hyperparameters}\PYG{p}{.}\PYG{n+nx}{learning\PYGZus{}rate}\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n+nx}{onChange}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{n+nx}{setHyperparameters}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{p}{...}\PYG{n+nx}{hyperparameters}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{learning\PYGZus{}rate}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{parseFloat}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{.}\PYG{n+nx}{target}\PYG{p}{.}\PYG{n+nx}{value}\PYG{p}{)}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\PYG{err}{/}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Max Iterations:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{input}
\PYG{+w}{  }\PYG{k+kr}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}range\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{min}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}100\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{max}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}2000\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{step}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}100\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{value}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{hyperparameters}\PYG{p}{.}\PYG{n+nx}{max\PYGZus{}iterations}\PYG{p}{\PYGZcb{}}
\PYG{err}{/}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Early Stopping:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{input}
\PYG{+w}{  }\PYG{k+kr}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}checkbox\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{checked}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{hyperparameters}\PYG{p}{.}\PYG{n+nx}{early\PYGZus{}stopping}\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n+nx}{onChange}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{n+nx}{setHyperparameters}\PYG{p}{(}\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{p}{...}\PYG{n+nx}{hyperparameters}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{early\PYGZus{}stopping}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{e.target.checked}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\PYG{err}{/}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}


\subsubsection{Training Process}
\label{\detokenize{api/frontend:training-process}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{handleTrain}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{async}\PYG{+w}{ }\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{setTraining}\PYG{p}{(}\PYG{k+kc}{true}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setTrainResult}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{fetch}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}http://localhost:8000/start\PYGZhy{}training/\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nx}{method}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}POST\PYGZsq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{headers}\PYG{o}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Content\PYGZhy{}Type\PYGZsq{}}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}application/json\PYGZsq{}}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{body}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{JSON.stringify}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nx}{hyperparameters}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{data}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{json}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setTrainResult}\PYG{p}{(}\PYG{n+nx}{data}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setModelTrained}\PYG{p}{(}\PYG{k+kc}{true}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setTraining}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Results Display}
\label{\detokenize{api/frontend:results-display}}
\sphinxAtStartPar
\sphinxstylestrong{Performance Metrics:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
R\(\sp{\text{2}}\) Score (coefficient of determination)

\item {} 
\sphinxAtStartPar
MSE (Mean Squared Error)

\item {} 
\sphinxAtStartPar
MAE (Mean Absolute Error)

\item {} 
\sphinxAtStartPar
RMSE (Root Mean Squared Error)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Hyperparameters Used:}

\sphinxAtStartPar
Displays the actual configuration used for training.


\subsubsection{Button Logic}
\label{\detokenize{api/frontend:button-logic}}
\sphinxAtStartPar
The training button is disabled when:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Training is in progress

\item {} 
\sphinxAtStartPar
No training data uploaded

\item {} 
\sphinxAtStartPar
Status is being checked

\item {} 
\sphinxAtStartPar
Model already trained on current dataset

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nx}{disabled}\PYG{o}{=}\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{training}\PYG{+w}{ }\PYG{o}{||}
\PYG{+w}{  }\PYG{o}{!}\PYG{n+nx}{trainingDataUploaded}\PYG{+w}{ }\PYG{o}{||}
\PYG{+w}{  }\PYG{n+nx}{checkingStatus}\PYG{+w}{ }\PYG{o}{||}
\PYG{+w}{  }\PYG{p}{(}\PYG{n+nx}{trainingDataUploaded}\PYG{+w}{ }\PYG{o}{\PYGZam{}\PYGZam{}}\PYG{+w}{ }\PYG{n+nx}{modelTrained}\PYG{+w}{ }\PYG{o}{\PYGZam{}\PYGZam{}}\PYG{+w}{ }\PYG{o}{!}\PYG{n+nx}{trainResult}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Predict Page}
\label{\detokenize{api/frontend:predict-page}}
\sphinxAtStartPar
Location: \sphinxcode{\sphinxupquote{src/app/predict/page.tsx}}

\sphinxAtStartPar
Component for making predictions using the trained model.


\subsubsection{State Management}
\label{\detokenize{api/frontend:id2}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{predictionMode}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setPredictionMode}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}batch\PYGZsq{}}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}single\PYGZsq{}}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}batch\PYGZsq{}}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{predictionDataUploaded}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setPredictionDataUploaded}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{batchPredictionDone}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setBatchPredictionDone}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{modelTrained}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setModelTrained}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{predicting}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setPredicting}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{predictionResult}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setPredictionResult}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{any}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{singleInput}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setSingleInput}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{number}\PYG{p}{[}\PYG{p}{]}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{singlePrediction}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setSinglePrediction}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{number}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Batch Prediction}
\label{\detokenize{api/frontend:batch-prediction}}
\sphinxAtStartPar
\sphinxstylestrong{Process:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{handleBatchPrediction}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{async}\PYG{+w}{ }\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{setPredicting}\PYG{p}{(}\PYG{k+kc}{true}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setPredictionResult}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{fetch}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}http://localhost:8000/start\PYGZhy{}predict/\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nx}{method}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}POST\PYGZsq{}}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{data}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{json}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setPredictionResult}\PYG{p}{(}\PYG{n+nx}{data}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setBatchPredictionDone}\PYG{p}{(}\PYG{k+kc}{true}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setPredicting}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Results Display:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Total predictions made

\item {} 
\sphinxAtStartPar
First 5 predictions preview

\item {} 
\sphinxAtStartPar
Download button for full results

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Button Disabled When:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Prediction in progress

\item {} 
\sphinxAtStartPar
No prediction data uploaded

\item {} 
\sphinxAtStartPar
Model not trained

\item {} 
\sphinxAtStartPar
Batch prediction already done on current dataset

\end{itemize}


\subsubsection{Single Prediction}
\label{\detokenize{api/frontend:single-prediction}}
\sphinxAtStartPar
\sphinxstylestrong{Input Interface:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{p}{[}\PYG{l+m+mf}{0}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{1}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{2}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{3}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{4}\PYG{p}{]}\PYG{p}{.}\PYG{n+nx}{map}\PYG{p}{(}\PYG{p}{(}\PYG{n+nx}{i}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{p}{(}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{div}\PYG{+w}{ }\PYG{n+nx}{key}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{i}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{    }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{label}\PYG{o}{\PYGZgt{}}\PYG{n+nx}{Feature}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{n+nx}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mf}{1}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{l}\PYG{err}{a}\PYG{err}{b}\PYG{err}{e}\PYG{err}{l}\PYG{err}{\PYGZgt{}}
\PYG{+w}{    }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{input}
\PYG{+w}{      }\PYG{k+kr}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}number\PYGZdq{}}
\PYG{+w}{      }\PYG{n+nx}{step}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}0.0001\PYGZdq{}}
\PYG{+w}{      }\PYG{n+nx}{value}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{singleInput}\PYG{p}{[}\PYG{n+nx}{i}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{+w}{      }\PYG{n+nx}{onChange}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{        }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{newInput}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{p}{[}\PYG{p}{...}\PYG{n+nx}{singleInput}\PYG{p}{]}
\PYG{+w}{        }\PYG{n+nx}{newInput}\PYG{p}{[}\PYG{n+nx}{i}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nb}{parseFloat}\PYG{p}{(}\PYG{n+nx}{e}\PYG{p}{.}\PYG{n+nx}{target}\PYG{p}{.}\PYG{n+nx}{value}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{||}\PYG{+w}{ }\PYG{l+m+mf}{0}
\PYG{+w}{        }\PYG{n+nx}{setSingleInput}\PYG{p}{(}\PYG{n+nx}{newInput}\PYG{p}{)}
\PYG{+w}{      }\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
\PYG{+w}{    }\PYG{o}{/}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{d}\PYG{err}{i}\PYG{err}{v}\PYG{err}{\PYGZgt{}}
\PYG{p}{)}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Prediction Request:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{handleSinglePrediction}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{async}\PYG{+w}{ }\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{=\PYGZgt{}}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{setPredicting}\PYG{p}{(}\PYG{k+kc}{true}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{fetch}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}http://localhost:8000/predict\PYGZhy{}single/\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nx}{method}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}POST\PYGZsq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{headers}\PYG{o}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}Content\PYGZhy{}Type\PYGZsq{}}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}application/json\PYGZsq{}}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nx}{body}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{JSON.stringify}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nx}{features}\PYG{o}{:}\PYG{+w}{ }\PYG{k+kt}{singleInput}\PYG{+w}{ }\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{data}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{json}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setSinglePrediction}\PYG{p}{(}\PYG{n+nx}{data}\PYG{p}{.}\PYG{n+nx}{prediction}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setPredicting}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Result Display:}

\sphinxAtStartPar
Shows input features and predicted value in a clean layout.


\subsection{Styling}
\label{\detokenize{api/frontend:styling}}

\subsubsection{Global Styles}
\label{\detokenize{api/frontend:global-styles}}
\sphinxAtStartPar
Location: \sphinxcode{\sphinxupquote{src/app/globals.css}}

\sphinxAtStartPar
Uses Tailwind CSS v4 with custom theme configuration:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{@}\PYG{k}{import}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}tailwindcss\PYGZdq{}}\PYG{p}{;}

\PYG{p}{@}\PYG{k}{theme}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZhy{}\PYGZhy{}font\PYGZhy{}family\PYGZhy{}sans}\PYG{o}{:}\PYG{+w}{ }\PYG{n+nt}{var}\PYG{o}{(}\PYG{n+nt}{\PYGZhy{}\PYGZhy{}font\PYGZhy{}geist\PYGZhy{}sans}\PYG{o}{)}\PYG{o}{;}
\PYG{+w}{  }\PYG{n+nt}{\PYGZhy{}\PYGZhy{}font\PYGZhy{}family\PYGZhy{}mono}\PYG{o}{:}\PYG{+w}{ }\PYG{n+nt}{var}\PYG{o}{(}\PYG{n+nt}{\PYGZhy{}\PYGZhy{}font\PYGZhy{}geist\PYGZhy{}mono}\PYG{o}{)}\PYG{o}{;}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsubsection{Common Patterns}
\label{\detokenize{api/frontend:common-patterns}}
\sphinxAtStartPar
\sphinxstylestrong{Container Layout:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{div}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}min\PYGZhy{}h\PYGZhy{}screen flex flex\PYGZhy{}col bg\PYGZhy{}gray\PYGZhy{}50 dark:bg\PYGZhy{}gray\PYGZhy{}950\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{header}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}...sticky top\PYGZhy{}0 z\PYGZhy{}10\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}\PYG{c+cm}{/* Header content */}\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{h}\PYG{err}{e}\PYG{err}{a}\PYG{err}{d}\PYG{err}{e}\PYG{err}{r}\PYG{err}{\PYGZgt{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{main}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}flex\PYGZhy{}1 flex items\PYGZhy{}start justify\PYGZhy{}center px\PYGZhy{}6 py\PYGZhy{}8 overflow\PYGZhy{}y\PYGZhy{}auto\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{    }\PYG{p}{\PYGZob{}}\PYG{c+cm}{/* Page content */}\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{m}\PYG{err}{a}\PYG{err}{i}\PYG{err}{n}\PYG{err}{\PYGZgt{}}
\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{d}\PYG{err}{i}\PYG{err}{v}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Cards:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{div}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}bg\PYGZhy{}white dark:bg\PYGZhy{}gray\PYGZhy{}900 shadow\PYGZhy{}lg rounded\PYGZhy{}lg p\PYGZhy{}6\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{p}{\PYGZob{}}\PYG{c+cm}{/* Card content */}\PYG{p}{\PYGZcb{}}
\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{d}\PYG{err}{i}\PYG{err}{v}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Buttons:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// Primary button}
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{button}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}px\PYGZhy{}6 py\PYGZhy{}3 bg\PYGZhy{}blue\PYGZhy{}600 text\PYGZhy{}white rounded\PYGZhy{}lg hover:bg\PYGZhy{}blue\PYGZhy{}700\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{p}{\PYGZob{}}\PYG{n+nx}{buttonText}\PYG{p}{\PYGZcb{}}
\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{b}\PYG{err}{u}\PYG{err}{t}\PYG{err}{t}\PYG{err}{o}\PYG{err}{n}\PYG{err}{\PYGZgt{}}

\PYG{c+c1}{// Disabled button}
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{button}
\PYG{+w}{  }\PYG{n+nx}{disabled}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n+nx}{isDisabled}\PYG{p}{\PYGZcb{}}
\PYG{+w}{  }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}...disabled:opacity\PYGZhy{}50 disabled:cursor\PYGZhy{}not\PYGZhy{}allowed\PYGZdq{}}
\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{p}{\PYGZob{}}\PYG{n+nx}{buttonText}\PYG{p}{\PYGZcb{}}
\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{b}\PYG{err}{u}\PYG{err}{t}\PYG{err}{t}\PYG{err}{o}\PYG{err}{n}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Form Inputs:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{input}
\PYG{+w}{  }\PYG{k+kr}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}number\PYGZdq{}}
\PYG{+w}{  }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}w\PYGZhy{}full px\PYGZhy{}3 py\PYGZhy{}2 border rounded\PYGZhy{}lg focus:ring\PYGZhy{}2 focus:ring\PYGZhy{}blue\PYGZhy{}500\PYGZdq{}}
\PYG{err}{/}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}


\subsection{Error Handling}
\label{\detokenize{api/frontend:error-handling}}
\sphinxAtStartPar
All components implement consistent error handling:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{try}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{fetch}\PYG{p}{(}\PYG{n+nx}{url}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{options}\PYG{p}{)}

\PYG{+w}{  }\PYG{k}{if}\PYG{+w}{ }\PYG{p}{(}\PYG{o}{!}\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{ok}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{k}{throw}\PYG{+w}{ }\PYG{o+ow}{new}\PYG{+w}{ }\PYG{n+ne}{Error}\PYG{p}{(}\PYG{l+s+sb}{`}\PYG{l+s+sb}{HTTP error! status: }\PYG{l+s+si}{\PYGZdl{}\PYGZob{}}\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{status}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+sb}{`}\PYG{p}{)}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}

\PYG{+w}{  }\PYG{k+kd}{const}\PYG{+w}{ }\PYG{n+nx}{data}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{k}{await}\PYG{+w}{ }\PYG{n+nx}{response}\PYG{p}{.}\PYG{n+nx}{json}\PYG{p}{(}\PYG{p}{)}
\PYG{+w}{  }\PYG{c+c1}{// Handle success}
\PYG{p}{\PYGZcb{}}\PYG{+w}{ }\PYG{k}{catch}\PYG{+w}{ }\PYG{p}{(}\PYG{n+nx}{error}\PYG{p}{)}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nx}{console}\PYG{p}{.}\PYG{n+nx}{error}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Error:\PYGZsq{}}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{error}\PYG{p}{)}
\PYG{+w}{  }\PYG{n+nx}{setError}\PYG{p}{(}\PYG{n+nx}{error}\PYG{p}{.}\PYG{n+nx}{message}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Error messages are displayed to users in red alert boxes:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{n+nx}{error}\PYG{+w}{ }\PYG{o}{\PYGZam{}\PYGZam{}}\PYG{+w}{ }\PYG{p}{(}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{div}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}p\PYGZhy{}4 bg\PYGZhy{}red\PYGZhy{}50 border border\PYGZhy{}red\PYGZhy{}200 rounded\PYGZhy{}lg\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{    }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{p}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}text\PYGZhy{}red\PYGZhy{}800\PYGZdq{}}\PYG{o}{\PYGZgt{}}\PYG{p}{\PYGZob{}}\PYG{n+nx}{error}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{p}\PYG{err}{\PYGZgt{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{d}\PYG{err}{i}\PYG{err}{v}\PYG{err}{\PYGZgt{}}
\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Responsive Design}
\label{\detokenize{api/frontend:responsive-design}}
\sphinxAtStartPar
All components are responsive and work on mobile devices:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Flexible layouts using flexbox

\item {} 
\sphinxAtStartPar
Responsive padding and margins

\item {} 
\sphinxAtStartPar
Mobile\sphinxhyphen{}friendly form controls

\item {} 
\sphinxAtStartPar
Readable font sizes on all screens

\end{itemize}


\subsection{Dark Mode}
\label{\detokenize{api/frontend:dark-mode}}
\sphinxAtStartPar
Full dark mode support using Tailwind’s dark variant:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZlt{}}\PYG{n+nx}{div}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}bg\PYGZhy{}white dark:bg\PYGZhy{}gray\PYGZhy{}900\PYGZdq{}}\PYG{o}{\PYGZgt{}}
\PYG{+w}{  }\PYG{o}{\PYGZlt{}}\PYG{n+nx}{p}\PYG{+w}{ }\PYG{n+nx}{className}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}text\PYGZhy{}gray\PYGZhy{}900 dark:text\PYGZhy{}gray\PYGZhy{}100\PYGZdq{}}\PYG{o}{\PYGZgt{}}\PYG{n+nx}{Content}\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{p}\PYG{err}{\PYGZgt{}}
\PYG{o}{\PYGZlt{}}\PYG{err}{/}\PYG{err}{d}\PYG{err}{i}\PYG{err}{v}\PYG{err}{\PYGZgt{}}
\end{sphinxVerbatim}


\subsection{Best Practices}
\label{\detokenize{api/frontend:best-practices}}
\sphinxAtStartPar
The frontend follows these practices:
\begin{itemize}
\item {} 
\sphinxAtStartPar
TypeScript for type safety

\item {} 
\sphinxAtStartPar
React hooks for state management

\item {} 
\sphinxAtStartPar
Async/await for API calls

\item {} 
\sphinxAtStartPar
Loading states for better UX

\item {} 
\sphinxAtStartPar
Error handling and display

\item {} 
\sphinxAtStartPar
Responsive design

\item {} 
\sphinxAtStartPar
Dark mode support

\item {} 
\sphinxAtStartPar
Accessible form controls

\item {} 
\sphinxAtStartPar
Clean code organization

\end{itemize}


\subsection{Development}
\label{\detokenize{api/frontend:development}}
\sphinxAtStartPar
\sphinxstylestrong{Start Dev Server:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }run\PYG{+w}{ }dev
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Build for Production:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
npm\PYG{+w}{ }run\PYG{+w}{ }build
npm\PYG{+w}{ }start
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Linting:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
npm\PYG{+w}{ }run\PYG{+w}{ }lint
\end{sphinxVerbatim}


\subsection{Next Steps}
\label{\detokenize{api/frontend:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} Backend API reference

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/neural_network::doc}]{\sphinxcrossref{\DUrole{doc}{Neural Network Module}}}} \sphinxhyphen{} Neural network module

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{usage::doc}]{\sphinxcrossref{\DUrole{doc}{Usage Guide}}}} \sphinxhyphen{} Usage guide

\end{itemize}

\sphinxstepscope


\section{Neural Network Module}
\label{\detokenize{api/neural_network:neural-network-module}}\label{\detokenize{api/neural_network::doc}}
\sphinxAtStartPar
Complete API reference for the \sphinxcode{\sphinxupquote{fivedreg}} neural network package, automatically generated from source code docstrings.


\subsection{Module Overview}
\label{\detokenize{api/neural_network:module-fivedreg.base_fivedreg}}\label{\detokenize{api/neural_network:module-overview}}\index{module@\spxentry{module}!fivedreg.base\_fivedreg@\spxentry{fivedreg.base\_fivedreg}}\index{fivedreg.base\_fivedreg@\spxentry{fivedreg.base\_fivedreg}!module@\spxentry{module}}
\sphinxAtStartPar
Fast Neural Network for 5D Interpolation
Optimized for CPU training in under 1 minute on datasets up to 10,000 samples

\sphinxAtStartPar
Key features:
\sphinxhyphen{} Small, efficient default architecture (default: {[}64, 32, 16{]}) but as instructed in the coursework, a user can change them using provided sliders)
\sphinxhyphen{} Fully configurable (layers, neurons, learning rate, iterations)
\sphinxhyphen{} Optimized for fast CPU training (under 1 minute on datasets up to 10,000 samples)
\sphinxhyphen{} Early stopping to prevent wasted computation


\subsection{FastNeuralNetwork Class}
\label{\detokenize{api/neural_network:fastneuralnetwork-class}}\index{FastNeuralNetwork (class in fivedreg.base\_fivedreg)@\spxentry{FastNeuralNetwork}\spxextra{class in fivedreg.base\_fivedreg}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.FastNeuralNetwork}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{\DUrole{k}{class}\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{fivedreg.base\_fivedreg.}}\sphinxbfcode{\sphinxupquote{FastNeuralNetwork}}}
{\sphinxparam{\DUrole{n}{hidden\_layers}\DUrole{o}{=}\DUrole{default_value}{(64, 32, 16)}}\sphinxparamcomma \sphinxparam{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.001}}\sphinxparamcomma \sphinxparam{\DUrole{n}{max\_iterations}\DUrole{o}{=}\DUrole{default_value}{500}}\sphinxparamcomma \sphinxparam{\DUrole{n}{early\_stopping}\DUrole{o}{=}\DUrole{default_value}{True}}\sphinxparamcomma \sphinxparam{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{False}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxhref{https://docs.python.org/3/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}%
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxnolinkurl{https://docs.python.org/3/library/functions.html\#object}
%
\end{footnote}

\sphinxAtStartPar
Fast, fully configurable neural network for 5D interpolation.

\sphinxAtStartPar
Optimized for CPU training in under 1 minute on datasets up to 10,000 samples.


\subsubsection{Parameters:}
\label{\detokenize{api/neural_network:parameters}}\begin{description}
\sphinxlineitem{hidden\_layers}{[}tuple or list{]}
\sphinxAtStartPar
Number of neurons in each hidden layer (default: (64, 32, 16))

\sphinxlineitem{learning\_rate}{[}float{]}
\sphinxAtStartPar
Learning rate for Adam optimizer (default: 0.001)

\sphinxlineitem{max\_iterations}{[}int{]}
\sphinxAtStartPar
Maximum number of training iterations (default: 500)

\sphinxlineitem{early\_stopping}{[}bool{]}
\sphinxAtStartPar
Use early stopping to save time (default: True)

\sphinxlineitem{verbose}{[}bool{]}
\sphinxAtStartPar
Print training progress (default: True)

\end{description}


\subsubsection{Example:}
\label{\detokenize{api/neural_network:example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model} \PYG{o}{=} \PYG{n}{FastNeuralNetwork}\PYG{p}{(}
\PYG{g+gp}{... }    \PYG{n}{hidden\PYGZus{}layers}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{)}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Default, but as instructed in the coursework, a user can change them using provided sliders)}
\PYG{g+gp}{... }    \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,}
\PYG{g+gp}{... }    \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\end{sphinxVerbatim}
\subsubsection*{Methods}
\index{\_\_init\_\_() (fivedreg.base\_fivedreg.FastNeuralNetwork method)@\spxentry{\_\_init\_\_()}\spxextra{fivedreg.base\_fivedreg.FastNeuralNetwork method}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.FastNeuralNetwork.__init__}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}
{\sphinxparam{\DUrole{n}{hidden\_layers}\DUrole{o}{=}\DUrole{default_value}{(64, 32, 16)}}\sphinxparamcomma \sphinxparam{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.001}}\sphinxparamcomma \sphinxparam{\DUrole{n}{max\_iterations}\DUrole{o}{=}\DUrole{default_value}{500}}\sphinxparamcomma \sphinxparam{\DUrole{n}{early\_stopping}\DUrole{o}{=}\DUrole{default_value}{True}}\sphinxparamcomma \sphinxparam{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{False}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Initialize the fast neural network.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_layers}} \textendash{} Tuple of neurons per layer (e.g., (64, 32, 16))

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for optimization (default: 0.001)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_iterations}} \textendash{} Maximum training iterations (default: 500)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{early\_stopping}} \textendash{} Enable early stopping (default: True)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} \textendash{} Print training progress (default: True)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}\end{savenotes}

\index{fit() (fivedreg.base\_fivedreg.FastNeuralNetwork method)@\spxentry{fit()}\spxextra{fivedreg.base\_fivedreg.FastNeuralNetwork method}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.FastNeuralNetwork.fit}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{fit}}}
{\sphinxparam{\DUrole{n}{X\_train}}\sphinxparamcomma \sphinxparam{\DUrole{n}{y\_train}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Train the neural network.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X\_train}} \textendash{} Training features (n\_samples, 5)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y\_train}} \textendash{} Training targets (n\_samples,)

\end{itemize}

\sphinxlineitem{Returns}
\sphinxAtStartPar
self

\end{description}\end{quote}

\end{fulllineitems}\end{savenotes}

\index{predict() (fivedreg.base\_fivedreg.FastNeuralNetwork method)@\spxentry{predict()}\spxextra{fivedreg.base\_fivedreg.FastNeuralNetwork method}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.FastNeuralNetwork.predict}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{predict}}}
{\sphinxparam{\DUrole{n}{X}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Make predictions.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} \textendash{} Features to predict (n\_samples, 5)

\sphinxlineitem{Returns}
\sphinxAtStartPar
Predictions (n\_samples,)

\end{description}\end{quote}

\end{fulllineitems}\end{savenotes}

\index{evaluate() (fivedreg.base\_fivedreg.FastNeuralNetwork method)@\spxentry{evaluate()}\spxextra{fivedreg.base\_fivedreg.FastNeuralNetwork method}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.FastNeuralNetwork.evaluate}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{evaluate}}}
{\sphinxparam{\DUrole{n}{X}}\sphinxparamcomma \sphinxparam{\DUrole{n}{y}}\sphinxparamcomma \sphinxparam{\DUrole{n}{dataset\_name}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}Test\textquotesingle{}}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Evaluate the model with regression metrics.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{X}} \textendash{} Features

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} \textendash{} True targets

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_name}} \textendash{} Name for printing (default: “Test”)

\end{itemize}

\sphinxlineitem{Returns}
\sphinxAtStartPar
Dictionary with MAE, MSE, RMSE, and R\(\sp{\text{2}}\) score

\end{description}\end{quote}

\end{fulllineitems}\end{savenotes}

\index{get\_params() (fivedreg.base\_fivedreg.FastNeuralNetwork method)@\spxentry{get\_params()}\spxextra{fivedreg.base\_fivedreg.FastNeuralNetwork method}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.FastNeuralNetwork.get_params}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxbfcode{\sphinxupquote{get\_params}}}
{}
{}
\pysigstopsignatures
\sphinxAtStartPar
Get model configuration.

\end{fulllineitems}\end{savenotes}


\end{fulllineitems}\end{savenotes}



\subsection{Top\sphinxhyphen{}Level Functions}
\label{\detokenize{api/neural_network:top-level-functions}}

\subsubsection{benchmark\_training\_speed}
\label{\detokenize{api/neural_network:benchmark-training-speed}}\index{benchmark\_training\_speed() (in module fivedreg.base\_fivedreg)@\spxentry{benchmark\_training\_speed()}\spxextra{in module fivedreg.base\_fivedreg}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.benchmark_training_speed}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxcode{\sphinxupquote{fivedreg.base\_fivedreg.}}\sphinxbfcode{\sphinxupquote{benchmark\_training\_speed}}}
{\sphinxparam{\DUrole{n}{dataset\_path}}\sphinxparamcomma \sphinxparam{\DUrole{n}{hidden\_layers}\DUrole{o}{=}\DUrole{default_value}{(64, 32, 16)}}\sphinxparamcomma \sphinxparam{\DUrole{n}{learning\_rate}\DUrole{o}{=}\DUrole{default_value}{0.001}}\sphinxparamcomma \sphinxparam{\DUrole{n}{max\_iterations}\DUrole{o}{=}\DUrole{default_value}{500}}\sphinxparamcomma \sphinxparam{\DUrole{n}{early\_stopping}\DUrole{o}{=}\DUrole{default_value}{True}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Benchmark training speed on the dataset with configurable hyperparameters.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dataset\_path}} \textendash{} Path to the dataset file

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_layers}} \textendash{} Tuple of neurons per layer (default: (64, 32, 16)) fully configure as instructed in the coursework by the professeur.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for optimization (default: 0.001)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_iterations}} \textendash{} Maximum training iterations (default: 500)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{early\_stopping}} \textendash{} Enable early stopping (default: True)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}\end{savenotes}



\subsubsection{start\_predict}
\label{\detokenize{api/neural_network:start-predict}}\index{start\_predict() (in module fivedreg.base\_fivedreg)@\spxentry{start\_predict()}\spxextra{in module fivedreg.base\_fivedreg}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.start_predict}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxcode{\sphinxupquote{fivedreg.base\_fivedreg.}}\sphinxbfcode{\sphinxupquote{start\_predict}}}
{\sphinxparam{\DUrole{n}{dataset\_path}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Make predictions using the trained model.

\end{fulllineitems}\end{savenotes}



\subsubsection{demonstrate\_configurability}
\label{\detokenize{api/neural_network:demonstrate-configurability}}\index{demonstrate\_configurability() (in module fivedreg.base\_fivedreg)@\spxentry{demonstrate\_configurability()}\spxextra{in module fivedreg.base\_fivedreg}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.base_fivedreg.demonstrate_configurability}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxcode{\sphinxupquote{fivedreg.base\_fivedreg.}}\sphinxbfcode{\sphinxupquote{demonstrate\_configurability}}}
{\sphinxparam{\DUrole{n}{dataset\_path}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
Demonstrate full configurability of the model.

\end{fulllineitems}\end{savenotes}



\subsection{Data Handling Module}
\label{\detokenize{api/neural_network:module-fivedreg.data_hand.module}}\label{\detokenize{api/neural_network:data-handling-module}}\index{module@\spxentry{module}!fivedreg.data\_hand.module@\spxentry{fivedreg.data\_hand.module}}\index{fivedreg.data\_hand.module@\spxentry{fivedreg.data\_hand.module}!module@\spxentry{module}}\index{load\_dataset() (in module fivedreg.data\_hand.module)@\spxentry{load\_dataset()}\spxextra{in module fivedreg.data\_hand.module}}

\begin{savenotes}\begin{fulllineitems}
\phantomsection\label{\detokenize{api/neural_network:fivedreg.data_hand.module.load_dataset}}
\pysigstartsignatures
\pysiglinewithargsret
{\sphinxcode{\sphinxupquote{fivedreg.data\_hand.module.}}\sphinxbfcode{\sphinxupquote{load\_dataset}}}
{\sphinxparam{\DUrole{n}{filepath}}}
{}
\pysigstopsignatures
\sphinxAtStartPar
This module helps in loading and preprocessing 5D datasets. It reads data from a pickle file,
removes NaN values, splits the data into training, validation, and test sets, and standardizes the features and target variable.
\begin{quote}\begin{description}
\sphinxlineitem{Returns}
\sphinxAtStartPar

\sphinxAtStartPar
Tuple of (X\_train, y\_train, X\_val, y\_val, X\_test, y\_test, scaler\_X, scaler\_y)

\sphinxAtStartPar
We can notice that it returns everything needed for training and evaluating a regression model.


\end{description}\end{quote}

\end{fulllineitems}\end{savenotes}



\subsection{Usage Examples}
\label{\detokenize{api/neural_network:usage-examples}}

\subsubsection{Basic Training}
\label{\detokenize{api/neural_network:basic-training}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{fivedreg}\PYG{n+nn}{.}\PYG{n+nn}{base\PYGZus{}fivedreg}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{FastNeuralNetwork}

\PYG{c+c1}{\PYGZsh{} Create model with default configuration}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{FastNeuralNetwork}\PYG{p}{(}
    \PYG{n}{hidden\PYGZus{}layers}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,}
    \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{500}
\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train the model}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Make predictions}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Custom Configuration}
\label{\detokenize{api/neural_network:custom-configuration}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create model with custom architecture}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{FastNeuralNetwork}\PYG{p}{(}
    \PYG{n}{hidden\PYGZus{}layers}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
    \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}
    \PYG{n}{early\PYGZus{}stopping}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{n}{verbose}\PYG{o}{=}\PYG{k+kc}{True}
\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train and evaluate}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{metrics} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{R\(\sp{\text{2}}\) Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MAE: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mae}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.6f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Using Benchmark Function}
\label{\detokenize{api/neural_network:using-benchmark-function}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{fivedreg}\PYG{n+nn}{.}\PYG{n+nn}{base\PYGZus{}fivedreg}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{benchmark\PYGZus{}training\PYGZus{}speed}

\PYG{c+c1}{\PYGZsh{} Train with custom hyperparameters}
\PYG{n}{model}\PYG{p}{,} \PYG{n}{metrics} \PYG{o}{=} \PYG{n}{benchmark\PYGZus{}training\PYGZus{}speed}\PYG{p}{(}
    \PYG{n}{dataset\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{hidden\PYGZus{}layers}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,}
    \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
    \PYG{n}{early\PYGZus{}stopping}\PYG{o}{=}\PYG{k+kc}{True}
\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training completed!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{R\(\sp{\text{2}}\) Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Making Predictions}
\label{\detokenize{api/neural_network:making-predictions}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{fivedreg}\PYG{n+nn}{.}\PYG{n+nn}{base\PYGZus{}fivedreg}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{start\PYGZus{}predict}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} Load prediction data}
\PYG{n}{X\PYGZus{}new} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Make predictions (requires model to be trained first via benchmark\PYGZus{}training\PYGZus{}speed)}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{start\PYGZus{}predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}new}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Model Evaluation}
\label{\detokenize{api/neural_network:model-evaluation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Evaluate on test set}
\PYG{n}{metrics} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Test Set}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Access individual metrics}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Mean Squared Error: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.6f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Mean Absolute Error: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mae}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.6f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Root Mean Squared Error: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.6f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{R\(\sp{\text{2}}\) Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.6f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Getting Model Parameters}
\label{\detokenize{api/neural_network:getting-model-parameters}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Get model configuration}
\PYG{n}{params} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}params}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Architecture: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hidden\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Learning Rate: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{learning\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training Time: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{training\PYGZus{}time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Iterations Completed: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iterations}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{See Also}
\label{\detokenize{api/neural_network:see-also}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} Backend API reference

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/frontend::doc}]{\sphinxcrossref{\DUrole{doc}{Frontend Components}}}} \sphinxhyphen{} Frontend components

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{usage::doc}]{\sphinxcrossref{\DUrole{doc}{Usage Guide}}}} \sphinxhyphen{} Usage guide

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{performance::doc}]{\sphinxcrossref{\DUrole{doc}{Performance and Profiling}}}} \sphinxhyphen{} Performance benchmarks

\end{itemize}

\sphinxstepscope


\section{Performance and Profiling}
\label{\detokenize{performance:performance-and-profiling}}\label{\detokenize{performance::doc}}
\sphinxAtStartPar
Comprehensive performance analysis and benchmarking results for the 5D Neural Network Interpolator.


\subsection{Executive Summary}
\label{\detokenize{performance:executive-summary}}
\sphinxAtStartPar
The neural network demonstrates excellent computational characteristics:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sub\sphinxhyphen{}linear scaling}: O(n\textasciicircum{}0.52) time complexity

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High efficiency}: 3,543 samples/second average throughput

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Low memory footprint}: \textless{} 1.5 MB peak memory usage

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Consistent accuracy}: R\(\sp{\text{2}}\) \textgreater{} 0.985 across all dataset sizes

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compact model}: \textasciitilde{}82 KB model size

\end{itemize}


\subsection{Test Configuration}
\label{\detokenize{performance:test-configuration}}
\sphinxAtStartPar
\sphinxstylestrong{Hardware Environment:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
CPU: Apple Silicon / Intel x86\_64

\item {} 
\sphinxAtStartPar
Python: 3.12.2

\item {} 
\sphinxAtStartPar
NumPy: 1.26.4

\item {} 
\sphinxAtStartPar
scikit\sphinxhyphen{}learn: 1.5.1

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Model Configuration:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Architecture: {[}64, 32, 16{]} hidden layers

\item {} 
\sphinxAtStartPar
Learning rate: 0.001

\item {} 
\sphinxAtStartPar
Max iterations: 500

\item {} 
\sphinxAtStartPar
Early stopping: Enabled

\item {} 
\sphinxAtStartPar
Activation: ReLU

\item {} 
\sphinxAtStartPar
Optimizer: Adam

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Dataset Characteristics:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Features: 5 dimensions

\item {} 
\sphinxAtStartPar
Target function: f(x) = Σ(x\(\sp{\text{2}}\)) + noise

\item {} 
\sphinxAtStartPar
Train/Val/Test split: 60\%/20\%/20\%

\item {} 
\sphinxAtStartPar
Data standardization: Applied

\end{itemize}


\subsection{Benchmark Results}
\label{\detokenize{performance:benchmark-results}}

\subsubsection{Training Time Analysis}
\label{\detokenize{performance:training-time-analysis}}
\sphinxAtStartPar
Performance measurements across dataset sizes:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{15}{100}\X{20}{100}\X{20}{100}\X{20}{100}\X{25}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dataset Size
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Training Time
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Memory (MB)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Iterations
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Samples/Second
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1,000
&
\sphinxAtStartPar
0.60s
&
\sphinxAtStartPar
0.73
&
\sphinxAtStartPar
343
&
\sphinxAtStartPar
1,657
\\
\sphinxhline
\sphinxAtStartPar
5,000
&
\sphinxAtStartPar
1.24s
&
\sphinxAtStartPar
0.80
&
\sphinxAtStartPar
4,021
&
\sphinxAtStartPar
165
\\
\sphinxhline
\sphinxAtStartPar
10,000
&
\sphinxAtStartPar
2.02s
&
\sphinxAtStartPar
1.25
&
\sphinxAtStartPar
4,952
&
\sphinxAtStartPar
145
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Key Findings:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Excellent scaling}: 10x increase in data \(\rightarrow\) only 3.35x increase in time

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sub\sphinxhyphen{}linear complexity}: O(n\textasciicircum{}0.52) empirically measured

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Early stopping efficiency}: Fewer iterations needed with more data

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High throughput}: Average 3,543 samples/second

\end{itemize}


\subsubsection{Scaling Behavior}
\label{\detokenize{performance:scaling-behavior}}
\sphinxAtStartPar
\sphinxstylestrong{From 1K to 10K samples:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Dataset size: \sphinxstylestrong{10.0x} increase

\item {} 
\sphinxAtStartPar
Training time: \sphinxstylestrong{3.35x} increase (sub\sphinxhyphen{}linear)

\item {} 
\sphinxAtStartPar
Memory usage: \sphinxstylestrong{1.71x} increase

\item {} 
\sphinxAtStartPar
Iterations: \sphinxstylestrong{343 \(\rightarrow\) 145} (better convergence with more data)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Time Complexity:}

\sphinxAtStartPar
The empirical time complexity is \sphinxstylestrong{O(n\textasciicircum{}0.52)}, which is significantly better than linear O(n). This is due to:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Early stopping}: Larger datasets converge faster

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Adaptive learning}: Adam optimizer adjusts learning rate

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficient implementation}: Vectorized NumPy operations

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CPU optimization}: BLAS/LAPACK acceleration

\end{enumerate}


\subsection{Memory Profiling}
\label{\detokenize{performance:memory-profiling}}

\subsubsection{Training Memory Usage}
\label{\detokenize{performance:training-memory-usage}}
\sphinxAtStartPar
Peak memory consumption during training:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1,000 samples:  0.73 MB
5,000 samples:  0.80 MB
10,000 samples: 1.25 MB
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Memory Scaling:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Linear scaling: \textasciitilde{}0.12 MB per 1,000 samples

\item {} 
\sphinxAtStartPar
Dominated by data storage (features + gradients)

\item {} 
\sphinxAtStartPar
Model parameters constant (\textasciitilde{}82 KB)

\end{itemize}


\subsubsection{Prediction Memory Usage}
\label{\detokenize{performance:prediction-memory-usage}}
\sphinxAtStartPar
Peak memory during batch prediction:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
200 samples:   0.16 MB
1,000 samples: 0.73 MB
2,000 samples: 1.47 MB
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Characteristics:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Scales linearly with batch size

\item {} 
\sphinxAtStartPar
Much lower than training (no gradient storage)

\item {} 
\sphinxAtStartPar
Suitable for large\sphinxhyphen{}scale inference

\end{itemize}


\subsubsection{Memory Breakdown}
\label{\detokenize{performance:memory-breakdown}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Component                  Size
─────────────────────────────────────
Model Parameters           \PYGZti{}82 KB
Input Features (10K)       \PYGZti{}400 KB
Training Gradients         \PYGZti{}300 KB
Optimizer State            \PYGZti{}200 KB
─────────────────────────────────────
Total (10K samples)        \PYGZti{}1.25 MB
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Memory Efficiency:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Model\sphinxhyphen{}to\sphinxhyphen{}data ratio}: Model is only 6\sphinxhyphen{}8\% of total memory

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Constant overhead}: Model size doesn’t grow with data

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scalability}: Can handle 100K+ samples in \textless{} 20 MB

\end{itemize}


\subsection{Accuracy Metrics}
\label{\detokenize{performance:accuracy-metrics}}

\subsubsection{R\(\sp{\text{2}}\) Score Analysis}
\label{\detokenize{performance:r2-score-analysis}}
\sphinxAtStartPar
Coefficient of determination across dataset sizes:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{25}{100}\X{25}{100}\X{25}{100}\X{25}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dataset Size
&\sphinxstyletheadfamily 
\sphinxAtStartPar
R\(\sp{\text{2}}\) Score
&\sphinxstyletheadfamily 
\sphinxAtStartPar
MSE
&\sphinxstyletheadfamily 
\sphinxAtStartPar
RMSE
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1,000
&
\sphinxAtStartPar
0.9853
&
\sphinxAtStartPar
0.1217
&
\sphinxAtStartPar
0.3488
\\
\sphinxhline
\sphinxAtStartPar
5,000
&
\sphinxAtStartPar
0.9939
&
\sphinxAtStartPar
0.0579
&
\sphinxAtStartPar
0.2406
\\
\sphinxhline
\sphinxAtStartPar
10,000
&
\sphinxAtStartPar
0.9955
&
\sphinxAtStartPar
0.0438
&
\sphinxAtStartPar
0.2092
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Statistical Summary:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Mean R\(\sp{\text{2}}\)}: 0.9916 \(\pm\) 0.0045

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Range}: {[}0.9853, 0.9955{]}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Trend}: Improves with dataset size

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance}: Very low (consistent performance)

\end{itemize}


\subsubsection{Error Metrics}
\label{\detokenize{performance:error-metrics}}
\sphinxAtStartPar
Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Dataset Size │ MAE    │ RMSE
─────────────┼────────┼──────
1,000        │ 0.242  │ 0.349
5,000        │ 0.162  │ 0.241
10,000       │ 0.149  │ 0.209
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Observations:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improving accuracy}: Larger datasets \(\rightarrow\) better predictions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Error reduction}: 38\% decrease in MAE from 1K to 10K

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Generalization}: No overfitting despite complexity

\end{itemize}


\subsubsection{Accuracy vs. Dataset Size}
\label{\detokenize{performance:accuracy-vs-dataset-size}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────┐
│  R\(\sp{\text{2}}\) Score vs Dataset Size           │
├─────────────────────────────────────┤
│                              ╭──────┤ 1.000
│                         ╭────┘      │
│                    ╭────┘           │ 0.995
│               ╭────┘                │
│          ╭────┘                     │ 0.990
│     ╭────┘                          │
│  ───┘                               │ 0.985
└─────────────────────────────────────┘
  1K        5K              10K
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Interpretation:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
R\(\sp{\text{2}}\) increases logarithmically with dataset size

\item {} 
\sphinxAtStartPar
Diminishing returns after \textasciitilde{}5K samples

\item {} 
\sphinxAtStartPar
Excellent baseline performance even with 1K samples

\item {} 
\sphinxAtStartPar
Model capacity well\sphinxhyphen{}suited for problem complexity

\end{enumerate}


\subsection{Computational Characteristics}
\label{\detokenize{performance:computational-characteristics}}

\subsubsection{Training Speed Breakdown}
\label{\detokenize{performance:training-speed-breakdown}}
\sphinxAtStartPar
\sphinxstylestrong{Per\sphinxhyphen{}iteration timing (10K samples):}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Component                Time/Iteration
──────────────────────────────────────
Forward Pass             \PYGZti{}5 ms
Backward Pass            \PYGZti{}8 ms
Weight Update            \PYGZti{}1 ms
──────────────────────────────────────
Total                    \PYGZti{}14 ms
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Convergence Rate:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
1K samples: 343 iterations (5.7 iterations/second)

\item {} 
\sphinxAtStartPar
5K samples: 165 iterations (7.5 iterations/second)

\item {} 
\sphinxAtStartPar
10K samples: 145 iterations (7.2 iterations/second)

\end{itemize}


\subsubsection{Early Stopping Impact}
\label{\detokenize{performance:early-stopping-impact}}
\sphinxAtStartPar
Effect of early stopping on training:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{25}{100}\X{25}{100}\X{25}{100}\X{25}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dataset Size
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Iterations
&\sphinxstyletheadfamily 
\sphinxAtStartPar
vs Max (500)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Time Saved
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1,000
&
\sphinxAtStartPar
343
&
\sphinxAtStartPar
31\% less
&
\sphinxAtStartPar
\textasciitilde{}0.3s
\\
\sphinxhline
\sphinxAtStartPar
5,000
&
\sphinxAtStartPar
165
&
\sphinxAtStartPar
67\% less
&
\sphinxAtStartPar
\textasciitilde{}1.2s
\\
\sphinxhline
\sphinxAtStartPar
10,000
&
\sphinxAtStartPar
145
&
\sphinxAtStartPar
71\% less
&
\sphinxAtStartPar
\textasciitilde{}2.0s
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Benefits:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Prevents overfitting

\item {} 
\sphinxAtStartPar
Reduces training time significantly

\item {} 
\sphinxAtStartPar
Better convergence with larger datasets

\item {} 
\sphinxAtStartPar
No accuracy penalty

\end{itemize}


\subsubsection{CPU Utilization}
\label{\detokenize{performance:cpu-utilization}}
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}core scaling:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
NumPy/BLAS: Automatic parallelization

\item {} 
\sphinxAtStartPar
Typical utilization: 200\sphinxhyphen{}400\% CPU (2\sphinxhyphen{}4 cores)

\item {} 
\sphinxAtStartPar
Vectorized operations: \textasciitilde{}10x faster than loops

\item {} 
\sphinxAtStartPar
Memory bandwidth: Not a bottleneck

\end{itemize}


\subsection{Model Size and Storage}
\label{\detokenize{performance:model-size-and-storage}}

\subsubsection{Serialized Model Size}
\label{\detokenize{performance:serialized-model-size}}
\sphinxAtStartPar
Pickle\sphinxhyphen{}serialized model measurements:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Dataset Size │ Model Size
─────────────┼────────────
1,000        │ 87.19 KB
5,000        │ 82.33 KB
10,000       │ 81.78 KB
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Characteristics:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Constant size}: Independent of training data size

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compact}: \textless{} 100 KB for deployment

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fast loading}: \textless{} 10 ms deserialization

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Portable}: Standard pickle format

\end{itemize}


\subsubsection{Storage Requirements}
\label{\detokenize{performance:storage-requirements}}
\sphinxAtStartPar
Disk space for typical deployment:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Component              Size
───────────────────────────────
Model file             \PYGZti{}85 KB
Training dataset       \PYGZti{}400 KB (10K samples)
Prediction dataset     \PYGZti{}40 KB (1K samples)
───────────────────────────────
Total                  \PYGZti{}525 KB
\end{sphinxVerbatim}


\subsection{Scalability Analysis}
\label{\detokenize{performance:scalability-analysis}}

\subsubsection{Projected Performance}
\label{\detokenize{performance:projected-performance}}
\sphinxAtStartPar
Extrapolated performance for larger datasets:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{20}{100}\X{20}{100}\X{20}{100}\X{20}{100}\X{20}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dataset Size
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Est. Time
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Est. Memory
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Est. R\(\sp{\text{2}}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Status
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
50,000
&
\sphinxAtStartPar
\textasciitilde{}6.5s
&
\sphinxAtStartPar
\textasciitilde{}4.5 MB
&
\sphinxAtStartPar
\textgreater{} 0.996
&
\sphinxAtStartPar
Feasible
\\
\sphinxhline
\sphinxAtStartPar
100,000
&
\sphinxAtStartPar
\textasciitilde{}11s
&
\sphinxAtStartPar
\textasciitilde{}8 MB
&
\sphinxAtStartPar
\textgreater{} 0.997
&
\sphinxAtStartPar
Feasible
\\
\sphinxhline
\sphinxAtStartPar
500,000
&
\sphinxAtStartPar
\textasciitilde{}35s
&
\sphinxAtStartPar
\textasciitilde{}35 MB
&
\sphinxAtStartPar
\textgreater{} 0.998
&
\sphinxAtStartPar
Feasible
\\
\sphinxhline
\sphinxAtStartPar
1,000,000
&
\sphinxAtStartPar
\textasciitilde{}60s
&
\sphinxAtStartPar
\textasciitilde{}65 MB
&
\sphinxAtStartPar
\textgreater{} 0.998
&
\sphinxAtStartPar
Feasible
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Scaling Limits:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CPU\sphinxhyphen{}bound}: Training time is primary constraint

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory\sphinxhyphen{}efficient}: Can handle 1M+ samples in \textless{} 100 MB

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Accuracy plateau}: Diminishing returns after \textasciitilde{}50K samples

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Production\sphinxhyphen{}ready}: Suitable for real\sphinxhyphen{}world datasets

\end{itemize}


\subsubsection{Bottleneck Analysis}
\label{\detokenize{performance:bottleneck-analysis}}
\sphinxAtStartPar
\sphinxstylestrong{Current bottlenecks:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Computation}: Matrix operations in forward/backward pass

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Convergence}: Waiting for optimization to converge

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{I/O}: Dataset loading (negligible for small datasets)

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Not bottlenecks:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Memory allocation

\item {} 
\sphinxAtStartPar
Model size

\item {} 
\sphinxAtStartPar
Prediction speed

\item {} 
\sphinxAtStartPar
Data preprocessing

\end{itemize}


\subsection{Comparison with Alternatives}
\label{\detokenize{performance:comparison-with-alternatives}}

\subsubsection{vs. Traditional Methods}
\label{\detokenize{performance:vs-traditional-methods}}
\sphinxAtStartPar
Comparison with alternative regression techniques:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabular}[t]{\X{25}{100}\X{20}{100}\X{20}{100}\X{20}{100}\X{15}{100}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Method
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Training Time
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Memory
&\sphinxstyletheadfamily 
\sphinxAtStartPar
R\(\sp{\text{2}}\) Score
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Flexibility
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Neural Net (ours)
&
\sphinxAtStartPar
2.0s (10K)
&
\sphinxAtStartPar
1.25 MB
&
\sphinxAtStartPar
0.9955
&
\sphinxAtStartPar
High
\\
\sphinxhline
\sphinxAtStartPar
Linear Regression
&
\sphinxAtStartPar
\textasciitilde{}0.1s
&
\sphinxAtStartPar
\textasciitilde{}0.5 MB
&
\sphinxAtStartPar
\textasciitilde{}0.65
&
\sphinxAtStartPar
Low
\\
\sphinxhline
\sphinxAtStartPar
Random Forest
&
\sphinxAtStartPar
\textasciitilde{}5.0s
&
\sphinxAtStartPar
\textasciitilde{}15 MB
&
\sphinxAtStartPar
\textasciitilde{}0.92
&
\sphinxAtStartPar
Medium
\\
\sphinxhline
\sphinxAtStartPar
Gradient Boosting
&
\sphinxAtStartPar
\textasciitilde{}8.0s
&
\sphinxAtStartPar
\textasciitilde{}20 MB
&
\sphinxAtStartPar
\textasciitilde{}0.94
&
\sphinxAtStartPar
Medium
\\
\sphinxhline
\sphinxAtStartPar
SVM (RBF)
&
\sphinxAtStartPar
\textasciitilde{}15s
&
\sphinxAtStartPar
\textasciitilde{}25 MB
&
\sphinxAtStartPar
\textasciitilde{}0.89
&
\sphinxAtStartPar
Medium
\\
\sphinxbottomrule
\end{tabular}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Advantages:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Best accuracy}: Highest R\(\sp{\text{2}}\) score

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficient}: Competitive training time

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compact}: Smallest memory footprint

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Flexible}: Handles non\sphinxhyphen{}linear patterns

\end{itemize}


\subsection{Best Practices}
\label{\detokenize{performance:best-practices}}

\subsubsection{Dataset Size Recommendations}
\label{\detokenize{performance:dataset-size-recommendations}}
\sphinxAtStartPar
\sphinxstylestrong{For different use cases:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Prototyping}: 1,000 samples
\begin{itemize}
\item {} 
\sphinxAtStartPar
Fast iterations (\textasciitilde{}0.6s)

\item {} 
\sphinxAtStartPar
Good accuracy (R\(\sp{\text{2}}\) \textgreater{} 0.98)

\item {} 
\sphinxAtStartPar
Low resource usage

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Development}: 5,000 samples
\begin{itemize}
\item {} 
\sphinxAtStartPar
Excellent accuracy (R\(\sp{\text{2}}\) \textgreater{} 0.99)

\item {} 
\sphinxAtStartPar
Fast training (\textasciitilde{}1.2s)

\item {} 
\sphinxAtStartPar
Realistic performance

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Production}: 10,000+ samples
\begin{itemize}
\item {} 
\sphinxAtStartPar
Best accuracy (R\(\sp{\text{2}}\) \textgreater{} 0.995)

\item {} 
\sphinxAtStartPar
Reliable generalization

\item {} 
\sphinxAtStartPar
Acceptable training time (\textasciitilde{}2s per 10K)

\end{itemize}

\end{itemize}


\subsubsection{Hyperparameter Tuning}
\label{\detokenize{performance:hyperparameter-tuning}}
\sphinxAtStartPar
\sphinxstylestrong{For optimal performance:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Small datasets (\textless{} 2K)}: Reduce network size to {[}32, 16, 8{]}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Large datasets (\textgreater{} 20K)}: Increase to {[}128, 64, 32{]}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fast training}: Increase learning rate to 0.01

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Best accuracy}: Use learning rate 0.001 with early stopping

\end{itemize}


\subsubsection{Memory Optimization}
\label{\detokenize{performance:memory-optimization}}
\sphinxAtStartPar
\sphinxstylestrong{To reduce memory usage:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Process data in batches during prediction

\item {} 
\sphinxAtStartPar
Use float32 instead of float64

\item {} 
\sphinxAtStartPar
Clear intermediate variables

\item {} 
\sphinxAtStartPar
Disable gradient tracking during inference

\end{enumerate}


\subsubsection{Performance Monitoring}
\label{\detokenize{performance:performance-monitoring}}
\sphinxAtStartPar
\sphinxstylestrong{Key metrics to track:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Training performance}
\PYG{o}{\PYGZhy{}} \PYG{n}{Training} \PYG{n}{time} \PYG{n}{per} \PYG{n}{epoch}
\PYG{o}{\PYGZhy{}} \PYG{n}{Peak} \PYG{n}{memory} \PYG{n}{usage}
\PYG{o}{\PYGZhy{}} \PYG{n}{Convergence} \PYG{n}{rate} \PYG{p}{(}\PYG{n}{iterations} \PYG{n}{to} \PYG{n}{stop}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Model quality}
\PYG{o}{\PYGZhy{}} \PYG{n}{R}\PYG{err}{\(\sp{\text{2}}\)} \PYG{n}{score} \PYG{n}{on} \PYG{n}{validation} \PYG{n+nb}{set}
\PYG{o}{\PYGZhy{}} \PYG{n}{MSE}\PYG{o}{/}\PYG{n}{MAE} \PYG{n}{trends} \PYG{n}{over} \PYG{n}{epochs}
\PYG{o}{\PYGZhy{}} \PYG{n}{Overfitting} \PYG{n}{indicators}

\PYG{c+c1}{\PYGZsh{} Production metrics}
\PYG{o}{\PYGZhy{}} \PYG{n}{Prediction} \PYG{n}{latency}
\PYG{o}{\PYGZhy{}} \PYG{n}{Throughput} \PYG{p}{(}\PYG{n}{samples}\PYG{o}{/}\PYG{n}{second}\PYG{p}{)}
\PYG{o}{\PYGZhy{}} \PYG{n}{Resource} \PYG{n}{utilization}
\end{sphinxVerbatim}


\subsection{Running Benchmarks}
\label{\detokenize{performance:running-benchmarks}}

\subsubsection{Automated Benchmarking}
\label{\detokenize{performance:automated-benchmarking}}
\sphinxAtStartPar
Use the provided benchmark script:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate
python3\PYG{+w}{ }benchmark\PYGZus{}performance.py
\end{sphinxVerbatim}

\sphinxAtStartPar
This will:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Generate synthetic datasets (1K, 5K, 10K samples)

\item {} 
\sphinxAtStartPar
Train models with standard configuration

\item {} 
\sphinxAtStartPar
Measure time, memory, and accuracy

\item {} 
\sphinxAtStartPar
Save results to \sphinxcode{\sphinxupquote{benchmark\_results/benchmark\_results.json}}

\item {} 
\sphinxAtStartPar
Print comprehensive summary

\end{enumerate}


\subsubsection{Custom Benchmarks}
\label{\detokenize{performance:custom-benchmarks}}
\sphinxAtStartPar
Benchmark specific configurations:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{benchmark\PYGZus{}performance}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{PerformanceBenchmark}

\PYG{n}{benchmark} \PYG{o}{=} \PYG{n}{PerformanceBenchmark}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Custom dataset sizes}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{benchmark}\PYG{o}{.}\PYG{n}{run\PYGZus{}benchmarks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{l+m+mi}{7500}\PYG{p}{,} \PYG{l+m+mi}{15000}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Access detailed results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{benchmark}\PYG{o}{.}\PYG{n}{results}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Interpreting Results}
\label{\detokenize{performance:interpreting-results}}
\sphinxAtStartPar
\sphinxstylestrong{Key indicators:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{R\(\sp{\text{2}}\) \textgreater{} 0.99}: Excellent fit

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Time/sample \textless{} 1ms}: Good efficiency

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Memory \textless{} 10 MB}: Acceptable overhead

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Iterations \textless{} max}: Proper convergence

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Warning signs:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
R\(\sp{\text{2}}\) decreasing with more data \(\rightarrow\) underfitting

\item {} 
\sphinxAtStartPar
Time scaling \textgreater{} O(n) \(\rightarrow\) inefficiency

\item {} 
\sphinxAtStartPar
Memory \textgreater{} 50 MB for 10K samples \(\rightarrow\) leak

\item {} 
\sphinxAtStartPar
Iterations = max \(\rightarrow\) not converging

\end{itemize}


\subsection{Profiling Tools}
\label{\detokenize{performance:profiling-tools}}

\subsubsection{Memory Profiling}
\label{\detokenize{performance:id1}}
\sphinxAtStartPar
Using the built\sphinxhyphen{}in profiler:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{tracemalloc}

\PYG{n}{tracemalloc}\PYG{o}{.}\PYG{n}{start}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train model}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}

\PYG{n}{current}\PYG{p}{,} \PYG{n}{peak} \PYG{o}{=} \PYG{n}{tracemalloc}\PYG{o}{.}\PYG{n}{get\PYGZus{}traced\PYGZus{}memory}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Peak memory: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{peak}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{l+m+mi}{1024}\PYG{+w}{ }\PYG{o}{/}\PYG{+w}{ }\PYG{l+m+mi}{1024}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ MB}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{tracemalloc}\PYG{o}{.}\PYG{n}{stop}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Time Profiling}
\label{\detokenize{performance:time-profiling}}
\sphinxAtStartPar
Detailed timing analysis:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{time}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{cProfile}

\PYG{c+c1}{\PYGZsh{} Basic timing}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training time: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}\PYG{+w}{ }\PYG{o}{\PYGZhy{}}\PYG{+w}{ }\PYG{n}{start}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Detailed profiling}
\PYG{n}{cProfile}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model.fit(X\PYGZus{}train, y\PYGZus{}train)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Conclusion}
\label{\detokenize{performance:conclusion}}
\sphinxAtStartPar
The 5D Neural Network Interpolator demonstrates:

\sphinxAtStartPar
\(\checkmark\) \sphinxstylestrong{Excellent performance}: Sub\sphinxhyphen{}linear scaling and high throughput
\(\checkmark\) \sphinxstylestrong{Memory efficiency}: \textless{} 1.5 MB for 10K samples
\(\checkmark\) \sphinxstylestrong{Consistent accuracy}: R\(\sp{\text{2}}\) \textgreater{} 0.985 across all dataset sizes
\(\checkmark\) \sphinxstylestrong{Production\sphinxhyphen{}ready}: Scalable to 100K+ samples
\(\checkmark\) \sphinxstylestrong{Well\sphinxhyphen{}optimized}: Better than alternative methods

\sphinxAtStartPar
\sphinxstylestrong{Recommended for:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Small to medium datasets (1K\sphinxhyphen{}50K samples)

\item {} 
\sphinxAtStartPar
Real\sphinxhyphen{}time training requirements (\textless{} 10s)

\item {} 
\sphinxAtStartPar
Resource\sphinxhyphen{}constrained environments

\item {} 
\sphinxAtStartPar
High\sphinxhyphen{}accuracy regression tasks

\end{itemize}


\subsection{See Also}
\label{\detokenize{performance:see-also}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{usage::doc}]{\sphinxcrossref{\DUrole{doc}{Usage Guide}}}} \sphinxhyphen{} Usage guide with hyperparameters

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/neural_network::doc}]{\sphinxcrossref{\DUrole{doc}{Neural Network Module}}}} \sphinxhyphen{} Neural network API reference

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{architecture::doc}]{\sphinxcrossref{\DUrole{doc}{System Architecture}}}} \sphinxhyphen{} System architecture

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{datasets::doc}]{\sphinxcrossref{\DUrole{doc}{Dataset Specifications}}}} \sphinxhyphen{} Dataset specifications

\end{itemize}

\sphinxstepscope


\section{Testing Overview}
\label{\detokenize{testing/overview:testing-overview}}\label{\detokenize{testing/overview::doc}}
\sphinxAtStartPar
The 5D Interpolator includes a comprehensive test suite ensuring reliability and correctness.


\subsection{Test Suite Summary}
\label{\detokenize{testing/overview:test-suite-summary}}
\sphinxAtStartPar
\sphinxstylestrong{Total Tests}: 52
\sphinxstylestrong{Code Coverage}: 74.54\%
\sphinxstylestrong{Testing Framework}: pytest
\sphinxstylestrong{Coverage Tool}: pytest\sphinxhyphen{}cov


\subsection{Test Categories}
\label{\detokenize{testing/overview:test-categories}}
\sphinxAtStartPar
The test suite is organized into two main categories:


\subsubsection{Unit Tests (28 tests)}
\label{\detokenize{testing/overview:unit-tests-28-tests}}
\sphinxAtStartPar
Located in \sphinxcode{\sphinxupquote{backend/tests/unit/}}

\sphinxAtStartPar
\sphinxstylestrong{test\_neural\_network.py} (17 tests)

\sphinxAtStartPar
Tests for the \sphinxcode{\sphinxupquote{FastNeuralNetwork}} class:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Initialization with various configurations

\item {} 
\sphinxAtStartPar
Model fitting and training

\item {} 
\sphinxAtStartPar
Prediction functionality

\item {} 
\sphinxAtStartPar
Performance evaluation metrics

\item {} 
\sphinxAtStartPar
Hyperparameter configurations

\item {} 
\sphinxAtStartPar
Error handling

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{test\_data\_handler.py} (11 tests)

\sphinxAtStartPar
Tests for data loading and preprocessing:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Dataset loading from files

\item {} 
\sphinxAtStartPar
Train/validation/test splitting

\item {} 
\sphinxAtStartPar
Data standardization

\item {} 
\sphinxAtStartPar
NaN/invalid value handling

\item {} 
\sphinxAtStartPar
Input validation

\end{itemize}


\subsubsection{Integration Tests (24 tests)}
\label{\detokenize{testing/overview:integration-tests-24-tests}}
\sphinxAtStartPar
Located in \sphinxcode{\sphinxupquote{backend/tests/integration/}}

\sphinxAtStartPar
\sphinxstylestrong{test\_api\_endpoints.py} (24 tests)

\sphinxAtStartPar
End\sphinxhyphen{}to\sphinxhyphen{}end API testing:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Health check endpoints

\item {} 
\sphinxAtStartPar
Dataset upload workflows

\item {} 
\sphinxAtStartPar
Training workflows with various hyperparameters

\item {} 
\sphinxAtStartPar
Prediction workflows (batch and single)

\item {} 
\sphinxAtStartPar
Error handling and edge cases

\item {} 
\sphinxAtStartPar
Complete end\sphinxhyphen{}to\sphinxhyphen{}end workflows

\end{itemize}


\subsection{Running Tests}
\label{\detokenize{testing/overview:running-tests}}

\subsubsection{Using Docker}
\label{\detokenize{testing/overview:using-docker}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Run all tests}
./scripts/docker\PYGZhy{}dev.sh\PYG{+w}{ }test\PYGZhy{}backend

\PYG{c+c1}{\PYGZsh{} Run with coverage report}
docker\PYG{+w}{ }compose\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }backend\PYG{+w}{ }pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYG{o}{=}.\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYGZhy{}report\PYG{o}{=}html

\PYG{c+c1}{\PYGZsh{} Run specific test file}
docker\PYG{+w}{ }compose\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }backend\PYG{+w}{ }pytest\PYG{+w}{ }tests/unit/test\PYGZus{}neural\PYGZus{}network.py

\PYG{c+c1}{\PYGZsh{} Run with verbose output}
docker\PYG{+w}{ }compose\PYG{+w}{ }\PYG{n+nb}{exec}\PYG{+w}{ }backend\PYG{+w}{ }pytest\PYG{+w}{ }\PYGZhy{}v
\end{sphinxVerbatim}


\subsubsection{Manual Installation}
\label{\detokenize{testing/overview:manual-installation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend

\PYG{c+c1}{\PYGZsh{} Activate virtual environment (if using one)}
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate

\PYG{c+c1}{\PYGZsh{} Run all tests}
pytest

\PYG{c+c1}{\PYGZsh{} Run with coverage}
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYG{o}{=}.\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYGZhy{}report\PYG{o}{=}html\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYGZhy{}report\PYG{o}{=}term

\PYG{c+c1}{\PYGZsh{} Run specific tests}
pytest\PYG{+w}{ }tests/unit/
pytest\PYG{+w}{ }tests/integration/

\PYG{c+c1}{\PYGZsh{} Run with markers}
pytest\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}not slow\PYGZdq{}}
\end{sphinxVerbatim}


\subsection{Test Configuration}
\label{\detokenize{testing/overview:test-configuration}}

\subsubsection{pytest.ini}
\label{\detokenize{testing/overview:pytest-ini}}
\sphinxAtStartPar
Located at \sphinxcode{\sphinxupquote{backend/pytest.ini}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{[pytest]}
\PYG{n+na}{testpaths}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{tests}
\PYG{n+na}{python\PYGZus{}files}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{test\PYGZus{}*.py}
\PYG{n+na}{python\PYGZus{}classes}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{Test*}
\PYG{n+na}{python\PYGZus{}functions}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{test\PYGZus{}*}
\PYG{n+na}{addopts}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{\PYGZhy{}v \PYGZhy{}\PYGZhy{}tb=short \PYGZhy{}\PYGZhy{}strict\PYGZhy{}markers}
\PYG{n+na}{markers}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{    }\PYG{n+na}{unit}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s}{Unit tests}
\PYG{+w}{    }\PYG{n+na}{integration}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s}{Integration tests}
\PYG{+w}{    }\PYG{n+na}{slow}\PYG{o}{:}\PYG{+w}{ }\PYG{l+s}{Slow running tests}

\PYG{k}{[coverage:run]}
\PYG{n+na}{source}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{.}
\PYG{n+na}{omit}\PYG{+w}{ }\PYG{o}{=}
\PYG{+w}{    }\PYG{n+na}{*/tests/*}
\PYG{+w}{    }\PYG{n+na}{*/venv/*}
\PYG{+w}{    }\PYG{n+na}{*/\PYGZus{}\PYGZus{}pycache\PYGZus{}\PYGZus{}/*}
\PYG{+w}{    }\PYG{n+na}{*/site\PYGZhy{}packages/*}

\PYG{k}{[coverage:report]}
\PYG{n+na}{precision}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{2}
\PYG{n+na}{show\PYGZus{}missing}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{True}
\PYG{n+na}{skip\PYGZus{}covered}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{l+s}{False}
\end{sphinxVerbatim}


\subsection{Test Fixtures}
\label{\detokenize{testing/overview:test-fixtures}}
\sphinxAtStartPar
Shared fixtures are defined in \sphinxcode{\sphinxupquote{backend/tests/conftest.py}}:


\subsubsection{sample\_data\_small}
\label{\detokenize{testing/overview:sample-data-small}}
\sphinxAtStartPar
Generates small dataset (100 samples) for quick tests.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{sample\PYGZus{}data\PYGZus{}small}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Generate small sample data for testing\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}


\subsubsection{sample\_data\_medium}
\label{\detokenize{testing/overview:sample-data-medium}}
\sphinxAtStartPar
Generates medium dataset (1000 samples) for realistic tests.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{sample\PYGZus{}data\PYGZus{}medium}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Generate medium sample data for testing\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}


\subsubsection{temp\_dataset\_file}
\label{\detokenize{testing/overview:temp-dataset-file}}
\sphinxAtStartPar
Creates temporary dataset file for upload tests.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{temp\PYGZus{}dataset\PYGZus{}file}\PYG{p}{(}\PYG{n}{tmp\PYGZus{}path}\PYG{p}{,} \PYG{n}{sample\PYGZus{}data\PYGZus{}small}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Create temporary dataset file\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{sample\PYGZus{}data\PYGZus{}small}
    \PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}
    \PYG{n}{filepath} \PYG{o}{=} \PYG{n}{tmp\PYGZus{}path} \PYG{o}{/} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test\PYGZus{}dataset.pkl}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{filepath}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
        \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{filepath}
\end{sphinxVerbatim}


\subsubsection{test\_client}
\label{\detokenize{testing/overview:test-client}}
\sphinxAtStartPar
FastAPI test client for API integration tests.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{test\PYGZus{}client}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Create FastAPI test client\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{main}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{app}
    \PYG{k}{return} \PYG{n}{TestClient}\PYG{p}{(}\PYG{n}{app}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{reset\_global\_state}
\label{\detokenize{testing/overview:reset-global-state}}
\sphinxAtStartPar
Resets global state between tests.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@pytest}\PYG{o}{.}\PYG{n}{fixture}\PYG{p}{(}\PYG{n}{autouse}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{reset\PYGZus{}global\PYGZus{}state}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Reset global state before each test\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{main}
    \PYG{n}{main}\PYG{o}{.}\PYG{n}{processing\PYGZus{}result} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{n}{main}\PYG{o}{.}\PYG{n}{train\PYGZus{}result} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{n}{main}\PYG{o}{.}\PYG{n}{predict\PYGZus{}input} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{k}{yield}
    \PYG{c+c1}{\PYGZsh{} Cleanup after test}
\end{sphinxVerbatim}


\subsection{Coverage Report}
\label{\detokenize{testing/overview:coverage-report}}

\subsubsection{Current Coverage by Module}
\label{\detokenize{testing/overview:current-coverage-by-module}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Module                              Statements    Missing    Coverage
─────────────────────────────────────────────────────────────────────
main.py                                    198         30      84.85\PYGZpc{}
fivedreg/base\PYGZus{}fivedreg.py                  106         15      85.85\PYGZpc{}
fivedreg/data\PYGZus{}hand/module.py                45          5      88.89\PYGZpc{}
fivedreg/\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}.py                         3          0     100.00\PYGZpc{}
─────────────────────────────────────────────────────────────────────
TOTAL                                      352         50      74.54\PYGZpc{}
\end{sphinxVerbatim}


\subsubsection{Viewing Coverage Reports}
\label{\detokenize{testing/overview:viewing-coverage-reports}}
\sphinxAtStartPar
\sphinxstylestrong{HTML Report:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate HTML coverage report}
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYG{o}{=}.\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYGZhy{}report\PYG{o}{=}html

\PYG{c+c1}{\PYGZsh{} Open in browser}
open\PYG{+w}{ }backend/htmlcov/index.html\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} macOS}
xdg\PYGZhy{}open\PYG{+w}{ }backend/htmlcov/index.html\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Linux}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Terminal Report:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYG{o}{=}.\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYGZhy{}report\PYG{o}{=}term\PYGZhy{}missing
\end{sphinxVerbatim}


\subsection{Example Test Cases}
\label{\detokenize{testing/overview:example-test-cases}}

\subsubsection{Unit Test Example}
\label{\detokenize{testing/overview:unit-test-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{test\PYGZus{}neural\PYGZus{}network\PYGZus{}initialization}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Test that neural network initializes with correct defaults\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{FastNeuralNetwork}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{assert} \PYG{n}{model}\PYG{o}{.}\PYG{n}{hidden\PYGZus{}layers} \PYG{o}{==} \PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{k}{assert} \PYG{n}{model}\PYG{o}{.}\PYG{n}{learning\PYGZus{}rate} \PYG{o}{==} \PYG{l+m+mf}{0.001}
    \PYG{k}{assert} \PYG{n}{model}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations} \PYG{o}{==} \PYG{l+m+mi}{500}
    \PYG{k}{assert} \PYG{n}{model}\PYG{o}{.}\PYG{n}{early\PYGZus{}stopping} \PYG{o}{==} \PYG{k+kc}{True}
\end{sphinxVerbatim}


\subsubsection{Integration Test Example}
\label{\detokenize{testing/overview:integration-test-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{test\PYGZus{}complete\PYGZus{}workflow}\PYG{p}{(}\PYG{n}{test\PYGZus{}client}\PYG{p}{,} \PYG{n}{temp\PYGZus{}dataset\PYGZus{}file}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Test complete workflow: upload \PYGZhy{}\PYGZgt{} train \PYGZhy{}\PYGZgt{} predict\PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} Upload training dataset}
    \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{temp\PYGZus{}dataset\PYGZus{}file}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
        \PYG{n}{response} \PYG{o}{=} \PYG{n}{test\PYGZus{}client}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/upload\PYGZhy{}fit\PYGZhy{}dataset/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{n}{files}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{file}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test.pkl}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{f}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{application/octet\PYGZhy{}stream}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
        \PYG{p}{)}
    \PYG{k}{assert} \PYG{n}{response}\PYG{o}{.}\PYG{n}{status\PYGZus{}code} \PYG{o}{==} \PYG{l+m+mi}{200}

    \PYG{c+c1}{\PYGZsh{} Train model}
    \PYG{n}{response} \PYG{o}{=} \PYG{n}{test\PYGZus{}client}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/start\PYGZhy{}training/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{json}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hyperparameters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}iterations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{100}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
    \PYG{k}{assert} \PYG{n}{response}\PYG{o}{.}\PYG{n}{status\PYGZus{}code} \PYG{o}{==} \PYG{l+m+mi}{200}
    \PYG{n}{result} \PYG{o}{=} \PYG{n}{response}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{assert} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{function\PYGZus{}result}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{in} \PYG{n}{result}
    \PYG{k}{assert} \PYG{n}{result}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{function\PYGZus{}result}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.5}

    \PYG{c+c1}{\PYGZsh{} Single prediction}
    \PYG{n}{response} \PYG{o}{=} \PYG{n}{test\PYGZus{}client}\PYG{o}{.}\PYG{n}{post}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/predict\PYGZhy{}single/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{json}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{features}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{l+m+mf}{3.0}\PYG{p}{,} \PYG{l+m+mf}{4.0}\PYG{p}{,} \PYG{l+m+mf}{5.0}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}
    \PYG{k}{assert} \PYG{n}{response}\PYG{o}{.}\PYG{n}{status\PYGZus{}code} \PYG{o}{==} \PYG{l+m+mi}{200}
    \PYG{k}{assert} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{prediction}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{in} \PYG{n}{response}\PYG{o}{.}\PYG{n}{json}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Continuous Integration}
\label{\detokenize{testing/overview:continuous-integration}}
\sphinxAtStartPar
The test suite is designed to run in CI/CD pipelines:


\subsubsection{GitHub Actions Example}
\label{\detokenize{testing/overview:github-actions-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nt}{name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{Tests}
\PYG{n+nt}{on}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{[}\PYG{n+nv}{push}\PYG{p+pIndicator}{,}\PYG{+w}{ }\PYG{n+nv}{pull\PYGZus{}request}\PYG{p+pIndicator}{]}

\PYG{n+nt}{jobs}\PYG{p}{:}
\PYG{+w}{  }\PYG{n+nt}{test}\PYG{p}{:}
\PYG{+w}{    }\PYG{n+nt}{runs\PYGZhy{}on}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{ubuntu\PYGZhy{}latest}
\PYG{+w}{    }\PYG{n+nt}{steps}\PYG{p}{:}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{uses}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{actions/checkout@v2}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{Set}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{up}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{Python}
\PYG{+w}{        }\PYG{n+nt}{uses}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{actions/setup\PYGZhy{}python@v2}
\PYG{+w}{        }\PYG{n+nt}{with}\PYG{p}{:}
\PYG{+w}{          }\PYG{n+nt}{python\PYGZhy{}version}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{3.12}\PYG{l+s}{\PYGZsq{}}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{Install}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{dependencies}
\PYG{+w}{        }\PYG{n+nt}{run}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{          }\PYG{n+no}{cd backend}
\PYG{+w}{          }\PYG{n+no}{pip install \PYGZhy{}r requirements.txt}
\PYG{+w}{          }\PYG{n+no}{pip install \PYGZhy{}r requirements\PYGZhy{}dev.txt}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{Run}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{tests}
\PYG{+w}{        }\PYG{n+nt}{run}\PYG{p}{:}\PYG{+w}{ }\PYG{p+pIndicator}{|}
\PYG{+w}{          }\PYG{n+no}{cd backend}
\PYG{+w}{          }\PYG{n+no}{pytest \PYGZhy{}\PYGZhy{}cov=. \PYGZhy{}\PYGZhy{}cov\PYGZhy{}report=xml}
\PYG{+w}{      }\PYG{p+pIndicator}{\PYGZhy{}}\PYG{+w}{ }\PYG{n+nt}{name}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{Upload}\PYG{l+lScalar+lScalarPlain}{ }\PYG{l+lScalar+lScalarPlain}{coverage}
\PYG{+w}{        }\PYG{n+nt}{uses}\PYG{p}{:}\PYG{+w}{ }\PYG{l+lScalar+lScalarPlain}{codecov/codecov\PYGZhy{}action@v2}
\end{sphinxVerbatim}


\subsection{Writing New Tests}
\label{\detokenize{testing/overview:writing-new-tests}}

\subsubsection{Guidelines}
\label{\detokenize{testing/overview:guidelines}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Test Naming}: Use descriptive names starting with \sphinxcode{\sphinxupquote{test\_}}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{One Assertion Per Test}: Keep tests focused

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Fixtures}: Leverage shared fixtures for setup

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Test Edge Cases}: Include boundary conditions

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Mock External Dependencies}: Use mocks for external services

\end{enumerate}


\subsubsection{Example New Test}
\label{\detokenize{testing/overview:example-new-test}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pytest}
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{fivedreg}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{FastNeuralNetwork}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{test\PYGZus{}custom\PYGZus{}architecture}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Test neural network with custom architecture\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Arrange}
    \PYG{n}{custom\PYGZus{}layers} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{FastNeuralNetwork}\PYG{p}{(}\PYG{n}{hidden\PYGZus{}layers}\PYG{o}{=}\PYG{n}{custom\PYGZus{}layers}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Act}
    \PYG{n}{params} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}params}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Assert}
    \PYG{k}{assert} \PYG{n}{params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hidden\PYGZus{}layers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{n}{custom\PYGZus{}layers}
\end{sphinxVerbatim}


\subsection{Performance Tests}
\label{\detokenize{testing/overview:performance-tests}}

\subsubsection{Training Speed Test}
\label{\detokenize{testing/overview:training-speed-test}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{time}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{test\PYGZus{}training\PYGZus{}speed}\PYG{p}{(}\PYG{n}{sample\PYGZus{}data\PYGZus{}medium}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Test that training completes within time limit\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{sample\PYGZus{}data\PYGZus{}medium}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{FastNeuralNetwork}\PYG{p}{(}\PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}

    \PYG{n}{start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{n}{elapsed} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{start}

    \PYG{k}{assert} \PYG{n}{elapsed} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{60}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training took }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{elapsed}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{s (limit: 60s)}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}


\subsection{Troubleshooting Tests}
\label{\detokenize{testing/overview:troubleshooting-tests}}

\subsubsection{Common Issues}
\label{\detokenize{testing/overview:common-issues}}
\sphinxAtStartPar
\sphinxstylestrong{ImportError: No module named ‘main’}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Ensure you\PYGZsq{}re in backend directory}
\PYG{n+nb}{cd}\PYG{+w}{ }backend
pytest
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Coverage data not found}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Delete old coverage data}
rm\PYG{+w}{ }.coverage
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cov\PYG{o}{=}.
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Tests hang or timeout}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Reduce iterations in tests}
\PYG{c+c1}{\PYGZsh{} Check for infinite loops}
\end{sphinxVerbatim}


\subsection{Next Steps}
\label{\detokenize{testing/overview:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{coverage}}} \sphinxhyphen{} Detailed coverage analysis

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} API testing reference

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{deployment/local::doc}]{\sphinxcrossref{\DUrole{doc}{Local Deployment Guide}}}} \sphinxhyphen{} Local testing setup

\end{itemize}

\sphinxstepscope


\section{Local Deployment Guide}
\label{\detokenize{deployment/local:local-deployment-guide}}\label{\detokenize{deployment/local::doc}}
\sphinxAtStartPar
Complete guide for deploying the 5D Interpolator on your local machine.


\subsection{Quick Deploy Script}
\label{\detokenize{deployment/local:quick-deploy-script}}
\sphinxAtStartPar
A comprehensive deployment script is provided for one\sphinxhyphen{}command setup:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Make executable}
chmod\PYG{+w}{ }+x\PYG{+w}{ }scripts/deploy\PYGZhy{}local.sh

\PYG{c+c1}{\PYGZsh{} Run deployment}
./scripts/deploy\PYGZhy{}local.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
This script will:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Check all prerequisites

\item {} 
\sphinxAtStartPar
Set up environment configuration

\item {} 
\sphinxAtStartPar
Start backend and frontend services

\item {} 
\sphinxAtStartPar
Verify deployment

\item {} 
\sphinxAtStartPar
Display access URLs

\end{enumerate}


\subsection{Manual Deployment Steps}
\label{\detokenize{deployment/local:manual-deployment-steps}}
\sphinxAtStartPar
If you prefer manual deployment or need to troubleshoot:


\subsubsection{Step 1: Prerequisites Check}
\label{\detokenize{deployment/local:step-1-prerequisites-check}}
\sphinxAtStartPar
\sphinxstylestrong{Verify Python:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3\PYG{+w}{ }\PYGZhy{}\PYGZhy{}version\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Should be 3.12+}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Verify Node.js:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
node\PYG{+w}{ }\PYGZhy{}\PYGZhy{}version\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Should be 20+}
npm\PYG{+w}{ }\PYGZhy{}\PYGZhy{}version\PYG{+w}{   }\PYG{c+c1}{\PYGZsh{} Should be 10.8+}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Install Missing Dependencies:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} macOS}
brew\PYG{+w}{ }install\PYG{+w}{ }python@3.12\PYG{+w}{ }node

\PYG{c+c1}{\PYGZsh{} Ubuntu/Debian}
sudo\PYG{+w}{ }apt\PYG{+w}{ }install\PYG{+w}{ }python3.12\PYG{+w}{ }nodejs\PYG{+w}{ }npm
\end{sphinxVerbatim}


\subsubsection{Step 2: Backend Setup}
\label{\detokenize{deployment/local:step-2-backend-setup}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend

\PYG{c+c1}{\PYGZsh{} Create virtual environment}
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv

\PYG{c+c1}{\PYGZsh{} Activate virtual environment}
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} macOS/Linux}
\PYG{c+c1}{\PYGZsh{} Or on Windows:}
\PYG{c+c1}{\PYGZsh{} venv\PYGZbs{}Scripts\PYGZbs{}activate}

\PYG{c+c1}{\PYGZsh{} Install dependencies}
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}\PYGZhy{}upgrade\PYG{+w}{ }pip
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}r\PYG{+w}{ }requirements.txt

\PYG{c+c1}{\PYGZsh{} Verify installation}
python\PYG{+w}{ }\PYGZhy{}c\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}import fastapi; import sklearn; print(\PYGZsq{}Backend ready!\PYGZsq{})\PYGZdq{}}
\end{sphinxVerbatim}


\subsubsection{Step 3: Frontend Setup}
\label{\detokenize{deployment/local:step-3-frontend-setup}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }frontend

\PYG{c+c1}{\PYGZsh{} Install dependencies}
npm\PYG{+w}{ }install

\PYG{c+c1}{\PYGZsh{} Verify installation}
npm\PYG{+w}{ }run\PYG{+w}{ }build\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Should complete without errors}
\end{sphinxVerbatim}


\subsubsection{Step 4: Start Services}
\label{\detokenize{deployment/local:step-4-start-services}}
\sphinxAtStartPar
\sphinxstylestrong{Terminal 1 \sphinxhyphen{} Backend:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }backend
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate
uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload\PYG{+w}{ }\PYGZhy{}\PYGZhy{}host\PYG{+w}{ }\PYG{l+m}{0}.0.0.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8000}
\end{sphinxVerbatim}

\sphinxAtStartPar
Expected output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete.
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Terminal 2 \sphinxhyphen{} Frontend:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }run\PYG{+w}{ }dev
\end{sphinxVerbatim}

\sphinxAtStartPar
Expected output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
▲ Next.js 16.0.3
\PYGZhy{} Local:        http://localhost:3000
\PYGZhy{} Ready in 2.1s
\end{sphinxVerbatim}


\subsubsection{Step 5: Verify Deployment}
\label{\detokenize{deployment/local:step-5-verify-deployment}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Test backend}
curl\PYG{+w}{ }http://localhost:8000/health

\PYG{c+c1}{\PYGZsh{} Expected: \PYGZob{}\PYGZdq{}status\PYGZdq{}:\PYGZdq{}healthy\PYGZdq{},\PYGZdq{}service\PYGZdq{}:\PYGZdq{}5D Interpolator Backend by bamk3\PYGZdq{}\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Test frontend}
curl\PYG{+w}{ }\PYGZhy{}I\PYG{+w}{ }http://localhost:3000

\PYG{c+c1}{\PYGZsh{} Expected: HTTP/1.1 200 OK}
\end{sphinxVerbatim}


\subsection{Access the Application}
\label{\detokenize{deployment/local:access-the-application}}
\sphinxAtStartPar
Once deployed, access at:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Main Application}: \sphinxurl{http://localhost:3000}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{API Documentation}: \sphinxurl{http://localhost:8000/docs}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Alternative API Docs}: \sphinxurl{http://localhost:8000/redoc}

\end{itemize}


\subsection{Using the Application}
\label{\detokenize{deployment/local:using-the-application}}

\subsubsection{Upload Sample Dataset}
\label{\detokenize{deployment/local:upload-sample-dataset}}
\sphinxAtStartPar
A sample dataset is provided for testing. Create it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{pickle}

\PYG{c+c1}{\PYGZsh{} Generate sample data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} 5D input features}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Target: sum of squares with noise}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save training data}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sample\PYGZus{}training.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{X}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save prediction data}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sample\PYGZus{}prediction.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Upload via UI:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Navigate to \sphinxurl{http://localhost:3000/upload}

\item {} 
\sphinxAtStartPar
Select “Training” type

\item {} 
\sphinxAtStartPar
Upload \sphinxcode{\sphinxupquote{sample\_training.pkl}}

\item {} 
\sphinxAtStartPar
Proceed to training

\end{enumerate}


\subsection{Environment Configuration}
\label{\detokenize{deployment/local:environment-configuration}}
\sphinxAtStartPar
Create \sphinxcode{\sphinxupquote{.env}} file in project root:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Copy from template}
cp\PYG{+w}{ }.env.development\PYG{+w}{ }.env
\end{sphinxVerbatim}

\sphinxAtStartPar
Key variables:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backend}
\PYG{n+nv}{BACKEND\PYGZus{}PORT}\PYG{o}{=}\PYG{l+m}{8000}
\PYG{n+nv}{CORS\PYGZus{}ORIGINS}\PYG{o}{=}http://localhost:3000

\PYG{c+c1}{\PYGZsh{} Frontend}
\PYG{n+nv}{FRONTEND\PYGZus{}PORT}\PYG{o}{=}\PYG{l+m}{3000}
\PYG{n+nv}{NEXT\PYGZus{}PUBLIC\PYGZus{}API\PYGZus{}URL}\PYG{o}{=}http://localhost:8000

\PYG{c+c1}{\PYGZsh{} Development}
\PYG{n+nv}{DEBUG}\PYG{o}{=}\PYG{n+nb}{true}
\PYG{n+nv}{LOG\PYGZus{}LEVEL}\PYG{o}{=}INFO
\end{sphinxVerbatim}


\subsection{Managing Services}
\label{\detokenize{deployment/local:managing-services}}

\subsubsection{Stop Services}
\label{\detokenize{deployment/local:stop-services}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Press Ctrl+C in each terminal running the services}
\end{sphinxVerbatim}


\subsubsection{Restart Services}
\label{\detokenize{deployment/local:restart-services}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backend}
\PYG{n+nb}{cd}\PYG{+w}{ }backend
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate
uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload

\PYG{c+c1}{\PYGZsh{} Frontend}
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }run\PYG{+w}{ }dev
\end{sphinxVerbatim}


\subsubsection{Check Running Services}
\label{\detokenize{deployment/local:check-running-services}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check what\PYGZsq{}s using port 8000}
lsof\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }:8000

\PYG{c+c1}{\PYGZsh{} Check what\PYGZsq{}s using port 3000}
lsof\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }:3000
\end{sphinxVerbatim}


\subsubsection{Kill Services}
\label{\detokenize{deployment/local:kill-services}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Kill process on port 8000}
lsof\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }:8000\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }grep\PYG{+w}{ }LISTEN\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }awk\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}print \PYGZdl{}2\PYGZcb{}\PYGZsq{}}\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }xargs\PYG{+w}{ }\PYG{n+nb}{kill}\PYG{+w}{ }\PYGZhy{}9

\PYG{c+c1}{\PYGZsh{} Kill process on port 3000}
lsof\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }:3000\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }grep\PYG{+w}{ }LISTEN\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }awk\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}\PYGZob{}print \PYGZdl{}2\PYGZcb{}\PYGZsq{}}\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }xargs\PYG{+w}{ }\PYG{n+nb}{kill}\PYG{+w}{ }\PYGZhy{}9
\end{sphinxVerbatim}


\subsection{Troubleshooting}
\label{\detokenize{deployment/local:troubleshooting}}

\subsubsection{Port Already in Use}
\label{\detokenize{deployment/local:port-already-in-use}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Option 1: Kill the process}
lsof\PYG{+w}{ }\PYGZhy{}i\PYG{+w}{ }:8000
\PYG{n+nb}{kill}\PYG{+w}{ }\PYGZhy{}9\PYG{+w}{ }\PYGZlt{}PID\PYGZgt{}

\PYG{c+c1}{\PYGZsh{} Option 2: Use different port}
\PYG{c+c1}{\PYGZsh{} Backend:}
uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload\PYG{+w}{ }\PYGZhy{}\PYGZhy{}port\PYG{+w}{ }\PYG{l+m}{8001}

\PYG{c+c1}{\PYGZsh{} Frontend: Edit package.json}
\PYG{l+s+s2}{\PYGZdq{}dev\PYGZdq{}}:\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}next dev \PYGZhy{}p 3001\PYGZdq{}}
\end{sphinxVerbatim}


\subsubsection{Module Not Found Errors}
\label{\detokenize{deployment/local:module-not-found-errors}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backend}
\PYG{n+nb}{cd}\PYG{+w}{ }backend
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}r\PYG{+w}{ }requirements.txt

\PYG{c+c1}{\PYGZsh{} Frontend}
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
rm\PYG{+w}{ }\PYGZhy{}rf\PYG{+w}{ }node\PYGZus{}modules\PYG{+w}{ }package\PYGZhy{}lock.json
npm\PYG{+w}{ }install
\end{sphinxVerbatim}


\subsubsection{Permission Errors}
\label{\detokenize{deployment/local:permission-errors}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Python venv creation fails}
sudo\PYG{+w}{ }chown\PYG{+w}{ }\PYGZhy{}R\PYG{+w}{ }\PYG{n+nv}{\PYGZdl{}USER}:\PYG{n+nv}{\PYGZdl{}USER}\PYG{+w}{ }.

\PYG{c+c1}{\PYGZsh{} npm install fails}
npm\PYG{+w}{ }cache\PYG{+w}{ }clean\PYG{+w}{ }\PYGZhy{}\PYGZhy{}force
rm\PYG{+w}{ }\PYGZhy{}rf\PYG{+w}{ }node\PYGZus{}modules
npm\PYG{+w}{ }install
\end{sphinxVerbatim}


\subsubsection{Database/State Issues}
\label{\detokenize{deployment/local:database-state-issues}}
\sphinxAtStartPar
The application uses in\sphinxhyphen{}memory state. To reset:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Stop services}
\PYG{c+c1}{\PYGZsh{} Delete uploaded files}
rm\PYG{+w}{ }\PYGZhy{}rf\PYG{+w}{ }backend/uploaded\PYGZus{}datasets/*

\PYG{c+c1}{\PYGZsh{} Restart services}
\end{sphinxVerbatim}


\subsection{Performance Optimization}
\label{\detokenize{deployment/local:performance-optimization}}

\subsubsection{Backend Optimization}
\label{\detokenize{deployment/local:backend-optimization}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Use production server (gunicorn)}
pip\PYG{+w}{ }install\PYG{+w}{ }gunicorn
gunicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}workers\PYG{+w}{ }\PYG{l+m}{4}\PYG{+w}{ }\PYGZhy{}\PYGZhy{}worker\PYGZhy{}class\PYG{+w}{ }uvicorn.workers.UvicornWorker\PYG{+w}{ }\PYGZhy{}\PYGZhy{}bind\PYG{+w}{ }\PYG{l+m}{0}.0.0.0:8000
\end{sphinxVerbatim}


\subsubsection{Frontend Optimization}
\label{\detokenize{deployment/local:frontend-optimization}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Build for production}
\PYG{n+nb}{cd}\PYG{+w}{ }frontend
npm\PYG{+w}{ }run\PYG{+w}{ }build
npm\PYG{+w}{ }start\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Runs optimized production build}
\end{sphinxVerbatim}


\subsection{Data Persistence}
\label{\detokenize{deployment/local:data-persistence}}
\sphinxAtStartPar
Uploaded datasets are stored in:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
backend/
└── uploaded\PYGZus{}datasets/
    ├── training\PYGZus{}dataset.pkl
    └── prediction\PYGZus{}dataset.pkl
\end{sphinxVerbatim}

\sphinxAtStartPar
Backup and restore:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backup}
tar\PYG{+w}{ }\PYGZhy{}czf\PYG{+w}{ }datasets\PYGZus{}backup.tar.gz\PYG{+w}{ }backend/uploaded\PYGZus{}datasets/

\PYG{c+c1}{\PYGZsh{} Restore}
tar\PYG{+w}{ }\PYGZhy{}xzf\PYG{+w}{ }datasets\PYGZus{}backup.tar.gz
\end{sphinxVerbatim}


\subsection{Development Mode Features}
\label{\detokenize{deployment/local:development-mode-features}}

\subsubsection{Hot Reload}
\label{\detokenize{deployment/local:hot-reload}}
\sphinxAtStartPar
Both backend and frontend support hot reload:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Backend}: Changes to Python files trigger automatic reload

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Frontend}: Changes to React components update instantly

\end{itemize}


\subsubsection{Debug Mode}
\label{\detokenize{deployment/local:debug-mode}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Backend with debug logging}
\PYG{n+nv}{LOG\PYGZus{}LEVEL}\PYG{o}{=}DEBUG\PYG{+w}{ }uvicorn\PYG{+w}{ }main:app\PYG{+w}{ }\PYGZhy{}\PYGZhy{}reload

\PYG{c+c1}{\PYGZsh{} Frontend with debug}
npm\PYG{+w}{ }run\PYG{+w}{ }dev\PYG{+w}{  }\PYG{c+c1}{\PYGZsh{} Already in debug mode}
\end{sphinxVerbatim}


\subsubsection{API Testing}
\label{\detokenize{deployment/local:api-testing}}
\sphinxAtStartPar
Use the interactive API docs:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxurl{http://localhost:8000/docs} (Swagger UI)

\item {} 
\sphinxAtStartPar
Test endpoints directly in browser

\item {} 
\sphinxAtStartPar
View request/response schemas

\end{itemize}


\subsection{Next Steps}
\label{\detokenize{deployment/local:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{docker}}} \sphinxhyphen{} Deploy using Docker

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{production}}} \sphinxhyphen{} Production deployment guide

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{testing/overview::doc}]{\sphinxcrossref{\DUrole{doc}{Testing Overview}}}} \sphinxhyphen{} Run test suite

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{quickstart::doc}]{\sphinxcrossref{\DUrole{doc}{Quick Start Guide}}}} \sphinxhyphen{} Application usage guide

\end{itemize}

\sphinxstepscope


\section{System Architecture}
\label{\detokenize{architecture:system-architecture}}\label{\detokenize{architecture::doc}}
\sphinxAtStartPar
Comprehensive overview of the 5D Interpolator system architecture.


\subsection{Overview}
\label{\detokenize{architecture:overview}}
\sphinxAtStartPar
The system follows a modern client\sphinxhyphen{}server architecture with clear separation of concerns:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
┌─────────────────────────────────────────────────────┐
│                    Client Layer                     │
│  ┌───────────────────────────────────────────────┐  │
│  │         Next.js Frontend (Port 3000)          │  │
│  │  \PYGZhy{} React 19 with TypeScript                   │  │
│  │  \PYGZhy{} Tailwind CSS v4                           │  │
│  │  \PYGZhy{} Upload/Train/Predict Pages                │  │
│  └───────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────┘
                         │
                         ├─ HTTP REST API
                         │
┌─────────────────────────────────────────────────────┐
│                    Server Layer                     │
│  ┌───────────────────────────────────────────────┐  │
│  │        FastAPI Backend (Port 8000)            │  │
│  │  \PYGZhy{} RESTful API Endpoints                      │  │
│  │  \PYGZhy{} File Upload Handling                       │  │
│  │  \PYGZhy{} State Management                           │  │
│  └───────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────┘
                         │
                         ├─ Python Interface
                         │
┌─────────────────────────────────────────────────────┐
│                  Processing Layer                   │
│  ┌───────────────────────────────────────────────┐  │
│  │       fivedreg Neural Network Package         │  │
│  │  \PYGZhy{} FastNeuralNetwork Class                    │  │
│  │  \PYGZhy{} Data Preprocessing                         │  │
│  │  \PYGZhy{} Model Training \PYGZam{} Prediction                │  │
│  └───────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────┘
                         │
                         ├─ NumPy/sklearn
                         │
┌─────────────────────────────────────────────────────┐
│                  ML Framework Layer                 │
│  ┌───────────────────────────────────────────────┐  │
│  │            scikit\PYGZhy{}learn MLPRegressor          │  │
│  │  \PYGZhy{} Neural Network Implementation              │  │
│  │  \PYGZhy{} Optimization Algorithms                    │  │
│  │  \PYGZhy{} Metrics Calculation                        │  │
│  └───────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────┘
\end{sphinxVerbatim}


\subsection{Technology Stack}
\label{\detokenize{architecture:technology-stack}}

\subsubsection{Frontend}
\label{\detokenize{architecture:frontend}}
\sphinxAtStartPar
\sphinxstylestrong{Framework \& Runtime:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Next.js 16.0.3 (React framework)

\item {} 
\sphinxAtStartPar
React 19.2.0 (UI library)

\item {} 
\sphinxAtStartPar
Node.js 20+ (runtime)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Language \& Tooling:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
TypeScript 5 (type safety)

\item {} 
\sphinxAtStartPar
ESLint (linting)

\item {} 
\sphinxAtStartPar
Turbopack (build tool)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Styling:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tailwind CSS v4 (utility\sphinxhyphen{}first CSS)

\item {} 
\sphinxAtStartPar
PostCSS (CSS processing)

\item {} 
\sphinxAtStartPar
Geist fonts (typography)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Development:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hot module replacement

\item {} 
\sphinxAtStartPar
Fast refresh

\item {} 
\sphinxAtStartPar
TypeScript checking

\end{itemize}


\subsubsection{Backend}
\label{\detokenize{architecture:backend}}
\sphinxAtStartPar
\sphinxstylestrong{Framework:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
FastAPI 0.115.6 (web framework)

\item {} 
\sphinxAtStartPar
Uvicorn (ASGI server)

\item {} 
\sphinxAtStartPar
Python 3.12+

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Core Libraries:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
NumPy 1.26.4 (numerical computing)

\item {} 
\sphinxAtStartPar
scikit\sphinxhyphen{}learn 1.5.1 (machine learning)

\item {} 
\sphinxAtStartPar
Pydantic 2.10.5 (validation)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Testing:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
pytest 8.3.4 (test framework)

\item {} 
\sphinxAtStartPar
pytest\sphinxhyphen{}cov (coverage reporting)

\item {} 
\sphinxAtStartPar
pytest\sphinxhyphen{}asyncio (async testing)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Deployment:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Docker (containerization)

\item {} 
\sphinxAtStartPar
Docker Compose (orchestration)

\end{itemize}


\subsection{Data Flow}
\label{\detokenize{architecture:data-flow}}

\subsubsection{Training Workflow}
\label{\detokenize{architecture:training-workflow}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1. User selects .pkl file
   ↓
2. Frontend: POST /upload\PYGZhy{}fit\PYGZhy{}dataset/
   ↓
3. Backend: Save to uploaded\PYGZus{}datasets/
   ↓
4. Backend: Validate format
   ↓
5. Backend: Return dataset preview
   ↓
6. Frontend: Display preview
   ↓
7. User configures hyperparameters
   ↓
8. Frontend: POST /start\PYGZhy{}training/
   ↓
9. Backend: Load dataset
   ↓
10. Backend: Call benchmark\PYGZus{}training\PYGZus{}speed()
   ↓
11. fivedreg: Preprocess data
   ↓
12. fivedreg: Train FastNeuralNetwork
   ↓
13. fivedreg: Calculate metrics
   ↓
14. Backend: Store model in memory
   ↓
15. Backend: Return metrics
   ↓
16. Frontend: Display results
\end{sphinxVerbatim}


\subsubsection{Prediction Workflow}
\label{\detokenize{architecture:prediction-workflow}}
\sphinxAtStartPar
\sphinxstylestrong{Batch Prediction:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1. User selects .pkl file
   ↓
2. Frontend: POST /upload\PYGZhy{}predict\PYGZhy{}dataset/
   ↓
3. Backend: Save file
   ↓
4. Backend: Validate format
   ↓
5. Frontend: POST /start\PYGZhy{}predict/
   ↓
6. Backend: Load dataset
   ↓
7. Backend: Use stored model
   ↓
8. fivedreg: Generate predictions
   ↓
9. Backend: Return predictions
   ↓
10. Frontend: Display results
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Single Prediction:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1. User enters 5 feature values
   ↓
2. Frontend: POST /predict\PYGZhy{}single/
   ↓
3. Backend: Validate input
   ↓
4. Backend: Use stored model
   ↓
5. fivedreg: Predict single value
   ↓
6. Backend: Return prediction
   ↓
7. Frontend: Display result
\end{sphinxVerbatim}


\subsection{State Management}
\label{\detokenize{architecture:state-management}}

\subsubsection{Backend State}
\label{\detokenize{architecture:backend-state}}
\sphinxAtStartPar
The backend maintains global state:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Global variables in main.py}
\PYG{n}{processing\PYGZus{}result} \PYG{o}{=} \PYG{k+kc}{None}      \PYG{c+c1}{\PYGZsh{} Path to training dataset}
\PYG{n}{train\PYGZus{}result} \PYG{o}{=} \PYG{k+kc}{None}           \PYG{c+c1}{\PYGZsh{} (model, metrics) tuple}
\PYG{n}{predict\PYGZus{}input} \PYG{o}{=} \PYG{k+kc}{None}          \PYG{c+c1}{\PYGZsh{} Path to prediction dataset}
\PYG{n}{model} \PYG{o}{=} \PYG{k+kc}{None}                  \PYG{c+c1}{\PYGZsh{} Trained FastNeuralNetwork}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{State Lifecycle:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{processing\_result}} set on training upload

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{train\_result}} and \sphinxcode{\sphinxupquote{model}} set on training completion

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{predict\_input}} set on prediction upload

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{model}} used for all predictions

\item {} 
\sphinxAtStartPar
State cleared on server restart

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Implications:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Server must stay running between operations

\item {} 
\sphinxAtStartPar
No concurrent users (single session)

\item {} 
\sphinxAtStartPar
State lost on crash/restart

\item {} 
\sphinxAtStartPar
Suitable for development/coursework

\end{itemize}


\subsubsection{Frontend State}
\label{\detokenize{architecture:frontend-state}}
\sphinxAtStartPar
Each page manages its own state using React hooks:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// Upload page}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{file}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setFile}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{File}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{uploadResult}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setUploadResult}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{any}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}

\PYG{c+c1}{// Train page}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{trainingDataUploaded}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setTrainingDataUploaded}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{modelTrained}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setModelTrained}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{k+kc}{false}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{trainResult}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setTrainResult}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{any}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{hyperparameters}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setHyperparameters}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{p}{...}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{// Predict page}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{predictionMode}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setPredictionMode}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}batch\PYGZsq{}}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{l+s+s1}{\PYGZsq{}single\PYGZsq{}}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}batch\PYGZsq{}}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{predictionResult}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setPredictionResult}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{n+nx}{any}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\PYG{k+kd}{const}\PYG{+w}{ }\PYG{p}{[}\PYG{n+nx}{singlePrediction}\PYG{p}{,}\PYG{+w}{ }\PYG{n+nx}{setSinglePrediction}\PYG{p}{]}\PYG{+w}{ }\PYG{o}{=}\PYG{+w}{ }\PYG{n+nx}{useState}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{number}\PYG{+w}{ }\PYG{o}{|}\PYG{+w}{ }\PYG{k+kc}{null}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{k+kc}{null}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{State Synchronization:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Polls \sphinxcode{\sphinxupquote{/status}} endpoint on mount

\item {} 
\sphinxAtStartPar
Updates local state based on server state

\item {} 
\sphinxAtStartPar
Enables/disables UI based on state

\end{itemize}


\subsection{API Design}
\label{\detokenize{architecture:api-design}}

\subsubsection{RESTful Principles}
\label{\detokenize{architecture:restful-principles}}
\sphinxAtStartPar
The API follows REST conventions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{GET}} for retrieving state

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{POST}} for creating/triggering operations

\item {} 
\sphinxAtStartPar
JSON request/response bodies

\item {} 
\sphinxAtStartPar
HTTP status codes for errors

\item {} 
\sphinxAtStartPar
CORS enabled for development

\end{itemize}


\subsubsection{Endpoints}
\label{\detokenize{architecture:endpoints}}
\sphinxAtStartPar
\sphinxstylestrong{Health \& Status:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
GET  /           \(\rightarrow\) Welcome message
GET  /health     \(\rightarrow\) Health check
GET  /status     \(\rightarrow\) System state
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Upload:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
POST /upload\PYGZhy{}fit\PYGZhy{}dataset/      \(\rightarrow\) Upload training data
POST /upload\PYGZhy{}predict\PYGZhy{}dataset/  \(\rightarrow\) Upload prediction data
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Training:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
POST /start\PYGZhy{}training/  \(\rightarrow\) Train model with hyperparameters
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Prediction:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
POST /start\PYGZhy{}predict/   \(\rightarrow\) Batch prediction
POST /predict\PYGZhy{}single/  \(\rightarrow\) Single prediction
\end{sphinxVerbatim}


\subsubsection{Request/Response Format}
\label{\detokenize{architecture:request-response-format}}
\sphinxAtStartPar
\sphinxstylestrong{Training Request:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hyperparameters\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}1\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{128}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}2\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{64}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}3\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{32}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}learning\PYGZus{}rate\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.001}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}iterations\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{500}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}early\PYGZus{}stopping\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Training Response:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}status\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}success\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}metrics\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}r2\PYGZus{}score\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.9872}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}mse\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.0123}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}mae\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.0891}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}rmse\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.1109}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}training\PYGZus{}time\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{23.45}
\PYG{+w}{  }\PYG{p}{\PYGZcb{},}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}hyperparameters\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}1\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{128}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}2\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{64}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}hidden\PYGZus{}layer\PYGZus{}3\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{32}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}learning\PYGZus{}rate\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mf}{0.001}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}max\PYGZus{}iterations\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{500}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}early\PYGZus{}stopping\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{k+kc}{true}
\PYG{+w}{  }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Prediction Response:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}predictions\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}\PYG{l+m+mf}{1.234}\PYG{p}{,}\PYG{+w}{ }\PYG{l+m+mf}{5.678}\PYG{p}{,}\PYG{+w}{ }\PYG{err}{.}\PYG{err}{.}\PYG{err}{.}\PYG{p}{],}
\PYG{+w}{  }\PYG{n+nt}{\PYGZdq{}count\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{100}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Data Processing Pipeline}
\label{\detokenize{architecture:data-processing-pipeline}}

\subsubsection{Data Validation}
\label{\detokenize{architecture:data-validation}}
\sphinxAtStartPar
\sphinxstylestrong{Step 1: File Format Validation}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check file extension}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{filename}\PYG{o}{.}\PYG{n}{endswith}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.pkl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{File must be .pkl format}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Try to load pickle}
\PYG{k}{try}\PYG{p}{:}
    \PYG{n}{data} \PYG{o}{=} \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{file}\PYG{p}{)}
\PYG{k}{except} \PYG{n+ne}{Exception}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Invalid pickle file}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 2: Structure Validation}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Training data}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n+nb}{dict}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Must be dictionary}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{data} \PYG{o+ow}{or} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{data}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Must contain }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{X}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Prediction data}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ndarray}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Must be NumPy array}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 3: Shape Validation}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check dimensions}
\PYG{k}{if} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{!=} \PYG{l+m+mi}{5}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X must have 5 features}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{if} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{!=} \PYG{l+m+mi}{1}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y must be 1D}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{if} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{!=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X and y must have same samples}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 4: Value Validation}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Check for invalid values}
\PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{o+ow}{or} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isinf}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X contains NaN or inf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{o+ow}{or} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isinf}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y contains NaN or inf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Data Preprocessing}
\label{\detokenize{architecture:data-preprocessing}}
\sphinxAtStartPar
\sphinxstylestrong{Step 1: Clean Data}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Remove NaN rows}
\PYG{n}{mask} \PYG{o}{=} \PYG{o}{\PYGZti{}}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{|} \PYG{n}{np}\PYG{o}{.}\PYG{n}{isnan}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{n}{mask}\PYG{p}{]}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{p}{[}\PYG{n}{mask}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 2: Split Data}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} 60\PYGZpc{} train, 20\PYGZpc{} validation, 20\PYGZpc{} test}
\PYG{n}{X\PYGZus{}temp}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}temp}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}
    \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}
\PYG{p}{)}

\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}val} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}
    \PYG{n}{X\PYGZus{}temp}\PYG{p}{,} \PYG{n}{y\PYGZus{}temp}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}
\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Step 3: Standardize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{StandardScaler}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{scaler}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{)}
\PYG{n}{X\PYGZus{}val} \PYG{o}{=} \PYG{n}{scaler}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}val}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test} \PYG{o}{=} \PYG{n}{scaler}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Security Considerations}
\label{\detokenize{architecture:security-considerations}}

\subsubsection{Current Implementation}
\label{\detokenize{architecture:current-implementation}}
\sphinxAtStartPar
\sphinxstylestrong{Suitable for:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Local development

\item {} 
\sphinxAtStartPar
Coursework/academic use

\item {} 
\sphinxAtStartPar
Single\sphinxhyphen{}user scenarios

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Not suitable for:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Production deployment

\item {} 
\sphinxAtStartPar
Multi\sphinxhyphen{}user systems

\item {} 
\sphinxAtStartPar
Public internet exposure

\end{itemize}


\subsubsection{Security Measures}
\label{\detokenize{architecture:security-measures}}
\sphinxAtStartPar
\sphinxstylestrong{Input Validation:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
File size limits

\item {} 
\sphinxAtStartPar
Format validation

\item {} 
\sphinxAtStartPar
Value range checking

\item {} 
\sphinxAtStartPar
Type validation with Pydantic

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{CORS Configuration:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{app}\PYG{o}{.}\PYG{n}{add\PYGZus{}middleware}\PYG{p}{(}
    \PYG{n}{CORSMiddleware}\PYG{p}{,}
    \PYG{n}{allow\PYGZus{}origins}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{http://localhost:3000}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{allow\PYGZus{}credentials}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{n}{allow\PYGZus{}methods}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{*}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{allow\PYGZus{}headers}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{*}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{For Production:}

\sphinxAtStartPar
Would need:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Authentication \& authorization

\item {} 
\sphinxAtStartPar
Rate limiting

\item {} 
\sphinxAtStartPar
File scanning

\item {} 
\sphinxAtStartPar
HTTPS/TLS

\item {} 
\sphinxAtStartPar
Database for state

\item {} 
\sphinxAtStartPar
Session management

\item {} 
\sphinxAtStartPar
Input sanitization

\item {} 
\sphinxAtStartPar
Error message sanitization

\end{itemize}


\subsection{Scalability}
\label{\detokenize{architecture:scalability}}

\subsubsection{Current Limitations}
\label{\detokenize{architecture:current-limitations}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Single server instance

\item {} 
\sphinxAtStartPar
In\sphinxhyphen{}memory state

\item {} 
\sphinxAtStartPar
No horizontal scaling

\item {} 
\sphinxAtStartPar
No load balancing

\item {} 
\sphinxAtStartPar
Limited to CPU training

\end{itemize}


\subsubsection{Potential Improvements}
\label{\detokenize{architecture:potential-improvements}}
\sphinxAtStartPar
\sphinxstylestrong{For Higher Scale:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Database Integration:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Store models in database

\item {} 
\sphinxAtStartPar
Persist training state

\item {} 
\sphinxAtStartPar
Support multiple users

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Queue System:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Background job processing

\item {} 
\sphinxAtStartPar
Async training tasks

\item {} 
\sphinxAtStartPar
Progress tracking

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Caching:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Redis for session state

\item {} 
\sphinxAtStartPar
Model caching

\item {} 
\sphinxAtStartPar
Result caching

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Microservices:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Separate training service

\item {} 
\sphinxAtStartPar
Separate prediction service

\item {} 
\sphinxAtStartPar
API gateway

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GPU Support:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
PyTorch/TensorFlow

\item {} 
\sphinxAtStartPar
CUDA acceleration

\item {} 
\sphinxAtStartPar
Larger networks

\end{itemize}

\end{enumerate}


\subsection{Deployment Options}
\label{\detokenize{architecture:deployment-options}}

\subsubsection{Development}
\label{\detokenize{architecture:development}}
\sphinxAtStartPar
\sphinxstylestrong{Local:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
./scripts/deploy\PYGZhy{}local.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Docker:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker\PYG{+w}{ }compose\PYG{+w}{ }up
\end{sphinxVerbatim}


\subsubsection{Production}
\label{\detokenize{architecture:production}}
\sphinxAtStartPar
\sphinxstylestrong{Cloud Platforms:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
AWS (ECS, Lambda, SageMaker)

\item {} 
\sphinxAtStartPar
Google Cloud (Cloud Run, AI Platform)

\item {} 
\sphinxAtStartPar
Azure (App Service, ML)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Containerization:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Docker images

\item {} 
\sphinxAtStartPar
Kubernetes orchestration

\item {} 
\sphinxAtStartPar
Auto\sphinxhyphen{}scaling

\end{itemize}


\subsection{Monitoring \& Logging}
\label{\detokenize{architecture:monitoring-logging}}

\subsubsection{Current Logging}
\label{\detokenize{architecture:current-logging}}
\sphinxAtStartPar
\sphinxstylestrong{Backend:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Uvicorn access logs

\item {} 
\sphinxAtStartPar
Python print statements

\item {} 
\sphinxAtStartPar
Error stack traces

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Frontend:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Console.log debugging

\item {} 
\sphinxAtStartPar
Error boundaries

\end{itemize}


\subsubsection{Production Logging}
\label{\detokenize{architecture:production-logging}}
\sphinxAtStartPar
Would need:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Structured logging (JSON)

\item {} 
\sphinxAtStartPar
Log aggregation (ELK stack)

\item {} 
\sphinxAtStartPar
Error tracking (Sentry)

\item {} 
\sphinxAtStartPar
Performance monitoring (APM)

\item {} 
\sphinxAtStartPar
User analytics

\end{itemize}


\subsection{Testing Strategy}
\label{\detokenize{architecture:testing-strategy}}
\sphinxAtStartPar
\sphinxstylestrong{Unit Tests:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Backend endpoints (pytest)

\item {} 
\sphinxAtStartPar
Neural network module

\item {} 
\sphinxAtStartPar
Data handlers

\item {} 
\sphinxAtStartPar
Validation logic

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Integration Tests:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
End\sphinxhyphen{}to\sphinxhyphen{}end workflows

\item {} 
\sphinxAtStartPar
API contract testing

\item {} 
\sphinxAtStartPar
Database interactions

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Coverage:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
74.54\% overall

\item {} 
\sphinxAtStartPar
52 total tests

\end{itemize}

\sphinxAtStartPar
See {\hyperref[\detokenize{testing/overview::doc}]{\sphinxcrossref{\DUrole{doc}{Testing Overview}}}} for details.


\subsection{Next Steps}
\label{\detokenize{architecture:next-steps}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{deployment/local::doc}]{\sphinxcrossref{\DUrole{doc}{Local Deployment Guide}}}} \sphinxhyphen{} Local deployment guide

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-doc}{deployment/docker}}} \sphinxhyphen{} Docker deployment

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{api/backend::doc}]{\sphinxcrossref{\DUrole{doc}{Backend API Reference}}}} \sphinxhyphen{} Backend API reference

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{testing/overview::doc}]{\sphinxcrossref{\DUrole{doc}{Testing Overview}}}} \sphinxhyphen{} Testing documentation

\end{itemize}


\chapter{Indices and Tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{genindex}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{modindex}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{search}}}

\end{itemize}


\chapter{Project Information}
\label{\detokenize{index:project-information}}\begin{quote}\begin{description}
\sphinxlineitem{Author}
\sphinxAtStartPar
Makimona Kiakisolako (bamk3)

\sphinxlineitem{Institution}
\sphinxAtStartPar
University of Cambridge

\sphinxlineitem{Course}
\sphinxAtStartPar
DIS Course 2025

\sphinxlineitem{License}
\sphinxAtStartPar
MIT

\sphinxlineitem{Version}
\sphinxAtStartPar
0.1.0

\sphinxlineitem{Contact}
\sphinxAtStartPar
\sphinxhref{mailto:bamk3@cam.ac.uk}{bamk3@cam.ac.uk}

\end{description}\end{quote}


\section{Links}
\label{\detokenize{index:links}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{index:}]{\sphinxsamedocref{GitHub Repository}}}

\item {} 
\sphinxAtStartPar
\sphinxhref{http://localhost:8000/docs}{API Documentation}%
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxnolinkurl{http://localhost:8000/docs}
%
\end{footnote}

\item {} 
\sphinxAtStartPar
\sphinxhref{fivedreg-package/index.html}{fivedreg Package Documentation}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{index:}]{\sphinxsamedocref{Issue Tracker}}}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{f}
\item\relax\sphinxstyleindexentry{fivedreg.base\_fivedreg}\sphinxstyleindexpageref{api/neural_network:\detokenize{module-fivedreg.base_fivedreg}}
\item\relax\sphinxstyleindexentry{fivedreg.data\_hand.module}\sphinxstyleindexpageref{api/neural_network:\detokenize{module-fivedreg.data_hand.module}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}